I0216 15:56:33.984625 24778 caffe.cpp:184] Using GPUs 0
I0216 15:56:34.276506 24778 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 1000
base_lr: 0.001
display: 200
max_iter: 60000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.004
snapshot: 10000
snapshot_prefix: "examples/cifar10/cifar10_full"
solver_mode: GPU
device_id: 0
net: "examples/cifar10/cifar10_full_train_test.prototxt"
snapshot_format: HDF5
I0216 15:56:34.276617 24778 solver.cpp:91] Creating training net from net file: examples/cifar10/cifar10_full_train_test.prototxt
I0216 15:56:34.277021 24778 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0216 15:56:34.277042 24778 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0216 15:56:34.277142 24778 net.cpp:49] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TRAIN
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0216 15:56:34.277215 24778 layer_factory.hpp:77] Creating layer cifar
I0216 15:56:34.277678 24778 net.cpp:106] Creating Layer cifar
I0216 15:56:34.277701 24778 net.cpp:411] cifar -> data
I0216 15:56:34.277737 24778 net.cpp:411] cifar -> label
I0216 15:56:34.277762 24778 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0216 15:56:34.278697 24782 db_lmdb.cpp:38] Opened lmdb examples/cifar10/cifar10_train_lmdb
I0216 15:56:34.303323 24778 data_layer.cpp:41] output data size: 100,3,32,32
I0216 15:56:34.319190 24778 net.cpp:150] Setting up cifar
I0216 15:56:34.319242 24778 net.cpp:157] Top shape: 100 3 32 32 (307200)
I0216 15:56:34.319252 24778 net.cpp:157] Top shape: 100 (100)
I0216 15:56:34.319259 24778 net.cpp:165] Memory required for data: 1229200
I0216 15:56:34.319273 24778 layer_factory.hpp:77] Creating layer conv1
I0216 15:56:34.319303 24778 net.cpp:106] Creating Layer conv1
I0216 15:56:34.319311 24778 net.cpp:454] conv1 <- data
I0216 15:56:34.319327 24778 net.cpp:411] conv1 -> conv1
I0216 15:56:34.580549 24778 net.cpp:150] Setting up conv1
I0216 15:56:34.580592 24778 net.cpp:157] Top shape: 100 32 32 32 (3276800)
I0216 15:56:34.580597 24778 net.cpp:165] Memory required for data: 14336400
I0216 15:56:34.580617 24778 layer_factory.hpp:77] Creating layer pool1
I0216 15:56:34.580631 24778 net.cpp:106] Creating Layer pool1
I0216 15:56:34.580637 24778 net.cpp:454] pool1 <- conv1
I0216 15:56:34.580644 24778 net.cpp:411] pool1 -> pool1
I0216 15:56:34.581300 24778 net.cpp:150] Setting up pool1
I0216 15:56:34.581320 24778 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 15:56:34.581323 24778 net.cpp:165] Memory required for data: 17613200
I0216 15:56:34.581328 24778 layer_factory.hpp:77] Creating layer relu1
I0216 15:56:34.581336 24778 net.cpp:106] Creating Layer relu1
I0216 15:56:34.581341 24778 net.cpp:454] relu1 <- pool1
I0216 15:56:34.581346 24778 net.cpp:397] relu1 -> pool1 (in-place)
I0216 15:56:34.582020 24778 net.cpp:150] Setting up relu1
I0216 15:56:34.582036 24778 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 15:56:34.582041 24778 net.cpp:165] Memory required for data: 20890000
I0216 15:56:34.582046 24778 layer_factory.hpp:77] Creating layer norm1
I0216 15:56:34.582057 24778 net.cpp:106] Creating Layer norm1
I0216 15:56:34.582062 24778 net.cpp:454] norm1 <- pool1
I0216 15:56:34.582068 24778 net.cpp:411] norm1 -> norm1
I0216 15:56:34.583585 24778 net.cpp:150] Setting up norm1
I0216 15:56:34.583606 24778 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 15:56:34.583611 24778 net.cpp:165] Memory required for data: 24166800
I0216 15:56:34.583614 24778 layer_factory.hpp:77] Creating layer conv2
I0216 15:56:34.583628 24778 net.cpp:106] Creating Layer conv2
I0216 15:56:34.583632 24778 net.cpp:454] conv2 <- norm1
I0216 15:56:34.583639 24778 net.cpp:411] conv2 -> conv2
I0216 15:56:34.587522 24778 net.cpp:150] Setting up conv2
I0216 15:56:34.587556 24778 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 15:56:34.587561 24778 net.cpp:165] Memory required for data: 27443600
I0216 15:56:34.587574 24778 layer_factory.hpp:77] Creating layer relu2
I0216 15:56:34.587584 24778 net.cpp:106] Creating Layer relu2
I0216 15:56:34.587589 24778 net.cpp:454] relu2 <- conv2
I0216 15:56:34.587597 24778 net.cpp:397] relu2 -> conv2 (in-place)
I0216 15:56:34.588742 24778 net.cpp:150] Setting up relu2
I0216 15:56:34.588757 24778 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 15:56:34.588760 24778 net.cpp:165] Memory required for data: 30720400
I0216 15:56:34.588764 24778 layer_factory.hpp:77] Creating layer pool2
I0216 15:56:34.588773 24778 net.cpp:106] Creating Layer pool2
I0216 15:56:34.588778 24778 net.cpp:454] pool2 <- conv2
I0216 15:56:34.588783 24778 net.cpp:411] pool2 -> pool2
I0216 15:56:34.589902 24778 net.cpp:150] Setting up pool2
I0216 15:56:34.589915 24778 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0216 15:56:34.589920 24778 net.cpp:165] Memory required for data: 31539600
I0216 15:56:34.589923 24778 layer_factory.hpp:77] Creating layer norm2
I0216 15:56:34.589934 24778 net.cpp:106] Creating Layer norm2
I0216 15:56:34.589938 24778 net.cpp:454] norm2 <- pool2
I0216 15:56:34.589946 24778 net.cpp:411] norm2 -> norm2
I0216 15:56:34.590821 24778 net.cpp:150] Setting up norm2
I0216 15:56:34.590850 24778 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0216 15:56:34.590855 24778 net.cpp:165] Memory required for data: 32358800
I0216 15:56:34.590859 24778 layer_factory.hpp:77] Creating layer conv3
I0216 15:56:34.590868 24778 net.cpp:106] Creating Layer conv3
I0216 15:56:34.590873 24778 net.cpp:454] conv3 <- norm2
I0216 15:56:34.590880 24778 net.cpp:411] conv3 -> conv3
I0216 15:56:34.594516 24778 net.cpp:150] Setting up conv3
I0216 15:56:34.594560 24778 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0216 15:56:34.594569 24778 net.cpp:165] Memory required for data: 33997200
I0216 15:56:34.594591 24778 layer_factory.hpp:77] Creating layer relu3
I0216 15:56:34.594606 24778 net.cpp:106] Creating Layer relu3
I0216 15:56:34.594614 24778 net.cpp:454] relu3 <- conv3
I0216 15:56:34.594624 24778 net.cpp:397] relu3 -> conv3 (in-place)
I0216 15:56:34.595369 24778 net.cpp:150] Setting up relu3
I0216 15:56:34.595387 24778 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0216 15:56:34.595391 24778 net.cpp:165] Memory required for data: 35635600
I0216 15:56:34.595396 24778 layer_factory.hpp:77] Creating layer pool3
I0216 15:56:34.595405 24778 net.cpp:106] Creating Layer pool3
I0216 15:56:34.595410 24778 net.cpp:454] pool3 <- conv3
I0216 15:56:34.595417 24778 net.cpp:411] pool3 -> pool3
I0216 15:56:34.596014 24778 net.cpp:150] Setting up pool3
I0216 15:56:34.596026 24778 net.cpp:157] Top shape: 100 64 4 4 (102400)
I0216 15:56:34.596030 24778 net.cpp:165] Memory required for data: 36045200
I0216 15:56:34.596035 24778 layer_factory.hpp:77] Creating layer ip1
I0216 15:56:34.596045 24778 net.cpp:106] Creating Layer ip1
I0216 15:56:34.596050 24778 net.cpp:454] ip1 <- pool3
I0216 15:56:34.596055 24778 net.cpp:411] ip1 -> ip1
I0216 15:56:34.609452 24778 net.cpp:150] Setting up ip1
I0216 15:56:34.609501 24778 net.cpp:157] Top shape: 100 500 (50000)
I0216 15:56:34.609510 24778 net.cpp:165] Memory required for data: 36245200
I0216 15:56:34.609524 24778 layer_factory.hpp:77] Creating layer relu4
I0216 15:56:34.609537 24778 net.cpp:106] Creating Layer relu4
I0216 15:56:34.609545 24778 net.cpp:454] relu4 <- ip1
I0216 15:56:34.609560 24778 net.cpp:397] relu4 -> ip1 (in-place)
I0216 15:56:34.610337 24778 net.cpp:150] Setting up relu4
I0216 15:56:34.610357 24778 net.cpp:157] Top shape: 100 500 (50000)
I0216 15:56:34.610362 24778 net.cpp:165] Memory required for data: 36445200
I0216 15:56:34.610366 24778 layer_factory.hpp:77] Creating layer ip2
I0216 15:56:34.610378 24778 net.cpp:106] Creating Layer ip2
I0216 15:56:34.610381 24778 net.cpp:454] ip2 <- ip1
I0216 15:56:34.610389 24778 net.cpp:411] ip2 -> ip2
I0216 15:56:34.611027 24778 net.cpp:150] Setting up ip2
I0216 15:56:34.611045 24778 net.cpp:157] Top shape: 100 10 (1000)
I0216 15:56:34.611049 24778 net.cpp:165] Memory required for data: 36449200
I0216 15:56:34.611062 24778 layer_factory.hpp:77] Creating layer loss
I0216 15:56:34.611070 24778 net.cpp:106] Creating Layer loss
I0216 15:56:34.611075 24778 net.cpp:454] loss <- ip2
I0216 15:56:34.611079 24778 net.cpp:454] loss <- label
I0216 15:56:34.611086 24778 net.cpp:411] loss -> loss
I0216 15:56:34.611098 24778 layer_factory.hpp:77] Creating layer loss
I0216 15:56:34.611913 24778 net.cpp:150] Setting up loss
I0216 15:56:34.611932 24778 net.cpp:157] Top shape: (1)
I0216 15:56:34.611940 24778 net.cpp:160]     with loss weight 1
I0216 15:56:34.611958 24778 net.cpp:165] Memory required for data: 36449204
I0216 15:56:34.611965 24778 net.cpp:226] loss needs backward computation.
I0216 15:56:34.611973 24778 net.cpp:226] ip2 needs backward computation.
I0216 15:56:34.611977 24778 net.cpp:226] relu4 needs backward computation.
I0216 15:56:34.611984 24778 net.cpp:226] ip1 needs backward computation.
I0216 15:56:34.611989 24778 net.cpp:226] pool3 needs backward computation.
I0216 15:56:34.611995 24778 net.cpp:226] relu3 needs backward computation.
I0216 15:56:34.612000 24778 net.cpp:226] conv3 needs backward computation.
I0216 15:56:34.612006 24778 net.cpp:226] norm2 needs backward computation.
I0216 15:56:34.612033 24778 net.cpp:226] pool2 needs backward computation.
I0216 15:56:34.612042 24778 net.cpp:226] relu2 needs backward computation.
I0216 15:56:34.612048 24778 net.cpp:226] conv2 needs backward computation.
I0216 15:56:34.612053 24778 net.cpp:226] norm1 needs backward computation.
I0216 15:56:34.612059 24778 net.cpp:226] relu1 needs backward computation.
I0216 15:56:34.612066 24778 net.cpp:226] pool1 needs backward computation.
I0216 15:56:34.612071 24778 net.cpp:226] conv1 needs backward computation.
I0216 15:56:34.612076 24778 net.cpp:228] cifar does not need backward computation.
I0216 15:56:34.612082 24778 net.cpp:270] This network produces output loss
I0216 15:56:34.612099 24778 net.cpp:283] Network initialization done.
I0216 15:56:34.612563 24778 solver.cpp:181] Creating test net (#0) specified by net file: examples/cifar10/cifar10_full_train_test.prototxt
I0216 15:56:34.612598 24778 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0216 15:56:34.612710 24778 net.cpp:49] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0216 15:56:34.612795 24778 layer_factory.hpp:77] Creating layer cifar
I0216 15:56:34.612893 24778 net.cpp:106] Creating Layer cifar
I0216 15:56:34.612915 24778 net.cpp:411] cifar -> data
I0216 15:56:34.612926 24778 net.cpp:411] cifar -> label
I0216 15:56:34.612934 24778 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0216 15:56:34.613965 24784 db_lmdb.cpp:38] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0216 15:56:34.614073 24778 data_layer.cpp:41] output data size: 100,3,32,32
I0216 15:56:34.617583 24778 net.cpp:150] Setting up cifar
I0216 15:56:34.617621 24778 net.cpp:157] Top shape: 100 3 32 32 (307200)
I0216 15:56:34.617632 24778 net.cpp:157] Top shape: 100 (100)
I0216 15:56:34.617640 24778 net.cpp:165] Memory required for data: 1229200
I0216 15:56:34.617648 24778 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0216 15:56:34.617665 24778 net.cpp:106] Creating Layer label_cifar_1_split
I0216 15:56:34.617671 24778 net.cpp:454] label_cifar_1_split <- label
I0216 15:56:34.617682 24778 net.cpp:411] label_cifar_1_split -> label_cifar_1_split_0
I0216 15:56:34.617697 24778 net.cpp:411] label_cifar_1_split -> label_cifar_1_split_1
I0216 15:56:34.617756 24778 net.cpp:150] Setting up label_cifar_1_split
I0216 15:56:34.617769 24778 net.cpp:157] Top shape: 100 (100)
I0216 15:56:34.617775 24778 net.cpp:157] Top shape: 100 (100)
I0216 15:56:34.617780 24778 net.cpp:165] Memory required for data: 1230000
I0216 15:56:34.617785 24778 layer_factory.hpp:77] Creating layer conv1
I0216 15:56:34.617801 24778 net.cpp:106] Creating Layer conv1
I0216 15:56:34.617808 24778 net.cpp:454] conv1 <- data
I0216 15:56:34.617818 24778 net.cpp:411] conv1 -> conv1
I0216 15:56:34.621101 24778 net.cpp:150] Setting up conv1
I0216 15:56:34.621132 24778 net.cpp:157] Top shape: 100 32 32 32 (3276800)
I0216 15:56:34.621140 24778 net.cpp:165] Memory required for data: 14337200
I0216 15:56:34.621158 24778 layer_factory.hpp:77] Creating layer pool1
I0216 15:56:34.621175 24778 net.cpp:106] Creating Layer pool1
I0216 15:56:34.621183 24778 net.cpp:454] pool1 <- conv1
I0216 15:56:34.621201 24778 net.cpp:411] pool1 -> pool1
I0216 15:56:34.622095 24778 net.cpp:150] Setting up pool1
I0216 15:56:34.622113 24778 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 15:56:34.622124 24778 net.cpp:165] Memory required for data: 17614000
I0216 15:56:34.622131 24778 layer_factory.hpp:77] Creating layer relu1
I0216 15:56:34.622145 24778 net.cpp:106] Creating Layer relu1
I0216 15:56:34.622153 24778 net.cpp:454] relu1 <- pool1
I0216 15:56:34.622161 24778 net.cpp:397] relu1 -> pool1 (in-place)
I0216 15:56:34.622974 24778 net.cpp:150] Setting up relu1
I0216 15:56:34.622997 24778 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 15:56:34.623003 24778 net.cpp:165] Memory required for data: 20890800
I0216 15:56:34.623010 24778 layer_factory.hpp:77] Creating layer norm1
I0216 15:56:34.623024 24778 net.cpp:106] Creating Layer norm1
I0216 15:56:34.623033 24778 net.cpp:454] norm1 <- pool1
I0216 15:56:34.623042 24778 net.cpp:411] norm1 -> norm1
I0216 15:56:34.624457 24778 net.cpp:150] Setting up norm1
I0216 15:56:34.624490 24778 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 15:56:34.624497 24778 net.cpp:165] Memory required for data: 24167600
I0216 15:56:34.624505 24778 layer_factory.hpp:77] Creating layer conv2
I0216 15:56:34.624519 24778 net.cpp:106] Creating Layer conv2
I0216 15:56:34.624527 24778 net.cpp:454] conv2 <- norm1
I0216 15:56:34.624541 24778 net.cpp:411] conv2 -> conv2
I0216 15:56:34.627919 24778 net.cpp:150] Setting up conv2
I0216 15:56:34.627965 24778 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 15:56:34.627974 24778 net.cpp:165] Memory required for data: 27444400
I0216 15:56:34.627995 24778 layer_factory.hpp:77] Creating layer relu2
I0216 15:56:34.628021 24778 net.cpp:106] Creating Layer relu2
I0216 15:56:34.628041 24778 net.cpp:454] relu2 <- conv2
I0216 15:56:34.628051 24778 net.cpp:397] relu2 -> conv2 (in-place)
I0216 15:56:34.628855 24778 net.cpp:150] Setting up relu2
I0216 15:56:34.628873 24778 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 15:56:34.628877 24778 net.cpp:165] Memory required for data: 30721200
I0216 15:56:34.628881 24778 layer_factory.hpp:77] Creating layer pool2
I0216 15:56:34.628891 24778 net.cpp:106] Creating Layer pool2
I0216 15:56:34.628896 24778 net.cpp:454] pool2 <- conv2
I0216 15:56:34.628901 24778 net.cpp:411] pool2 -> pool2
I0216 15:56:34.630256 24778 net.cpp:150] Setting up pool2
I0216 15:56:34.630285 24778 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0216 15:56:34.630292 24778 net.cpp:165] Memory required for data: 31540400
I0216 15:56:34.630300 24778 layer_factory.hpp:77] Creating layer norm2
I0216 15:56:34.630314 24778 net.cpp:106] Creating Layer norm2
I0216 15:56:34.630321 24778 net.cpp:454] norm2 <- pool2
I0216 15:56:34.630331 24778 net.cpp:411] norm2 -> norm2
I0216 15:56:34.631853 24778 net.cpp:150] Setting up norm2
I0216 15:56:34.631871 24778 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0216 15:56:34.631875 24778 net.cpp:165] Memory required for data: 32359600
I0216 15:56:34.631880 24778 layer_factory.hpp:77] Creating layer conv3
I0216 15:56:34.631891 24778 net.cpp:106] Creating Layer conv3
I0216 15:56:34.631896 24778 net.cpp:454] conv3 <- norm2
I0216 15:56:34.631903 24778 net.cpp:411] conv3 -> conv3
I0216 15:56:34.635768 24778 net.cpp:150] Setting up conv3
I0216 15:56:34.635787 24778 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0216 15:56:34.635792 24778 net.cpp:165] Memory required for data: 33998000
I0216 15:56:34.635804 24778 layer_factory.hpp:77] Creating layer relu3
I0216 15:56:34.635812 24778 net.cpp:106] Creating Layer relu3
I0216 15:56:34.635817 24778 net.cpp:454] relu3 <- conv3
I0216 15:56:34.635823 24778 net.cpp:397] relu3 -> conv3 (in-place)
I0216 15:56:34.636932 24778 net.cpp:150] Setting up relu3
I0216 15:56:34.636948 24778 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0216 15:56:34.636952 24778 net.cpp:165] Memory required for data: 35636400
I0216 15:56:34.636957 24778 layer_factory.hpp:77] Creating layer pool3
I0216 15:56:34.636965 24778 net.cpp:106] Creating Layer pool3
I0216 15:56:34.636970 24778 net.cpp:454] pool3 <- conv3
I0216 15:56:34.636977 24778 net.cpp:411] pool3 -> pool3
I0216 15:56:34.638295 24778 net.cpp:150] Setting up pool3
I0216 15:56:34.638322 24778 net.cpp:157] Top shape: 100 64 4 4 (102400)
I0216 15:56:34.638330 24778 net.cpp:165] Memory required for data: 36046000
I0216 15:56:34.638337 24778 layer_factory.hpp:77] Creating layer ip1
I0216 15:56:34.638353 24778 net.cpp:106] Creating Layer ip1
I0216 15:56:34.638360 24778 net.cpp:454] ip1 <- pool3
I0216 15:56:34.638373 24778 net.cpp:411] ip1 -> ip1
I0216 15:56:34.655282 24778 net.cpp:150] Setting up ip1
I0216 15:56:34.655326 24778 net.cpp:157] Top shape: 100 500 (50000)
I0216 15:56:34.655333 24778 net.cpp:165] Memory required for data: 36246000
I0216 15:56:34.655347 24778 layer_factory.hpp:77] Creating layer relu4
I0216 15:56:34.655360 24778 net.cpp:106] Creating Layer relu4
I0216 15:56:34.655367 24778 net.cpp:454] relu4 <- ip1
I0216 15:56:34.655377 24778 net.cpp:397] relu4 -> ip1 (in-place)
I0216 15:56:34.656343 24778 net.cpp:150] Setting up relu4
I0216 15:56:34.656361 24778 net.cpp:157] Top shape: 100 500 (50000)
I0216 15:56:34.656369 24778 net.cpp:165] Memory required for data: 36446000
I0216 15:56:34.656375 24778 layer_factory.hpp:77] Creating layer ip2
I0216 15:56:34.656394 24778 net.cpp:106] Creating Layer ip2
I0216 15:56:34.656402 24778 net.cpp:454] ip2 <- ip1
I0216 15:56:34.656412 24778 net.cpp:411] ip2 -> ip2
I0216 15:56:34.656745 24778 net.cpp:150] Setting up ip2
I0216 15:56:34.656757 24778 net.cpp:157] Top shape: 100 10 (1000)
I0216 15:56:34.656764 24778 net.cpp:165] Memory required for data: 36450000
I0216 15:56:34.656780 24778 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0216 15:56:34.656792 24778 net.cpp:106] Creating Layer ip2_ip2_0_split
I0216 15:56:34.656808 24778 net.cpp:454] ip2_ip2_0_split <- ip2
I0216 15:56:34.656827 24778 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0216 15:56:34.656837 24778 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0216 15:56:34.656889 24778 net.cpp:150] Setting up ip2_ip2_0_split
I0216 15:56:34.656899 24778 net.cpp:157] Top shape: 100 10 (1000)
I0216 15:56:34.656906 24778 net.cpp:157] Top shape: 100 10 (1000)
I0216 15:56:34.656913 24778 net.cpp:165] Memory required for data: 36458000
I0216 15:56:34.656918 24778 layer_factory.hpp:77] Creating layer accuracy
I0216 15:56:34.656929 24778 net.cpp:106] Creating Layer accuracy
I0216 15:56:34.656935 24778 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I0216 15:56:34.656942 24778 net.cpp:454] accuracy <- label_cifar_1_split_0
I0216 15:56:34.656950 24778 net.cpp:411] accuracy -> accuracy
I0216 15:56:34.656963 24778 net.cpp:150] Setting up accuracy
I0216 15:56:34.656973 24778 net.cpp:157] Top shape: (1)
I0216 15:56:34.656980 24778 net.cpp:165] Memory required for data: 36458004
I0216 15:56:34.656985 24778 layer_factory.hpp:77] Creating layer loss
I0216 15:56:34.656993 24778 net.cpp:106] Creating Layer loss
I0216 15:56:34.656999 24778 net.cpp:454] loss <- ip2_ip2_0_split_1
I0216 15:56:34.657006 24778 net.cpp:454] loss <- label_cifar_1_split_1
I0216 15:56:34.657014 24778 net.cpp:411] loss -> loss
I0216 15:56:34.657027 24778 layer_factory.hpp:77] Creating layer loss
I0216 15:56:34.658174 24778 net.cpp:150] Setting up loss
I0216 15:56:34.658197 24778 net.cpp:157] Top shape: (1)
I0216 15:56:34.658206 24778 net.cpp:160]     with loss weight 1
I0216 15:56:34.658217 24778 net.cpp:165] Memory required for data: 36458008
I0216 15:56:34.658224 24778 net.cpp:226] loss needs backward computation.
I0216 15:56:34.658231 24778 net.cpp:228] accuracy does not need backward computation.
I0216 15:56:34.658237 24778 net.cpp:226] ip2_ip2_0_split needs backward computation.
I0216 15:56:34.658243 24778 net.cpp:226] ip2 needs backward computation.
I0216 15:56:34.658249 24778 net.cpp:226] relu4 needs backward computation.
I0216 15:56:34.658254 24778 net.cpp:226] ip1 needs backward computation.
I0216 15:56:34.658259 24778 net.cpp:226] pool3 needs backward computation.
I0216 15:56:34.658265 24778 net.cpp:226] relu3 needs backward computation.
I0216 15:56:34.658272 24778 net.cpp:226] conv3 needs backward computation.
I0216 15:56:34.658277 24778 net.cpp:226] norm2 needs backward computation.
I0216 15:56:34.658282 24778 net.cpp:226] pool2 needs backward computation.
I0216 15:56:34.658288 24778 net.cpp:226] relu2 needs backward computation.
I0216 15:56:34.658293 24778 net.cpp:226] conv2 needs backward computation.
I0216 15:56:34.658299 24778 net.cpp:226] norm1 needs backward computation.
I0216 15:56:34.658304 24778 net.cpp:226] relu1 needs backward computation.
I0216 15:56:34.658309 24778 net.cpp:226] pool1 needs backward computation.
I0216 15:56:34.658315 24778 net.cpp:226] conv1 needs backward computation.
I0216 15:56:34.658321 24778 net.cpp:228] label_cifar_1_split does not need backward computation.
I0216 15:56:34.658330 24778 net.cpp:228] cifar does not need backward computation.
I0216 15:56:34.658336 24778 net.cpp:270] This network produces output accuracy
I0216 15:56:34.658342 24778 net.cpp:270] This network produces output loss
I0216 15:56:34.658360 24778 net.cpp:283] Network initialization done.
I0216 15:56:34.658458 24778 solver.cpp:60] Solver scaffolding done.
I0216 15:56:34.658915 24778 caffe.cpp:212] Starting Optimization
I0216 15:56:34.658931 24778 solver.cpp:288] Solving CIFAR10_full
I0216 15:56:34.658936 24778 solver.cpp:289] Learning Rate Policy: fixed
I0216 15:56:34.659596 24778 solver.cpp:341] Iteration 0, Testing net (#0)
I0216 15:56:35.785326 24778 solver.cpp:409]     Test net output #0: accuracy = 0.0756
I0216 15:56:35.785372 24778 solver.cpp:409]     Test net output #1: loss = 2.30259 (* 1 = 2.30259 loss)
I0216 15:56:35.807693 24778 solver.cpp:237] Iteration 0, loss = 2.30259
I0216 15:56:35.807744 24778 solver.cpp:253]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0216 15:56:35.807765 24778 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I0216 15:56:41.007802 24778 solver.cpp:237] Iteration 200, loss = 2.29903
I0216 15:56:41.007874 24778 solver.cpp:253]     Train net output #0: loss = 2.29903 (* 1 = 2.29903 loss)
I0216 15:56:41.007889 24778 sgd_solver.cpp:106] Iteration 200, lr = 0.001
I0216 15:56:46.006050 24778 solver.cpp:237] Iteration 400, loss = 2.07065
I0216 15:56:46.006103 24778 solver.cpp:253]     Train net output #0: loss = 2.07065 (* 1 = 2.07065 loss)
I0216 15:56:46.006114 24778 sgd_solver.cpp:106] Iteration 400, lr = 0.001
I0216 15:56:51.283447 24778 solver.cpp:237] Iteration 600, loss = 1.97786
I0216 15:56:51.283491 24778 solver.cpp:253]     Train net output #0: loss = 1.97786 (* 1 = 1.97786 loss)
I0216 15:56:51.283499 24778 sgd_solver.cpp:106] Iteration 600, lr = 0.001
I0216 15:56:56.594043 24778 solver.cpp:237] Iteration 800, loss = 1.66389
I0216 15:56:56.594352 24778 solver.cpp:253]     Train net output #0: loss = 1.66389 (* 1 = 1.66389 loss)
I0216 15:56:56.594365 24778 sgd_solver.cpp:106] Iteration 800, lr = 0.001
I0216 15:57:01.585892 24778 solver.cpp:341] Iteration 1000, Testing net (#0)
I0216 15:57:02.840562 24778 solver.cpp:409]     Test net output #0: accuracy = 0.3955
I0216 15:57:02.840622 24778 solver.cpp:409]     Test net output #1: loss = 1.6689 (* 1 = 1.6689 loss)
I0216 15:57:02.851779 24778 solver.cpp:237] Iteration 1000, loss = 1.77497
I0216 15:57:02.851845 24778 solver.cpp:253]     Train net output #0: loss = 1.77497 (* 1 = 1.77497 loss)
I0216 15:57:02.851857 24778 sgd_solver.cpp:106] Iteration 1000, lr = 0.001
I0216 15:57:07.415374 24778 solver.cpp:237] Iteration 1200, loss = 1.70814
I0216 15:57:07.415468 24778 solver.cpp:253]     Train net output #0: loss = 1.70814 (* 1 = 1.70814 loss)
I0216 15:57:07.415480 24778 sgd_solver.cpp:106] Iteration 1200, lr = 0.001
I0216 15:57:12.706876 24778 solver.cpp:237] Iteration 1400, loss = 1.40703
I0216 15:57:12.706943 24778 solver.cpp:253]     Train net output #0: loss = 1.40703 (* 1 = 1.40703 loss)
I0216 15:57:12.706954 24778 sgd_solver.cpp:106] Iteration 1400, lr = 0.001
I0216 15:57:17.953734 24778 solver.cpp:237] Iteration 1600, loss = 1.448
I0216 15:57:17.953801 24778 solver.cpp:253]     Train net output #0: loss = 1.448 (* 1 = 1.448 loss)
I0216 15:57:17.954044 24778 sgd_solver.cpp:106] Iteration 1600, lr = 0.001
I0216 15:57:23.232784 24778 solver.cpp:237] Iteration 1800, loss = 1.40253
I0216 15:57:23.232846 24778 solver.cpp:253]     Train net output #0: loss = 1.40253 (* 1 = 1.40253 loss)
I0216 15:57:23.233088 24778 sgd_solver.cpp:106] Iteration 1800, lr = 0.001
I0216 15:57:28.576151 24778 solver.cpp:341] Iteration 2000, Testing net (#0)
I0216 15:57:29.828171 24778 solver.cpp:409]     Test net output #0: accuracy = 0.4795
I0216 15:57:29.828227 24778 solver.cpp:409]     Test net output #1: loss = 1.43693 (* 1 = 1.43693 loss)
I0216 15:57:29.844696 24778 solver.cpp:237] Iteration 2000, loss = 1.43336
I0216 15:57:29.844761 24778 solver.cpp:253]     Train net output #0: loss = 1.43336 (* 1 = 1.43336 loss)
I0216 15:57:29.844779 24778 sgd_solver.cpp:106] Iteration 2000, lr = 0.001
I0216 15:57:34.926821 24778 solver.cpp:237] Iteration 2200, loss = 1.43974
I0216 15:57:34.926882 24778 solver.cpp:253]     Train net output #0: loss = 1.43974 (* 1 = 1.43974 loss)
I0216 15:57:34.926898 24778 sgd_solver.cpp:106] Iteration 2200, lr = 0.001
I0216 15:57:40.040594 24778 solver.cpp:237] Iteration 2400, loss = 1.19571
I0216 15:57:40.040688 24778 solver.cpp:253]     Train net output #0: loss = 1.19571 (* 1 = 1.19571 loss)
I0216 15:57:40.040952 24778 sgd_solver.cpp:106] Iteration 2400, lr = 0.001
I0216 15:57:44.719490 24778 solver.cpp:237] Iteration 2600, loss = 1.243
I0216 15:57:44.719552 24778 solver.cpp:253]     Train net output #0: loss = 1.243 (* 1 = 1.243 loss)
I0216 15:57:44.719568 24778 sgd_solver.cpp:106] Iteration 2600, lr = 0.001
I0216 15:57:49.925935 24778 solver.cpp:237] Iteration 2800, loss = 1.26873
I0216 15:57:49.925977 24778 solver.cpp:253]     Train net output #0: loss = 1.26873 (* 1 = 1.26873 loss)
I0216 15:57:49.925997 24778 sgd_solver.cpp:106] Iteration 2800, lr = 0.001
I0216 15:57:55.020297 24778 solver.cpp:341] Iteration 3000, Testing net (#0)
I0216 15:57:56.297859 24778 solver.cpp:409]     Test net output #0: accuracy = 0.5453
I0216 15:57:56.297920 24778 solver.cpp:409]     Test net output #1: loss = 1.2749 (* 1 = 1.2749 loss)
I0216 15:57:56.306205 24778 solver.cpp:237] Iteration 3000, loss = 1.22702
I0216 15:57:56.306257 24778 solver.cpp:253]     Train net output #0: loss = 1.22702 (* 1 = 1.22702 loss)
I0216 15:57:56.306272 24778 sgd_solver.cpp:106] Iteration 3000, lr = 0.001
I0216 15:58:01.556462 24778 solver.cpp:237] Iteration 3200, loss = 1.26819
I0216 15:58:01.556519 24778 solver.cpp:253]     Train net output #0: loss = 1.26819 (* 1 = 1.26819 loss)
I0216 15:58:01.556797 24778 sgd_solver.cpp:106] Iteration 3200, lr = 0.001
I0216 15:58:06.632557 24778 solver.cpp:237] Iteration 3400, loss = 1.04158
I0216 15:58:06.632616 24778 solver.cpp:253]     Train net output #0: loss = 1.04158 (* 1 = 1.04158 loss)
I0216 15:58:06.632861 24778 sgd_solver.cpp:106] Iteration 3400, lr = 0.001
I0216 15:58:11.845281 24778 solver.cpp:237] Iteration 3600, loss = 1.12517
I0216 15:58:11.845423 24778 solver.cpp:253]     Train net output #0: loss = 1.12517 (* 1 = 1.12517 loss)
I0216 15:58:11.845813 24778 sgd_solver.cpp:106] Iteration 3600, lr = 0.001
I0216 15:58:16.860147 24778 solver.cpp:237] Iteration 3800, loss = 1.11624
I0216 15:58:16.860193 24778 solver.cpp:253]     Train net output #0: loss = 1.11624 (* 1 = 1.11624 loss)
I0216 15:58:16.860479 24778 sgd_solver.cpp:106] Iteration 3800, lr = 0.001
I0216 15:58:21.134421 24778 solver.cpp:341] Iteration 4000, Testing net (#0)
I0216 15:58:22.339422 24778 solver.cpp:409]     Test net output #0: accuracy = 0.5866
I0216 15:58:22.339488 24778 solver.cpp:409]     Test net output #1: loss = 1.15546 (* 1 = 1.15546 loss)
I0216 15:58:22.351958 24778 solver.cpp:237] Iteration 4000, loss = 1.05359
I0216 15:58:22.352025 24778 solver.cpp:253]     Train net output #0: loss = 1.05359 (* 1 = 1.05359 loss)
I0216 15:58:22.352042 24778 sgd_solver.cpp:106] Iteration 4000, lr = 0.001
I0216 15:58:27.641402 24778 solver.cpp:237] Iteration 4200, loss = 1.14887
I0216 15:58:27.641472 24778 solver.cpp:253]     Train net output #0: loss = 1.14887 (* 1 = 1.14887 loss)
I0216 15:58:27.641717 24778 sgd_solver.cpp:106] Iteration 4200, lr = 0.001
I0216 15:58:32.988554 24778 solver.cpp:237] Iteration 4400, loss = 0.969618
I0216 15:58:32.988618 24778 solver.cpp:253]     Train net output #0: loss = 0.969618 (* 1 = 0.969618 loss)
I0216 15:58:32.988867 24778 sgd_solver.cpp:106] Iteration 4400, lr = 0.001
I0216 15:58:38.213682 24778 solver.cpp:237] Iteration 4600, loss = 1.01022
I0216 15:58:38.213757 24778 solver.cpp:253]     Train net output #0: loss = 1.01022 (* 1 = 1.01022 loss)
I0216 15:58:38.214011 24778 sgd_solver.cpp:106] Iteration 4600, lr = 0.001
I0216 15:58:43.501641 24778 solver.cpp:237] Iteration 4800, loss = 0.960236
I0216 15:58:43.501727 24778 solver.cpp:253]     Train net output #0: loss = 0.960236 (* 1 = 0.960236 loss)
I0216 15:58:43.501737 24778 sgd_solver.cpp:106] Iteration 4800, lr = 0.001
I0216 15:58:48.730556 24778 solver.cpp:341] Iteration 5000, Testing net (#0)
I0216 15:58:50.008909 24778 solver.cpp:409]     Test net output #0: accuracy = 0.6155
I0216 15:58:50.008966 24778 solver.cpp:409]     Test net output #1: loss = 1.06979 (* 1 = 1.06979 loss)
I0216 15:58:50.016468 24778 solver.cpp:237] Iteration 5000, loss = 0.942693
I0216 15:58:50.016523 24778 solver.cpp:253]     Train net output #0: loss = 0.942693 (* 1 = 0.942693 loss)
I0216 15:58:50.016540 24778 sgd_solver.cpp:106] Iteration 5000, lr = 0.001
I0216 15:58:54.559618 24778 solver.cpp:237] Iteration 5200, loss = 1.0416
I0216 15:58:54.559659 24778 solver.cpp:253]     Train net output #0: loss = 1.0416 (* 1 = 1.0416 loss)
I0216 15:58:54.559666 24778 sgd_solver.cpp:106] Iteration 5200, lr = 0.001
I0216 15:58:59.473752 24778 solver.cpp:237] Iteration 5400, loss = 0.895529
I0216 15:58:59.473811 24778 solver.cpp:253]     Train net output #0: loss = 0.895529 (* 1 = 0.895529 loss)
I0216 15:58:59.473839 24778 sgd_solver.cpp:106] Iteration 5400, lr = 0.001
I0216 15:59:04.804267 24778 solver.cpp:237] Iteration 5600, loss = 0.936118
I0216 15:59:04.804318 24778 solver.cpp:253]     Train net output #0: loss = 0.936118 (* 1 = 0.936118 loss)
I0216 15:59:04.804591 24778 sgd_solver.cpp:106] Iteration 5600, lr = 0.001
I0216 15:59:10.018409 24778 solver.cpp:237] Iteration 5800, loss = 0.837666
I0216 15:59:10.018476 24778 solver.cpp:253]     Train net output #0: loss = 0.837666 (* 1 = 0.837666 loss)
I0216 15:59:10.021147 24778 sgd_solver.cpp:106] Iteration 5800, lr = 0.001
I0216 15:59:15.281388 24778 solver.cpp:341] Iteration 6000, Testing net (#0)
I0216 15:59:16.584791 24778 solver.cpp:409]     Test net output #0: accuracy = 0.6391
I0216 15:59:16.584851 24778 solver.cpp:409]     Test net output #1: loss = 1.01612 (* 1 = 1.01612 loss)
I0216 15:59:16.602396 24778 solver.cpp:237] Iteration 6000, loss = 0.85195
I0216 15:59:16.602438 24778 solver.cpp:253]     Train net output #0: loss = 0.85195 (* 1 = 0.85195 loss)
I0216 15:59:16.602447 24778 sgd_solver.cpp:106] Iteration 6000, lr = 0.001
I0216 15:59:21.920908 24778 solver.cpp:237] Iteration 6200, loss = 0.961721
I0216 15:59:21.920958 24778 solver.cpp:253]     Train net output #0: loss = 0.961721 (* 1 = 0.961721 loss)
I0216 15:59:21.921279 24778 sgd_solver.cpp:106] Iteration 6200, lr = 0.001
I0216 15:59:27.110462 24778 solver.cpp:237] Iteration 6400, loss = 0.817498
I0216 15:59:27.110527 24778 solver.cpp:253]     Train net output #0: loss = 0.817498 (* 1 = 0.817498 loss)
I0216 15:59:27.112146 24778 sgd_solver.cpp:106] Iteration 6400, lr = 0.001
I0216 15:59:31.459516 24778 solver.cpp:237] Iteration 6600, loss = 0.843983
I0216 15:59:31.459570 24778 solver.cpp:253]     Train net output #0: loss = 0.843983 (* 1 = 0.843983 loss)
I0216 15:59:31.459581 24778 sgd_solver.cpp:106] Iteration 6600, lr = 0.001
I0216 15:59:36.797675 24778 solver.cpp:237] Iteration 6800, loss = 0.754951
I0216 15:59:36.797741 24778 solver.cpp:253]     Train net output #0: loss = 0.754951 (* 1 = 0.754951 loss)
I0216 15:59:36.797754 24778 sgd_solver.cpp:106] Iteration 6800, lr = 0.001
I0216 15:59:41.810674 24778 solver.cpp:341] Iteration 7000, Testing net (#0)
I0216 15:59:43.153221 24778 solver.cpp:409]     Test net output #0: accuracy = 0.6546
I0216 15:59:43.153285 24778 solver.cpp:409]     Test net output #1: loss = 0.972945 (* 1 = 0.972945 loss)
I0216 15:59:43.170073 24778 solver.cpp:237] Iteration 7000, loss = 0.795188
I0216 15:59:43.170131 24778 solver.cpp:253]     Train net output #0: loss = 0.795188 (* 1 = 0.795188 loss)
I0216 15:59:43.170142 24778 sgd_solver.cpp:106] Iteration 7000, lr = 0.001
I0216 15:59:48.406529 24778 solver.cpp:237] Iteration 7200, loss = 0.864482
I0216 15:59:48.406611 24778 solver.cpp:253]     Train net output #0: loss = 0.864482 (* 1 = 0.864482 loss)
I0216 15:59:48.406623 24778 sgd_solver.cpp:106] Iteration 7200, lr = 0.001
I0216 15:59:53.223129 24778 solver.cpp:237] Iteration 7400, loss = 0.74151
I0216 15:59:53.223189 24778 solver.cpp:253]     Train net output #0: loss = 0.74151 (* 1 = 0.74151 loss)
I0216 15:59:53.223206 24778 sgd_solver.cpp:106] Iteration 7400, lr = 0.001
I0216 15:59:58.571923 24778 solver.cpp:237] Iteration 7600, loss = 0.75975
I0216 15:59:58.571987 24778 solver.cpp:253]     Train net output #0: loss = 0.75975 (* 1 = 0.75975 loss)
I0216 15:59:58.572003 24778 sgd_solver.cpp:106] Iteration 7600, lr = 0.001
I0216 16:00:03.815891 24778 solver.cpp:237] Iteration 7800, loss = 0.700464
I0216 16:00:03.815939 24778 solver.cpp:253]     Train net output #0: loss = 0.700464 (* 1 = 0.700464 loss)
I0216 16:00:03.816231 24778 sgd_solver.cpp:106] Iteration 7800, lr = 0.001
I0216 16:00:08.164482 24778 solver.cpp:341] Iteration 8000, Testing net (#0)
I0216 16:00:09.640167 24778 solver.cpp:409]     Test net output #0: accuracy = 0.6718
I0216 16:00:09.640225 24778 solver.cpp:409]     Test net output #1: loss = 0.929749 (* 1 = 0.929749 loss)
I0216 16:00:09.653506 24778 solver.cpp:237] Iteration 8000, loss = 0.748254
I0216 16:00:09.653580 24778 solver.cpp:253]     Train net output #0: loss = 0.748254 (* 1 = 0.748254 loss)
I0216 16:00:09.653595 24778 sgd_solver.cpp:106] Iteration 8000, lr = 0.001
I0216 16:00:14.791254 24778 solver.cpp:237] Iteration 8200, loss = 0.782381
I0216 16:00:14.791314 24778 solver.cpp:253]     Train net output #0: loss = 0.782381 (* 1 = 0.782381 loss)
I0216 16:00:14.791728 24778 sgd_solver.cpp:106] Iteration 8200, lr = 0.001
I0216 16:00:20.080004 24778 solver.cpp:237] Iteration 8400, loss = 0.693492
I0216 16:00:20.080134 24778 solver.cpp:253]     Train net output #0: loss = 0.693492 (* 1 = 0.693492 loss)
I0216 16:00:20.080509 24778 sgd_solver.cpp:106] Iteration 8400, lr = 0.001
I0216 16:00:25.298174 24778 solver.cpp:237] Iteration 8600, loss = 0.688114
I0216 16:00:25.298219 24778 solver.cpp:253]     Train net output #0: loss = 0.688114 (* 1 = 0.688114 loss)
I0216 16:00:25.298228 24778 sgd_solver.cpp:106] Iteration 8600, lr = 0.001
I0216 16:00:30.472139 24778 solver.cpp:237] Iteration 8800, loss = 0.658855
I0216 16:00:30.472203 24778 solver.cpp:253]     Train net output #0: loss = 0.658855 (* 1 = 0.658855 loss)
I0216 16:00:30.472446 24778 sgd_solver.cpp:106] Iteration 8800, lr = 0.001
I0216 16:00:35.704588 24778 solver.cpp:341] Iteration 9000, Testing net (#0)
I0216 16:00:36.920383 24778 solver.cpp:409]     Test net output #0: accuracy = 0.6862
I0216 16:00:36.920428 24778 solver.cpp:409]     Test net output #1: loss = 0.891661 (* 1 = 0.891661 loss)
I0216 16:00:36.928614 24778 solver.cpp:237] Iteration 9000, loss = 0.705137
I0216 16:00:36.928656 24778 solver.cpp:253]     Train net output #0: loss = 0.705137 (* 1 = 0.705137 loss)
I0216 16:00:36.928663 24778 sgd_solver.cpp:106] Iteration 9000, lr = 0.001
I0216 16:00:41.959624 24778 solver.cpp:237] Iteration 9200, loss = 0.724066
I0216 16:00:41.959691 24778 solver.cpp:253]     Train net output #0: loss = 0.724066 (* 1 = 0.724066 loss)
I0216 16:00:41.959988 24778 sgd_solver.cpp:106] Iteration 9200, lr = 0.001
I0216 16:00:46.556989 24778 solver.cpp:237] Iteration 9400, loss = 0.649651
I0216 16:00:46.557050 24778 solver.cpp:253]     Train net output #0: loss = 0.649651 (* 1 = 0.649651 loss)
I0216 16:00:46.557067 24778 sgd_solver.cpp:106] Iteration 9400, lr = 0.001
I0216 16:00:51.855450 24778 solver.cpp:237] Iteration 9600, loss = 0.637964
I0216 16:00:51.855526 24778 solver.cpp:253]     Train net output #0: loss = 0.637964 (* 1 = 0.637964 loss)
I0216 16:00:51.855535 24778 sgd_solver.cpp:106] Iteration 9600, lr = 0.001
I0216 16:00:57.250253 24778 solver.cpp:237] Iteration 9800, loss = 0.615206
I0216 16:00:57.250315 24778 solver.cpp:253]     Train net output #0: loss = 0.615206 (* 1 = 0.615206 loss)
I0216 16:00:57.250594 24778 sgd_solver.cpp:106] Iteration 9800, lr = 0.001
I0216 16:01:02.392199 24778 solver.cpp:472] Snapshotting to HDF5 file examples/cifar10/cifar10_full_iter_10000.caffemodel.h5
I0216 16:01:03.090862 24778 sgd_solver.cpp:279] Snapshotting solver state to HDF5 file examples/cifar10/cifar10_full_iter_10000.solverstate.h5
I0216 16:01:03.095516 24778 solver.cpp:341] Iteration 10000, Testing net (#0)
I0216 16:01:04.230809 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7004
I0216 16:01:04.230861 24778 solver.cpp:409]     Test net output #1: loss = 0.852197 (* 1 = 0.852197 loss)
I0216 16:01:04.244460 24778 solver.cpp:237] Iteration 10000, loss = 0.660497
I0216 16:01:04.244519 24778 solver.cpp:253]     Train net output #0: loss = 0.660497 (* 1 = 0.660497 loss)
I0216 16:01:04.244532 24778 sgd_solver.cpp:106] Iteration 10000, lr = 0.001
I0216 16:01:09.483274 24778 solver.cpp:237] Iteration 10200, loss = 0.698569
I0216 16:01:09.483341 24778 solver.cpp:253]     Train net output #0: loss = 0.698569 (* 1 = 0.698569 loss)
I0216 16:01:09.483360 24778 sgd_solver.cpp:106] Iteration 10200, lr = 0.001
I0216 16:01:14.834050 24778 solver.cpp:237] Iteration 10400, loss = 0.637946
I0216 16:01:14.834138 24778 solver.cpp:253]     Train net output #0: loss = 0.637946 (* 1 = 0.637946 loss)
I0216 16:01:14.834393 24778 sgd_solver.cpp:106] Iteration 10400, lr = 0.001
I0216 16:01:19.310472 24778 solver.cpp:237] Iteration 10600, loss = 0.606706
I0216 16:01:19.310525 24778 solver.cpp:253]     Train net output #0: loss = 0.606706 (* 1 = 0.606706 loss)
I0216 16:01:19.310534 24778 sgd_solver.cpp:106] Iteration 10600, lr = 0.001
I0216 16:01:24.522339 24778 solver.cpp:237] Iteration 10800, loss = 0.585293
I0216 16:01:24.522456 24778 solver.cpp:253]     Train net output #0: loss = 0.585293 (* 1 = 0.585293 loss)
I0216 16:01:24.522475 24778 sgd_solver.cpp:106] Iteration 10800, lr = 0.001
I0216 16:01:29.831269 24778 solver.cpp:341] Iteration 11000, Testing net (#0)
I0216 16:01:31.132434 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7128
I0216 16:01:31.132478 24778 solver.cpp:409]     Test net output #1: loss = 0.822145 (* 1 = 0.822145 loss)
I0216 16:01:31.140925 24778 solver.cpp:237] Iteration 11000, loss = 0.624565
I0216 16:01:31.140965 24778 solver.cpp:253]     Train net output #0: loss = 0.624565 (* 1 = 0.624565 loss)
I0216 16:01:31.140972 24778 sgd_solver.cpp:106] Iteration 11000, lr = 0.001
I0216 16:01:36.470831 24778 solver.cpp:237] Iteration 11200, loss = 0.688729
I0216 16:01:36.470895 24778 solver.cpp:253]     Train net output #0: loss = 0.688729 (* 1 = 0.688729 loss)
I0216 16:01:36.470911 24778 sgd_solver.cpp:106] Iteration 11200, lr = 0.001
I0216 16:01:41.677686 24778 solver.cpp:237] Iteration 11400, loss = 0.62612
I0216 16:01:41.677752 24778 solver.cpp:253]     Train net output #0: loss = 0.62612 (* 1 = 0.62612 loss)
I0216 16:01:41.677997 24778 sgd_solver.cpp:106] Iteration 11400, lr = 0.001
I0216 16:01:46.908030 24778 solver.cpp:237] Iteration 11600, loss = 0.594931
I0216 16:01:46.908092 24778 solver.cpp:253]     Train net output #0: loss = 0.594931 (* 1 = 0.594931 loss)
I0216 16:01:46.908109 24778 sgd_solver.cpp:106] Iteration 11600, lr = 0.001
I0216 16:01:52.047693 24778 solver.cpp:237] Iteration 11800, loss = 0.571057
I0216 16:01:52.047744 24778 solver.cpp:253]     Train net output #0: loss = 0.571057 (* 1 = 0.571057 loss)
I0216 16:01:52.048023 24778 sgd_solver.cpp:106] Iteration 11800, lr = 0.001
I0216 16:01:56.596122 24778 solver.cpp:341] Iteration 12000, Testing net (#0)
I0216 16:01:57.849548 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7182
I0216 16:01:57.849603 24778 solver.cpp:409]     Test net output #1: loss = 0.803483 (* 1 = 0.803483 loss)
I0216 16:01:57.866334 24778 solver.cpp:237] Iteration 12000, loss = 0.595115
I0216 16:01:57.866396 24778 solver.cpp:253]     Train net output #0: loss = 0.595115 (* 1 = 0.595115 loss)
I0216 16:01:57.866415 24778 sgd_solver.cpp:106] Iteration 12000, lr = 0.001
I0216 16:02:02.996250 24778 solver.cpp:237] Iteration 12200, loss = 0.668613
I0216 16:02:02.996312 24778 solver.cpp:253]     Train net output #0: loss = 0.668613 (* 1 = 0.668613 loss)
I0216 16:02:02.996551 24778 sgd_solver.cpp:106] Iteration 12200, lr = 0.001
I0216 16:02:08.343850 24778 solver.cpp:237] Iteration 12400, loss = 0.603864
I0216 16:02:08.343912 24778 solver.cpp:253]     Train net output #0: loss = 0.603864 (* 1 = 0.603864 loss)
I0216 16:02:08.343929 24778 sgd_solver.cpp:106] Iteration 12400, lr = 0.001
I0216 16:02:13.655390 24778 solver.cpp:237] Iteration 12600, loss = 0.58017
I0216 16:02:13.655447 24778 solver.cpp:253]     Train net output #0: loss = 0.58017 (* 1 = 0.58017 loss)
I0216 16:02:13.655686 24778 sgd_solver.cpp:106] Iteration 12600, lr = 0.001
I0216 16:02:18.971439 24778 solver.cpp:237] Iteration 12800, loss = 0.540632
I0216 16:02:18.971498 24778 solver.cpp:253]     Train net output #0: loss = 0.540632 (* 1 = 0.540632 loss)
I0216 16:02:18.971729 24778 sgd_solver.cpp:106] Iteration 12800, lr = 0.001
I0216 16:02:24.144774 24778 solver.cpp:341] Iteration 13000, Testing net (#0)
I0216 16:02:25.359086 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7251
I0216 16:02:25.359158 24778 solver.cpp:409]     Test net output #1: loss = 0.789597 (* 1 = 0.789597 loss)
I0216 16:02:25.377339 24778 solver.cpp:237] Iteration 13000, loss = 0.582054
I0216 16:02:25.377405 24778 solver.cpp:253]     Train net output #0: loss = 0.582054 (* 1 = 0.582054 loss)
I0216 16:02:25.377444 24778 sgd_solver.cpp:106] Iteration 13000, lr = 0.001
I0216 16:02:30.325556 24778 solver.cpp:237] Iteration 13200, loss = 0.651702
I0216 16:02:30.325688 24778 solver.cpp:253]     Train net output #0: loss = 0.651702 (* 1 = 0.651702 loss)
I0216 16:02:30.325706 24778 sgd_solver.cpp:106] Iteration 13200, lr = 0.001
I0216 16:02:34.983799 24778 solver.cpp:237] Iteration 13400, loss = 0.589169
I0216 16:02:34.983865 24778 solver.cpp:253]     Train net output #0: loss = 0.589169 (* 1 = 0.589169 loss)
I0216 16:02:34.983881 24778 sgd_solver.cpp:106] Iteration 13400, lr = 0.001
I0216 16:02:40.238070 24778 solver.cpp:237] Iteration 13600, loss = 0.573136
I0216 16:02:40.238133 24778 solver.cpp:253]     Train net output #0: loss = 0.573136 (* 1 = 0.573136 loss)
I0216 16:02:40.238406 24778 sgd_solver.cpp:106] Iteration 13600, lr = 0.001
I0216 16:02:45.501691 24778 solver.cpp:237] Iteration 13800, loss = 0.513175
I0216 16:02:45.501756 24778 solver.cpp:253]     Train net output #0: loss = 0.513175 (* 1 = 0.513175 loss)
I0216 16:02:45.501767 24778 sgd_solver.cpp:106] Iteration 13800, lr = 0.001
I0216 16:02:50.650120 24778 solver.cpp:341] Iteration 14000, Testing net (#0)
I0216 16:02:51.946666 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7276
I0216 16:02:51.946710 24778 solver.cpp:409]     Test net output #1: loss = 0.786747 (* 1 = 0.786747 loss)
I0216 16:02:51.958555 24778 solver.cpp:237] Iteration 14000, loss = 0.573782
I0216 16:02:51.958612 24778 solver.cpp:253]     Train net output #0: loss = 0.573782 (* 1 = 0.573782 loss)
I0216 16:02:51.958626 24778 sgd_solver.cpp:106] Iteration 14000, lr = 0.001
I0216 16:02:57.332509 24778 solver.cpp:237] Iteration 14200, loss = 0.615643
I0216 16:02:57.332567 24778 solver.cpp:253]     Train net output #0: loss = 0.615643 (* 1 = 0.615643 loss)
I0216 16:02:57.332587 24778 sgd_solver.cpp:106] Iteration 14200, lr = 0.001
I0216 16:03:02.398308 24778 solver.cpp:237] Iteration 14400, loss = 0.564177
I0216 16:03:02.398583 24778 solver.cpp:253]     Train net output #0: loss = 0.564177 (* 1 = 0.564177 loss)
I0216 16:03:02.398599 24778 sgd_solver.cpp:106] Iteration 14400, lr = 0.001
I0216 16:03:06.815593 24778 solver.cpp:237] Iteration 14600, loss = 0.553608
I0216 16:03:06.815656 24778 solver.cpp:253]     Train net output #0: loss = 0.553608 (* 1 = 0.553608 loss)
I0216 16:03:06.815673 24778 sgd_solver.cpp:106] Iteration 14600, lr = 0.001
I0216 16:03:12.117360 24778 solver.cpp:237] Iteration 14800, loss = 0.485707
I0216 16:03:12.117424 24778 solver.cpp:253]     Train net output #0: loss = 0.485707 (* 1 = 0.485707 loss)
I0216 16:03:12.117442 24778 sgd_solver.cpp:106] Iteration 14800, lr = 0.001
I0216 16:03:17.302338 24778 solver.cpp:341] Iteration 15000, Testing net (#0)
I0216 16:03:18.656087 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7308
I0216 16:03:18.656147 24778 solver.cpp:409]     Test net output #1: loss = 0.778885 (* 1 = 0.778885 loss)
I0216 16:03:18.668701 24778 solver.cpp:237] Iteration 15000, loss = 0.540714
I0216 16:03:18.668767 24778 solver.cpp:253]     Train net output #0: loss = 0.540714 (* 1 = 0.540714 loss)
I0216 16:03:18.668786 24778 sgd_solver.cpp:106] Iteration 15000, lr = 0.001
I0216 16:03:23.760279 24778 solver.cpp:237] Iteration 15200, loss = 0.576974
I0216 16:03:23.760341 24778 solver.cpp:253]     Train net output #0: loss = 0.576974 (* 1 = 0.576974 loss)
I0216 16:03:23.760354 24778 sgd_solver.cpp:106] Iteration 15200, lr = 0.001
I0216 16:03:29.072338 24778 solver.cpp:237] Iteration 15400, loss = 0.538788
I0216 16:03:29.072386 24778 solver.cpp:253]     Train net output #0: loss = 0.538788 (* 1 = 0.538788 loss)
I0216 16:03:29.072619 24778 sgd_solver.cpp:106] Iteration 15400, lr = 0.001
I0216 16:03:34.396245 24778 solver.cpp:237] Iteration 15600, loss = 0.5226
I0216 16:03:34.396342 24778 solver.cpp:253]     Train net output #0: loss = 0.5226 (* 1 = 0.5226 loss)
I0216 16:03:34.396355 24778 sgd_solver.cpp:106] Iteration 15600, lr = 0.001
I0216 16:03:39.806418 24778 solver.cpp:237] Iteration 15800, loss = 0.451924
I0216 16:03:39.806474 24778 solver.cpp:253]     Train net output #0: loss = 0.451924 (* 1 = 0.451924 loss)
I0216 16:03:39.806733 24778 sgd_solver.cpp:106] Iteration 15800, lr = 0.001
I0216 16:03:44.393065 24778 solver.cpp:341] Iteration 16000, Testing net (#0)
I0216 16:03:45.664469 24778 solver.cpp:409]     Test net output #0: accuracy = 0.731
I0216 16:03:45.664536 24778 solver.cpp:409]     Test net output #1: loss = 0.778601 (* 1 = 0.778601 loss)
I0216 16:03:45.670747 24778 solver.cpp:237] Iteration 16000, loss = 0.521825
I0216 16:03:45.670806 24778 solver.cpp:253]     Train net output #0: loss = 0.521825 (* 1 = 0.521825 loss)
I0216 16:03:45.670819 24778 sgd_solver.cpp:106] Iteration 16000, lr = 0.001
I0216 16:03:50.904589 24778 solver.cpp:237] Iteration 16200, loss = 0.546756
I0216 16:03:50.904656 24778 solver.cpp:253]     Train net output #0: loss = 0.546756 (* 1 = 0.546756 loss)
I0216 16:03:50.904903 24778 sgd_solver.cpp:106] Iteration 16200, lr = 0.001
I0216 16:03:56.095293 24778 solver.cpp:237] Iteration 16400, loss = 0.507378
I0216 16:03:56.095360 24778 solver.cpp:253]     Train net output #0: loss = 0.507378 (* 1 = 0.507378 loss)
I0216 16:03:56.095595 24778 sgd_solver.cpp:106] Iteration 16400, lr = 0.001
I0216 16:04:01.354923 24778 solver.cpp:237] Iteration 16600, loss = 0.505853
I0216 16:04:01.354976 24778 solver.cpp:253]     Train net output #0: loss = 0.505853 (* 1 = 0.505853 loss)
I0216 16:04:01.354986 24778 sgd_solver.cpp:106] Iteration 16600, lr = 0.001
I0216 16:04:06.478006 24778 solver.cpp:237] Iteration 16800, loss = 0.43967
I0216 16:04:06.478163 24778 solver.cpp:253]     Train net output #0: loss = 0.43967 (* 1 = 0.43967 loss)
I0216 16:04:06.478611 24778 sgd_solver.cpp:106] Iteration 16800, lr = 0.001
I0216 16:04:11.650092 24778 solver.cpp:341] Iteration 17000, Testing net (#0)
I0216 16:04:12.812988 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7308
I0216 16:04:12.813042 24778 solver.cpp:409]     Test net output #1: loss = 0.787265 (* 1 = 0.787265 loss)
I0216 16:04:12.819458 24778 solver.cpp:237] Iteration 17000, loss = 0.497418
I0216 16:04:12.819519 24778 solver.cpp:253]     Train net output #0: loss = 0.497418 (* 1 = 0.497418 loss)
I0216 16:04:12.819535 24778 sgd_solver.cpp:106] Iteration 17000, lr = 0.001
I0216 16:04:17.886782 24778 solver.cpp:237] Iteration 17200, loss = 0.508313
I0216 16:04:17.886845 24778 solver.cpp:253]     Train net output #0: loss = 0.508313 (* 1 = 0.508313 loss)
I0216 16:04:17.886863 24778 sgd_solver.cpp:106] Iteration 17200, lr = 0.001
I0216 16:04:22.806051 24778 solver.cpp:237] Iteration 17400, loss = 0.484363
I0216 16:04:22.806107 24778 solver.cpp:253]     Train net output #0: loss = 0.484363 (* 1 = 0.484363 loss)
I0216 16:04:22.806118 24778 sgd_solver.cpp:106] Iteration 17400, lr = 0.001
I0216 16:04:27.941251 24778 solver.cpp:237] Iteration 17600, loss = 0.485211
I0216 16:04:27.941318 24778 solver.cpp:253]     Train net output #0: loss = 0.485211 (* 1 = 0.485211 loss)
I0216 16:04:27.941334 24778 sgd_solver.cpp:106] Iteration 17600, lr = 0.001
I0216 16:04:33.239256 24778 solver.cpp:237] Iteration 17800, loss = 0.426248
I0216 16:04:33.239326 24778 solver.cpp:253]     Train net output #0: loss = 0.426248 (* 1 = 0.426248 loss)
I0216 16:04:33.239599 24778 sgd_solver.cpp:106] Iteration 17800, lr = 0.001
I0216 16:04:38.546835 24778 solver.cpp:341] Iteration 18000, Testing net (#0)
I0216 16:04:39.801316 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7342
I0216 16:04:39.801375 24778 solver.cpp:409]     Test net output #1: loss = 0.771451 (* 1 = 0.771451 loss)
I0216 16:04:39.817529 24778 solver.cpp:237] Iteration 18000, loss = 0.461028
I0216 16:04:39.817589 24778 solver.cpp:253]     Train net output #0: loss = 0.461028 (* 1 = 0.461028 loss)
I0216 16:04:39.817606 24778 sgd_solver.cpp:106] Iteration 18000, lr = 0.001
I0216 16:04:45.007272 24778 solver.cpp:237] Iteration 18200, loss = 0.490295
I0216 16:04:45.007328 24778 solver.cpp:253]     Train net output #0: loss = 0.490295 (* 1 = 0.490295 loss)
I0216 16:04:45.007341 24778 sgd_solver.cpp:106] Iteration 18200, lr = 0.001
I0216 16:04:50.360610 24778 solver.cpp:237] Iteration 18400, loss = 0.453986
I0216 16:04:50.360695 24778 solver.cpp:253]     Train net output #0: loss = 0.453986 (* 1 = 0.453986 loss)
I0216 16:04:50.360713 24778 sgd_solver.cpp:106] Iteration 18400, lr = 0.001
I0216 16:04:54.984136 24778 solver.cpp:237] Iteration 18600, loss = 0.470314
I0216 16:04:54.984189 24778 solver.cpp:253]     Train net output #0: loss = 0.470314 (* 1 = 0.470314 loss)
I0216 16:04:54.984199 24778 sgd_solver.cpp:106] Iteration 18600, lr = 0.001
I0216 16:05:00.102514 24778 solver.cpp:237] Iteration 18800, loss = 0.416597
I0216 16:05:00.102584 24778 solver.cpp:253]     Train net output #0: loss = 0.416597 (* 1 = 0.416597 loss)
I0216 16:05:00.102597 24778 sgd_solver.cpp:106] Iteration 18800, lr = 0.001
I0216 16:05:05.289515 24778 solver.cpp:341] Iteration 19000, Testing net (#0)
I0216 16:05:06.550158 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7436
I0216 16:05:06.550206 24778 solver.cpp:409]     Test net output #1: loss = 0.75857 (* 1 = 0.75857 loss)
I0216 16:05:06.556308 24778 solver.cpp:237] Iteration 19000, loss = 0.435666
I0216 16:05:06.556360 24778 solver.cpp:253]     Train net output #0: loss = 0.435666 (* 1 = 0.435666 loss)
I0216 16:05:06.556372 24778 sgd_solver.cpp:106] Iteration 19000, lr = 0.001
I0216 16:05:11.763703 24778 solver.cpp:237] Iteration 19200, loss = 0.451835
I0216 16:05:11.763844 24778 solver.cpp:253]     Train net output #0: loss = 0.451835 (* 1 = 0.451835 loss)
I0216 16:05:11.763862 24778 sgd_solver.cpp:106] Iteration 19200, lr = 0.001
I0216 16:05:16.639000 24778 solver.cpp:237] Iteration 19400, loss = 0.423457
I0216 16:05:16.639055 24778 solver.cpp:253]     Train net output #0: loss = 0.423457 (* 1 = 0.423457 loss)
I0216 16:05:16.639071 24778 sgd_solver.cpp:106] Iteration 19400, lr = 0.001
I0216 16:05:21.841493 24778 solver.cpp:237] Iteration 19600, loss = 0.462837
I0216 16:05:21.841548 24778 solver.cpp:253]     Train net output #0: loss = 0.462837 (* 1 = 0.462837 loss)
I0216 16:05:21.841559 24778 sgd_solver.cpp:106] Iteration 19600, lr = 0.001
I0216 16:05:27.144582 24778 solver.cpp:237] Iteration 19800, loss = 0.417227
I0216 16:05:27.144628 24778 solver.cpp:253]     Train net output #0: loss = 0.417227 (* 1 = 0.417227 loss)
I0216 16:05:27.144635 24778 sgd_solver.cpp:106] Iteration 19800, lr = 0.001
I0216 16:05:31.444907 24778 solver.cpp:472] Snapshotting to HDF5 file examples/cifar10/cifar10_full_iter_20000.caffemodel.h5
I0216 16:05:32.125565 24778 sgd_solver.cpp:279] Snapshotting solver state to HDF5 file examples/cifar10/cifar10_full_iter_20000.solverstate.h5
I0216 16:05:32.128459 24778 solver.cpp:341] Iteration 20000, Testing net (#0)
I0216 16:05:33.363023 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7427
I0216 16:05:33.363080 24778 solver.cpp:409]     Test net output #1: loss = 0.754613 (* 1 = 0.754613 loss)
I0216 16:05:33.369912 24778 solver.cpp:237] Iteration 20000, loss = 0.40961
I0216 16:05:33.369972 24778 solver.cpp:253]     Train net output #0: loss = 0.40961 (* 1 = 0.40961 loss)
I0216 16:05:33.369988 24778 sgd_solver.cpp:106] Iteration 20000, lr = 0.001
I0216 16:05:38.571493 24778 solver.cpp:237] Iteration 20200, loss = 0.422565
I0216 16:05:38.571553 24778 solver.cpp:253]     Train net output #0: loss = 0.422565 (* 1 = 0.422565 loss)
I0216 16:05:38.571566 24778 sgd_solver.cpp:106] Iteration 20200, lr = 0.001
I0216 16:05:43.775379 24778 solver.cpp:237] Iteration 20400, loss = 0.390107
I0216 16:05:43.775470 24778 solver.cpp:253]     Train net output #0: loss = 0.390107 (* 1 = 0.390107 loss)
I0216 16:05:43.775706 24778 sgd_solver.cpp:106] Iteration 20400, lr = 0.001
I0216 16:05:49.114244 24778 solver.cpp:237] Iteration 20600, loss = 0.428557
I0216 16:05:49.114312 24778 solver.cpp:253]     Train net output #0: loss = 0.428557 (* 1 = 0.428557 loss)
I0216 16:05:49.114559 24778 sgd_solver.cpp:106] Iteration 20600, lr = 0.001
I0216 16:05:54.319229 24778 solver.cpp:237] Iteration 20800, loss = 0.413456
I0216 16:05:54.319274 24778 solver.cpp:253]     Train net output #0: loss = 0.413456 (* 1 = 0.413456 loss)
I0216 16:05:54.319294 24778 sgd_solver.cpp:106] Iteration 20800, lr = 0.001
I0216 16:05:59.227138 24778 solver.cpp:341] Iteration 21000, Testing net (#0)
I0216 16:06:00.396415 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7465
I0216 16:06:00.396456 24778 solver.cpp:409]     Test net output #1: loss = 0.744584 (* 1 = 0.744584 loss)
I0216 16:06:00.408841 24778 solver.cpp:237] Iteration 21000, loss = 0.380262
I0216 16:06:00.408881 24778 solver.cpp:253]     Train net output #0: loss = 0.380262 (* 1 = 0.380262 loss)
I0216 16:06:00.408888 24778 sgd_solver.cpp:106] Iteration 21000, lr = 0.001
I0216 16:06:05.402107 24778 solver.cpp:237] Iteration 21200, loss = 0.388031
I0216 16:06:05.402175 24778 solver.cpp:253]     Train net output #0: loss = 0.388031 (* 1 = 0.388031 loss)
I0216 16:06:05.402420 24778 sgd_solver.cpp:106] Iteration 21200, lr = 0.001
I0216 16:06:10.168699 24778 solver.cpp:237] Iteration 21400, loss = 0.35838
I0216 16:06:10.168743 24778 solver.cpp:253]     Train net output #0: loss = 0.35838 (* 1 = 0.35838 loss)
I0216 16:06:10.168751 24778 sgd_solver.cpp:106] Iteration 21400, lr = 0.001
I0216 16:06:15.217665 24778 solver.cpp:237] Iteration 21600, loss = 0.386506
I0216 16:06:15.217806 24778 solver.cpp:253]     Train net output #0: loss = 0.386506 (* 1 = 0.386506 loss)
I0216 16:06:15.217824 24778 sgd_solver.cpp:106] Iteration 21600, lr = 0.001
I0216 16:06:20.445721 24778 solver.cpp:237] Iteration 21800, loss = 0.3929
I0216 16:06:20.445786 24778 solver.cpp:253]     Train net output #0: loss = 0.3929 (* 1 = 0.3929 loss)
I0216 16:06:20.445804 24778 sgd_solver.cpp:106] Iteration 21800, lr = 0.001
I0216 16:06:25.581640 24778 solver.cpp:341] Iteration 22000, Testing net (#0)
I0216 16:06:26.845947 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7521
I0216 16:06:26.846010 24778 solver.cpp:409]     Test net output #1: loss = 0.738233 (* 1 = 0.738233 loss)
I0216 16:06:26.858786 24778 solver.cpp:237] Iteration 22000, loss = 0.368979
I0216 16:06:26.858841 24778 solver.cpp:253]     Train net output #0: loss = 0.368979 (* 1 = 0.368979 loss)
I0216 16:06:26.858852 24778 sgd_solver.cpp:106] Iteration 22000, lr = 0.001
I0216 16:06:32.155150 24778 solver.cpp:237] Iteration 22200, loss = 0.363708
I0216 16:06:32.155225 24778 solver.cpp:253]     Train net output #0: loss = 0.363708 (* 1 = 0.363708 loss)
I0216 16:06:32.155238 24778 sgd_solver.cpp:106] Iteration 22200, lr = 0.001
I0216 16:06:37.484808 24778 solver.cpp:237] Iteration 22400, loss = 0.339533
I0216 16:06:37.484866 24778 solver.cpp:253]     Train net output #0: loss = 0.339533 (* 1 = 0.339533 loss)
I0216 16:06:37.484882 24778 sgd_solver.cpp:106] Iteration 22400, lr = 0.001
I0216 16:06:42.432039 24778 solver.cpp:237] Iteration 22600, loss = 0.376945
I0216 16:06:42.432099 24778 solver.cpp:253]     Train net output #0: loss = 0.376945 (* 1 = 0.376945 loss)
I0216 16:06:42.432116 24778 sgd_solver.cpp:106] Iteration 22600, lr = 0.001
I0216 16:06:47.182262 24778 solver.cpp:237] Iteration 22800, loss = 0.380724
I0216 16:06:47.182365 24778 solver.cpp:253]     Train net output #0: loss = 0.380724 (* 1 = 0.380724 loss)
I0216 16:06:47.182379 24778 sgd_solver.cpp:106] Iteration 22800, lr = 0.001
I0216 16:06:52.315323 24778 solver.cpp:341] Iteration 23000, Testing net (#0)
I0216 16:06:53.444201 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7509
I0216 16:06:53.444259 24778 solver.cpp:409]     Test net output #1: loss = 0.744569 (* 1 = 0.744569 loss)
I0216 16:06:53.452986 24778 solver.cpp:237] Iteration 23000, loss = 0.357003
I0216 16:06:53.453047 24778 solver.cpp:253]     Train net output #0: loss = 0.357003 (* 1 = 0.357003 loss)
I0216 16:06:53.453060 24778 sgd_solver.cpp:106] Iteration 23000, lr = 0.001
I0216 16:06:58.515640 24778 solver.cpp:237] Iteration 23200, loss = 0.338517
I0216 16:06:58.515697 24778 solver.cpp:253]     Train net output #0: loss = 0.338517 (* 1 = 0.338517 loss)
I0216 16:06:58.515709 24778 sgd_solver.cpp:106] Iteration 23200, lr = 0.001
I0216 16:07:03.798346 24778 solver.cpp:237] Iteration 23400, loss = 0.344685
I0216 16:07:03.798398 24778 solver.cpp:253]     Train net output #0: loss = 0.344685 (* 1 = 0.344685 loss)
I0216 16:07:03.798723 24778 sgd_solver.cpp:106] Iteration 23400, lr = 0.001
I0216 16:07:09.016607 24778 solver.cpp:237] Iteration 23600, loss = 0.358299
I0216 16:07:09.016657 24778 solver.cpp:253]     Train net output #0: loss = 0.358299 (* 1 = 0.358299 loss)
I0216 16:07:09.016901 24778 sgd_solver.cpp:106] Iteration 23600, lr = 0.001
I0216 16:07:14.256803 24778 solver.cpp:237] Iteration 23800, loss = 0.357001
I0216 16:07:14.256858 24778 solver.cpp:253]     Train net output #0: loss = 0.357001 (* 1 = 0.357001 loss)
I0216 16:07:14.256870 24778 sgd_solver.cpp:106] Iteration 23800, lr = 0.001
I0216 16:07:19.135887 24778 solver.cpp:341] Iteration 24000, Testing net (#0)
I0216 16:07:19.635967 24778 blocking_queue.cpp:50] Data layer prefetch queue empty
I0216 16:07:19.674978 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7528
I0216 16:07:19.675025 24778 solver.cpp:409]     Test net output #1: loss = 0.750903 (* 1 = 0.750903 loss)
I0216 16:07:19.679811 24778 solver.cpp:237] Iteration 24000, loss = 0.337318
I0216 16:07:19.679843 24778 solver.cpp:253]     Train net output #0: loss = 0.337318 (* 1 = 0.337318 loss)
I0216 16:07:19.679852 24778 sgd_solver.cpp:106] Iteration 24000, lr = 0.001
I0216 16:07:24.843614 24778 solver.cpp:237] Iteration 24200, loss = 0.316713
I0216 16:07:24.843688 24778 solver.cpp:253]     Train net output #0: loss = 0.316713 (* 1 = 0.316713 loss)
I0216 16:07:24.844141 24778 sgd_solver.cpp:106] Iteration 24200, lr = 0.001
I0216 16:07:29.917417 24778 solver.cpp:237] Iteration 24400, loss = 0.315121
I0216 16:07:29.917466 24778 solver.cpp:253]     Train net output #0: loss = 0.315121 (* 1 = 0.315121 loss)
I0216 16:07:29.917814 24778 sgd_solver.cpp:106] Iteration 24400, lr = 0.001
I0216 16:07:35.054651 24778 solver.cpp:237] Iteration 24600, loss = 0.334717
I0216 16:07:35.054714 24778 solver.cpp:253]     Train net output #0: loss = 0.334717 (* 1 = 0.334717 loss)
I0216 16:07:35.054947 24778 sgd_solver.cpp:106] Iteration 24600, lr = 0.001
I0216 16:07:40.281024 24778 solver.cpp:237] Iteration 24800, loss = 0.350763
I0216 16:07:40.281098 24778 solver.cpp:253]     Train net output #0: loss = 0.350763 (* 1 = 0.350763 loss)
I0216 16:07:40.281429 24778 sgd_solver.cpp:106] Iteration 24800, lr = 0.001
I0216 16:07:45.138502 24778 solver.cpp:341] Iteration 25000, Testing net (#0)
I0216 16:07:46.382730 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7486
I0216 16:07:46.382786 24778 solver.cpp:409]     Test net output #1: loss = 0.764214 (* 1 = 0.764214 loss)
I0216 16:07:46.390993 24778 solver.cpp:237] Iteration 25000, loss = 0.322322
I0216 16:07:46.391042 24778 solver.cpp:253]     Train net output #0: loss = 0.322322 (* 1 = 0.322322 loss)
I0216 16:07:46.391055 24778 sgd_solver.cpp:106] Iteration 25000, lr = 0.001
I0216 16:07:51.607317 24778 solver.cpp:237] Iteration 25200, loss = 0.295745
I0216 16:07:51.607414 24778 solver.cpp:253]     Train net output #0: loss = 0.295745 (* 1 = 0.295745 loss)
I0216 16:07:51.607669 24778 sgd_solver.cpp:106] Iteration 25200, lr = 0.001
I0216 16:07:56.378957 24778 solver.cpp:237] Iteration 25400, loss = 0.291226
I0216 16:07:56.379025 24778 solver.cpp:253]     Train net output #0: loss = 0.291227 (* 1 = 0.291227 loss)
I0216 16:07:56.379043 24778 sgd_solver.cpp:106] Iteration 25400, lr = 0.001
I0216 16:08:01.066313 24778 solver.cpp:237] Iteration 25600, loss = 0.320377
I0216 16:08:01.066352 24778 solver.cpp:253]     Train net output #0: loss = 0.320377 (* 1 = 0.320377 loss)
I0216 16:08:01.066360 24778 sgd_solver.cpp:106] Iteration 25600, lr = 0.001
I0216 16:08:06.334590 24778 solver.cpp:237] Iteration 25800, loss = 0.331851
I0216 16:08:06.334650 24778 solver.cpp:253]     Train net output #0: loss = 0.331851 (* 1 = 0.331851 loss)
I0216 16:08:06.334662 24778 sgd_solver.cpp:106] Iteration 25800, lr = 0.001
I0216 16:08:11.267500 24778 solver.cpp:341] Iteration 26000, Testing net (#0)
I0216 16:08:12.495925 24778 solver.cpp:409]     Test net output #0: accuracy = 0.747
I0216 16:08:12.495983 24778 solver.cpp:409]     Test net output #1: loss = 0.777378 (* 1 = 0.777378 loss)
I0216 16:08:12.507570 24778 solver.cpp:237] Iteration 26000, loss = 0.307976
I0216 16:08:12.507627 24778 solver.cpp:253]     Train net output #0: loss = 0.307976 (* 1 = 0.307976 loss)
I0216 16:08:12.507643 24778 sgd_solver.cpp:106] Iteration 26000, lr = 0.001
I0216 16:08:17.661872 24778 solver.cpp:237] Iteration 26200, loss = 0.28306
I0216 16:08:17.661941 24778 solver.cpp:253]     Train net output #0: loss = 0.28306 (* 1 = 0.28306 loss)
I0216 16:08:17.662271 24778 sgd_solver.cpp:106] Iteration 26200, lr = 0.001
I0216 16:08:23.029824 24778 solver.cpp:237] Iteration 26400, loss = 0.265183
I0216 16:08:23.029958 24778 solver.cpp:253]     Train net output #0: loss = 0.265184 (* 1 = 0.265184 loss)
I0216 16:08:23.030421 24778 sgd_solver.cpp:106] Iteration 26400, lr = 0.001
I0216 16:08:28.189960 24778 solver.cpp:237] Iteration 26600, loss = 0.295386
I0216 16:08:28.190021 24778 solver.cpp:253]     Train net output #0: loss = 0.295387 (* 1 = 0.295387 loss)
I0216 16:08:28.190382 24778 sgd_solver.cpp:106] Iteration 26600, lr = 0.001
I0216 16:08:33.102687 24778 solver.cpp:237] Iteration 26800, loss = 0.327253
I0216 16:08:33.102743 24778 solver.cpp:253]     Train net output #0: loss = 0.327253 (* 1 = 0.327253 loss)
I0216 16:08:33.102758 24778 sgd_solver.cpp:106] Iteration 26800, lr = 0.001
I0216 16:08:37.830695 24778 solver.cpp:341] Iteration 27000, Testing net (#0)
I0216 16:08:38.985962 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7445
I0216 16:08:38.986019 24778 solver.cpp:409]     Test net output #1: loss = 0.79354 (* 1 = 0.79354 loss)
I0216 16:08:39.004559 24778 solver.cpp:237] Iteration 27000, loss = 0.311114
I0216 16:08:39.004618 24778 solver.cpp:253]     Train net output #0: loss = 0.311114 (* 1 = 0.311114 loss)
I0216 16:08:39.004636 24778 sgd_solver.cpp:106] Iteration 27000, lr = 0.001
I0216 16:08:44.275140 24778 solver.cpp:237] Iteration 27200, loss = 0.283322
I0216 16:08:44.275187 24778 solver.cpp:253]     Train net output #0: loss = 0.283322 (* 1 = 0.283322 loss)
I0216 16:08:44.275194 24778 sgd_solver.cpp:106] Iteration 27200, lr = 0.001
I0216 16:08:49.544492 24778 solver.cpp:237] Iteration 27400, loss = 0.250064
I0216 16:08:49.544550 24778 solver.cpp:253]     Train net output #0: loss = 0.250064 (* 1 = 0.250064 loss)
I0216 16:08:49.544562 24778 sgd_solver.cpp:106] Iteration 27400, lr = 0.001
I0216 16:08:54.755519 24778 solver.cpp:237] Iteration 27600, loss = 0.27998
I0216 16:08:54.755635 24778 solver.cpp:253]     Train net output #0: loss = 0.27998 (* 1 = 0.27998 loss)
I0216 16:08:54.755655 24778 sgd_solver.cpp:106] Iteration 27600, lr = 0.001
I0216 16:08:59.709700 24778 solver.cpp:237] Iteration 27800, loss = 0.321204
I0216 16:08:59.709764 24778 solver.cpp:253]     Train net output #0: loss = 0.321204 (* 1 = 0.321204 loss)
I0216 16:08:59.710204 24778 sgd_solver.cpp:106] Iteration 27800, lr = 0.001
I0216 16:09:04.886888 24778 solver.cpp:341] Iteration 28000, Testing net (#0)
I0216 16:09:06.087218 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7484
I0216 16:09:06.087278 24778 solver.cpp:409]     Test net output #1: loss = 0.78091 (* 1 = 0.78091 loss)
I0216 16:09:06.102838 24778 solver.cpp:237] Iteration 28000, loss = 0.277425
I0216 16:09:06.102890 24778 solver.cpp:253]     Train net output #0: loss = 0.277425 (* 1 = 0.277425 loss)
I0216 16:09:06.102901 24778 sgd_solver.cpp:106] Iteration 28000, lr = 0.001
I0216 16:09:10.404376 24778 solver.cpp:237] Iteration 28200, loss = 0.304881
I0216 16:09:10.404419 24778 solver.cpp:253]     Train net output #0: loss = 0.304881 (* 1 = 0.304881 loss)
I0216 16:09:10.404428 24778 sgd_solver.cpp:106] Iteration 28200, lr = 0.001
I0216 16:09:15.632570 24778 solver.cpp:237] Iteration 28400, loss = 0.256151
I0216 16:09:15.632616 24778 solver.cpp:253]     Train net output #0: loss = 0.256151 (* 1 = 0.256151 loss)
I0216 16:09:15.632624 24778 sgd_solver.cpp:106] Iteration 28400, lr = 0.001
I0216 16:09:20.910539 24778 solver.cpp:237] Iteration 28600, loss = 0.271973
I0216 16:09:20.910606 24778 solver.cpp:253]     Train net output #0: loss = 0.271973 (* 1 = 0.271973 loss)
I0216 16:09:20.910640 24778 sgd_solver.cpp:106] Iteration 28600, lr = 0.001
I0216 16:09:26.124785 24778 solver.cpp:237] Iteration 28800, loss = 0.326809
I0216 16:09:26.124918 24778 solver.cpp:253]     Train net output #0: loss = 0.326809 (* 1 = 0.326809 loss)
I0216 16:09:26.124934 24778 sgd_solver.cpp:106] Iteration 28800, lr = 0.001
I0216 16:09:31.284689 24778 solver.cpp:341] Iteration 29000, Testing net (#0)
I0216 16:09:32.550498 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7558
I0216 16:09:32.550559 24778 solver.cpp:409]     Test net output #1: loss = 0.767407 (* 1 = 0.767407 loss)
I0216 16:09:32.567082 24778 solver.cpp:237] Iteration 29000, loss = 0.253366
I0216 16:09:32.567137 24778 solver.cpp:253]     Train net output #0: loss = 0.253366 (* 1 = 0.253366 loss)
I0216 16:09:32.567148 24778 sgd_solver.cpp:106] Iteration 29000, lr = 0.001
I0216 16:09:37.767827 24778 solver.cpp:237] Iteration 29200, loss = 0.312499
I0216 16:09:37.767885 24778 solver.cpp:253]     Train net output #0: loss = 0.312499 (* 1 = 0.312499 loss)
I0216 16:09:37.767902 24778 sgd_solver.cpp:106] Iteration 29200, lr = 0.001
I0216 16:09:43.054709 24778 solver.cpp:237] Iteration 29400, loss = 0.240088
I0216 16:09:43.054767 24778 solver.cpp:253]     Train net output #0: loss = 0.240088 (* 1 = 0.240088 loss)
I0216 16:09:43.054780 24778 sgd_solver.cpp:106] Iteration 29400, lr = 0.001
I0216 16:09:47.423696 24778 solver.cpp:237] Iteration 29600, loss = 0.279377
I0216 16:09:47.423740 24778 solver.cpp:253]     Train net output #0: loss = 0.279377 (* 1 = 0.279377 loss)
I0216 16:09:47.423748 24778 sgd_solver.cpp:106] Iteration 29600, lr = 0.001
I0216 16:09:52.762675 24778 solver.cpp:237] Iteration 29800, loss = 0.286146
I0216 16:09:52.762732 24778 solver.cpp:253]     Train net output #0: loss = 0.286146 (* 1 = 0.286146 loss)
I0216 16:09:52.762743 24778 sgd_solver.cpp:106] Iteration 29800, lr = 0.001
I0216 16:09:57.776170 24778 solver.cpp:472] Snapshotting to HDF5 file examples/cifar10/cifar10_full_iter_30000.caffemodel.h5
I0216 16:09:58.620786 24778 sgd_solver.cpp:279] Snapshotting solver state to HDF5 file examples/cifar10/cifar10_full_iter_30000.solverstate.h5
I0216 16:09:58.624485 24778 solver.cpp:341] Iteration 30000, Testing net (#0)
I0216 16:09:59.712703 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7633
I0216 16:09:59.712759 24778 solver.cpp:409]     Test net output #1: loss = 0.747 (* 1 = 0.747 loss)
I0216 16:09:59.718551 24778 solver.cpp:237] Iteration 30000, loss = 0.225015
I0216 16:09:59.718607 24778 solver.cpp:253]     Train net output #0: loss = 0.225015 (* 1 = 0.225015 loss)
I0216 16:09:59.718624 24778 sgd_solver.cpp:106] Iteration 30000, lr = 0.001
I0216 16:10:04.911711 24778 solver.cpp:237] Iteration 30200, loss = 0.305683
I0216 16:10:04.911761 24778 solver.cpp:253]     Train net output #0: loss = 0.305683 (* 1 = 0.305683 loss)
I0216 16:10:04.911769 24778 sgd_solver.cpp:106] Iteration 30200, lr = 0.001
I0216 16:10:10.115941 24778 solver.cpp:237] Iteration 30400, loss = 0.248301
I0216 16:10:10.116009 24778 solver.cpp:253]     Train net output #0: loss = 0.248301 (* 1 = 0.248301 loss)
I0216 16:10:10.116027 24778 sgd_solver.cpp:106] Iteration 30400, lr = 0.001
I0216 16:10:15.279908 24778 solver.cpp:237] Iteration 30600, loss = 0.282282
I0216 16:10:15.279969 24778 solver.cpp:253]     Train net output #0: loss = 0.282282 (* 1 = 0.282282 loss)
I0216 16:10:15.279985 24778 sgd_solver.cpp:106] Iteration 30600, lr = 0.001
I0216 16:10:20.453274 24778 solver.cpp:237] Iteration 30800, loss = 0.258742
I0216 16:10:20.453338 24778 solver.cpp:253]     Train net output #0: loss = 0.258742 (* 1 = 0.258742 loss)
I0216 16:10:20.453353 24778 sgd_solver.cpp:106] Iteration 30800, lr = 0.001
I0216 16:10:24.829989 24778 solver.cpp:341] Iteration 31000, Testing net (#0)
I0216 16:10:26.083267 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7644
I0216 16:10:26.083325 24778 solver.cpp:409]     Test net output #1: loss = 0.739413 (* 1 = 0.739413 loss)
I0216 16:10:26.101135 24778 solver.cpp:237] Iteration 31000, loss = 0.209379
I0216 16:10:26.101215 24778 solver.cpp:253]     Train net output #0: loss = 0.209379 (* 1 = 0.209379 loss)
I0216 16:10:26.101233 24778 sgd_solver.cpp:106] Iteration 31000, lr = 0.001
I0216 16:10:31.411933 24778 solver.cpp:237] Iteration 31200, loss = 0.2894
I0216 16:10:31.412065 24778 solver.cpp:253]     Train net output #0: loss = 0.2894 (* 1 = 0.2894 loss)
I0216 16:10:31.412080 24778 sgd_solver.cpp:106] Iteration 31200, lr = 0.001
I0216 16:10:36.683143 24778 solver.cpp:237] Iteration 31400, loss = 0.23848
I0216 16:10:36.683188 24778 solver.cpp:253]     Train net output #0: loss = 0.23848 (* 1 = 0.23848 loss)
I0216 16:10:36.683197 24778 sgd_solver.cpp:106] Iteration 31400, lr = 0.001
I0216 16:10:41.800025 24778 solver.cpp:237] Iteration 31600, loss = 0.259048
I0216 16:10:41.800088 24778 solver.cpp:253]     Train net output #0: loss = 0.259048 (* 1 = 0.259048 loss)
I0216 16:10:41.800107 24778 sgd_solver.cpp:106] Iteration 31600, lr = 0.001
I0216 16:10:47.025842 24778 solver.cpp:237] Iteration 31800, loss = 0.245826
I0216 16:10:47.025905 24778 solver.cpp:253]     Train net output #0: loss = 0.245826 (* 1 = 0.245826 loss)
I0216 16:10:47.025923 24778 sgd_solver.cpp:106] Iteration 31800, lr = 0.001
I0216 16:10:52.038736 24778 solver.cpp:341] Iteration 32000, Testing net (#0)
I0216 16:10:53.249074 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7622
I0216 16:10:53.249132 24778 solver.cpp:409]     Test net output #1: loss = 0.750355 (* 1 = 0.750355 loss)
I0216 16:10:53.262974 24778 solver.cpp:237] Iteration 32000, loss = 0.254164
I0216 16:10:53.263031 24778 solver.cpp:253]     Train net output #0: loss = 0.254164 (* 1 = 0.254164 loss)
I0216 16:10:53.263046 24778 sgd_solver.cpp:106] Iteration 32000, lr = 0.001
I0216 16:10:58.292896 24778 solver.cpp:237] Iteration 32200, loss = 0.269137
I0216 16:10:58.292956 24778 solver.cpp:253]     Train net output #0: loss = 0.269137 (* 1 = 0.269137 loss)
I0216 16:10:58.292968 24778 sgd_solver.cpp:106] Iteration 32200, lr = 0.001
I0216 16:11:03.140998 24778 solver.cpp:237] Iteration 32400, loss = 0.248367
I0216 16:11:03.141108 24778 solver.cpp:253]     Train net output #0: loss = 0.248367 (* 1 = 0.248367 loss)
I0216 16:11:03.141127 24778 sgd_solver.cpp:106] Iteration 32400, lr = 0.001
I0216 16:11:08.443212 24778 solver.cpp:237] Iteration 32600, loss = 0.259252
I0216 16:11:08.443261 24778 solver.cpp:253]     Train net output #0: loss = 0.259252 (* 1 = 0.259252 loss)
I0216 16:11:08.443271 24778 sgd_solver.cpp:106] Iteration 32600, lr = 0.001
I0216 16:11:13.625145 24778 solver.cpp:237] Iteration 32800, loss = 0.242139
I0216 16:11:13.625208 24778 solver.cpp:253]     Train net output #0: loss = 0.242139 (* 1 = 0.242139 loss)
I0216 16:11:13.625221 24778 sgd_solver.cpp:106] Iteration 32800, lr = 0.001
I0216 16:11:18.824631 24778 solver.cpp:341] Iteration 33000, Testing net (#0)
I0216 16:11:20.054265 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7419
I0216 16:11:20.054316 24778 solver.cpp:409]     Test net output #1: loss = 0.827367 (* 1 = 0.827367 loss)
I0216 16:11:20.071801 24778 solver.cpp:237] Iteration 33000, loss = 0.337975
I0216 16:11:20.071864 24778 solver.cpp:253]     Train net output #0: loss = 0.337975 (* 1 = 0.337975 loss)
I0216 16:11:20.071882 24778 sgd_solver.cpp:106] Iteration 33000, lr = 0.001
I0216 16:11:25.383584 24778 solver.cpp:237] Iteration 33200, loss = 0.244027
I0216 16:11:25.383641 24778 solver.cpp:253]     Train net output #0: loss = 0.244027 (* 1 = 0.244027 loss)
I0216 16:11:25.383652 24778 sgd_solver.cpp:106] Iteration 33200, lr = 0.001
I0216 16:11:30.694046 24778 solver.cpp:237] Iteration 33400, loss = 0.327958
I0216 16:11:30.694109 24778 solver.cpp:253]     Train net output #0: loss = 0.327959 (* 1 = 0.327959 loss)
I0216 16:11:30.694128 24778 sgd_solver.cpp:106] Iteration 33400, lr = 0.001
I0216 16:11:35.191212 24778 solver.cpp:237] Iteration 33600, loss = 0.249044
I0216 16:11:35.191360 24778 solver.cpp:253]     Train net output #0: loss = 0.249044 (* 1 = 0.249044 loss)
I0216 16:11:35.191380 24778 sgd_solver.cpp:106] Iteration 33600, lr = 0.001
I0216 16:11:40.468174 24778 solver.cpp:237] Iteration 33800, loss = 0.244076
I0216 16:11:40.468240 24778 solver.cpp:253]     Train net output #0: loss = 0.244076 (* 1 = 0.244076 loss)
I0216 16:11:40.468256 24778 sgd_solver.cpp:106] Iteration 33800, lr = 0.001
I0216 16:11:45.739543 24778 solver.cpp:341] Iteration 34000, Testing net (#0)
I0216 16:11:46.866190 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7297
I0216 16:11:46.866236 24778 solver.cpp:409]     Test net output #1: loss = 0.880859 (* 1 = 0.880859 loss)
I0216 16:11:46.880307 24778 solver.cpp:237] Iteration 34000, loss = 0.33522
I0216 16:11:46.880357 24778 solver.cpp:253]     Train net output #0: loss = 0.33522 (* 1 = 0.33522 loss)
I0216 16:11:46.880368 24778 sgd_solver.cpp:106] Iteration 34000, lr = 0.001
I0216 16:11:52.281502 24778 solver.cpp:237] Iteration 34200, loss = 0.224488
I0216 16:11:52.281569 24778 solver.cpp:253]     Train net output #0: loss = 0.224488 (* 1 = 0.224488 loss)
I0216 16:11:52.281589 24778 sgd_solver.cpp:106] Iteration 34200, lr = 0.001
I0216 16:11:57.433750 24778 solver.cpp:237] Iteration 34400, loss = 0.336265
I0216 16:11:57.433812 24778 solver.cpp:253]     Train net output #0: loss = 0.336265 (* 1 = 0.336265 loss)
I0216 16:11:57.434087 24778 sgd_solver.cpp:106] Iteration 34400, lr = 0.001
I0216 16:12:02.755597 24778 solver.cpp:237] Iteration 34600, loss = 0.253248
I0216 16:12:02.755661 24778 solver.cpp:253]     Train net output #0: loss = 0.253248 (* 1 = 0.253248 loss)
I0216 16:12:02.755906 24778 sgd_solver.cpp:106] Iteration 34600, lr = 0.001
I0216 16:12:08.006175 24778 solver.cpp:237] Iteration 34800, loss = 0.209191
I0216 16:12:08.006275 24778 solver.cpp:253]     Train net output #0: loss = 0.209191 (* 1 = 0.209191 loss)
I0216 16:12:08.006500 24778 sgd_solver.cpp:106] Iteration 34800, lr = 0.001
I0216 16:12:12.321931 24778 solver.cpp:341] Iteration 35000, Testing net (#0)
I0216 16:12:13.623242 24778 solver.cpp:409]     Test net output #0: accuracy = 0.711
I0216 16:12:13.623301 24778 solver.cpp:409]     Test net output #1: loss = 0.966185 (* 1 = 0.966185 loss)
I0216 16:12:13.639760 24778 solver.cpp:237] Iteration 35000, loss = 0.345772
I0216 16:12:13.639816 24778 solver.cpp:253]     Train net output #0: loss = 0.345772 (* 1 = 0.345772 loss)
I0216 16:12:13.639827 24778 sgd_solver.cpp:106] Iteration 35000, lr = 0.001
I0216 16:12:18.878382 24778 solver.cpp:237] Iteration 35200, loss = 0.21577
I0216 16:12:18.878451 24778 solver.cpp:253]     Train net output #0: loss = 0.21577 (* 1 = 0.21577 loss)
I0216 16:12:18.878859 24778 sgd_solver.cpp:106] Iteration 35200, lr = 0.001
I0216 16:12:23.862233 24778 solver.cpp:237] Iteration 35400, loss = 0.288458
I0216 16:12:23.862298 24778 solver.cpp:253]     Train net output #0: loss = 0.288458 (* 1 = 0.288458 loss)
I0216 16:12:23.862316 24778 sgd_solver.cpp:106] Iteration 35400, lr = 0.001
I0216 16:12:29.081487 24778 solver.cpp:237] Iteration 35600, loss = 0.245931
I0216 16:12:29.081548 24778 solver.cpp:253]     Train net output #0: loss = 0.245931 (* 1 = 0.245931 loss)
I0216 16:12:29.081565 24778 sgd_solver.cpp:106] Iteration 35600, lr = 0.001
I0216 16:12:33.936167 24778 solver.cpp:237] Iteration 35800, loss = 0.179703
I0216 16:12:33.936211 24778 solver.cpp:253]     Train net output #0: loss = 0.179703 (* 1 = 0.179703 loss)
I0216 16:12:33.936219 24778 sgd_solver.cpp:106] Iteration 35800, lr = 0.001
I0216 16:12:39.134564 24778 solver.cpp:341] Iteration 36000, Testing net (#0)
I0216 16:12:40.457700 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7043
I0216 16:12:40.457763 24778 solver.cpp:409]     Test net output #1: loss = 0.993729 (* 1 = 0.993729 loss)
I0216 16:12:40.472872 24778 solver.cpp:237] Iteration 36000, loss = 0.331354
I0216 16:12:40.472926 24778 solver.cpp:253]     Train net output #0: loss = 0.331354 (* 1 = 0.331354 loss)
I0216 16:12:40.472939 24778 sgd_solver.cpp:106] Iteration 36000, lr = 0.001
I0216 16:12:45.619189 24778 solver.cpp:237] Iteration 36200, loss = 0.211375
I0216 16:12:45.619252 24778 solver.cpp:253]     Train net output #0: loss = 0.211375 (* 1 = 0.211375 loss)
I0216 16:12:45.619288 24778 sgd_solver.cpp:106] Iteration 36200, lr = 0.001
I0216 16:12:50.238654 24778 solver.cpp:237] Iteration 36400, loss = 0.242608
I0216 16:12:50.238718 24778 solver.cpp:253]     Train net output #0: loss = 0.242608 (* 1 = 0.242608 loss)
I0216 16:12:50.238735 24778 sgd_solver.cpp:106] Iteration 36400, lr = 0.001
I0216 16:12:55.383044 24778 solver.cpp:237] Iteration 36600, loss = 0.242485
I0216 16:12:55.383105 24778 solver.cpp:253]     Train net output #0: loss = 0.242486 (* 1 = 0.242486 loss)
I0216 16:12:55.383117 24778 sgd_solver.cpp:106] Iteration 36600, lr = 0.001
I0216 16:13:00.625437 24778 solver.cpp:237] Iteration 36800, loss = 0.16943
I0216 16:13:00.625499 24778 solver.cpp:253]     Train net output #0: loss = 0.16943 (* 1 = 0.16943 loss)
I0216 16:13:00.625516 24778 sgd_solver.cpp:106] Iteration 36800, lr = 0.001
I0216 16:13:05.906440 24778 solver.cpp:341] Iteration 37000, Testing net (#0)
I0216 16:13:07.201045 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7208
I0216 16:13:07.201092 24778 solver.cpp:409]     Test net output #1: loss = 0.926086 (* 1 = 0.926086 loss)
I0216 16:13:07.217558 24778 solver.cpp:237] Iteration 37000, loss = 0.26726
I0216 16:13:07.217600 24778 solver.cpp:253]     Train net output #0: loss = 0.26726 (* 1 = 0.26726 loss)
I0216 16:13:07.217608 24778 sgd_solver.cpp:106] Iteration 37000, lr = 0.001
I0216 16:13:12.372114 24778 solver.cpp:237] Iteration 37200, loss = 0.21572
I0216 16:13:12.372256 24778 solver.cpp:253]     Train net output #0: loss = 0.21572 (* 1 = 0.21572 loss)
I0216 16:13:12.372275 24778 sgd_solver.cpp:106] Iteration 37200, lr = 0.001
I0216 16:13:17.492069 24778 solver.cpp:237] Iteration 37400, loss = 0.222847
I0216 16:13:17.492141 24778 solver.cpp:253]     Train net output #0: loss = 0.222847 (* 1 = 0.222847 loss)
I0216 16:13:17.492544 24778 sgd_solver.cpp:106] Iteration 37400, lr = 0.001
I0216 16:13:22.578548 24778 solver.cpp:237] Iteration 37600, loss = 0.239886
I0216 16:13:22.578594 24778 solver.cpp:253]     Train net output #0: loss = 0.239887 (* 1 = 0.239887 loss)
I0216 16:13:22.578603 24778 sgd_solver.cpp:106] Iteration 37600, lr = 0.001
I0216 16:13:27.146399 24778 solver.cpp:237] Iteration 37800, loss = 0.156632
I0216 16:13:27.146462 24778 solver.cpp:253]     Train net output #0: loss = 0.156632 (* 1 = 0.156632 loss)
I0216 16:13:27.146481 24778 sgd_solver.cpp:106] Iteration 37800, lr = 0.001
I0216 16:13:32.374089 24778 solver.cpp:341] Iteration 38000, Testing net (#0)
I0216 16:13:33.563328 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7226
I0216 16:13:33.563385 24778 solver.cpp:409]     Test net output #1: loss = 0.928053 (* 1 = 0.928053 loss)
I0216 16:13:33.569242 24778 solver.cpp:237] Iteration 38000, loss = 0.27846
I0216 16:13:33.569299 24778 solver.cpp:253]     Train net output #0: loss = 0.27846 (* 1 = 0.27846 loss)
I0216 16:13:33.569316 24778 sgd_solver.cpp:106] Iteration 38000, lr = 0.001
I0216 16:13:38.961520 24778 solver.cpp:237] Iteration 38200, loss = 0.199178
I0216 16:13:38.961576 24778 solver.cpp:253]     Train net output #0: loss = 0.199178 (* 1 = 0.199178 loss)
I0216 16:13:38.961587 24778 sgd_solver.cpp:106] Iteration 38200, lr = 0.001
I0216 16:13:44.295261 24778 solver.cpp:237] Iteration 38400, loss = 0.206618
I0216 16:13:44.295357 24778 solver.cpp:253]     Train net output #0: loss = 0.206618 (* 1 = 0.206618 loss)
I0216 16:13:44.295375 24778 sgd_solver.cpp:106] Iteration 38400, lr = 0.001
I0216 16:13:49.565343 24778 solver.cpp:237] Iteration 38600, loss = 0.228241
I0216 16:13:49.565423 24778 solver.cpp:253]     Train net output #0: loss = 0.228241 (* 1 = 0.228241 loss)
I0216 16:13:49.565444 24778 sgd_solver.cpp:106] Iteration 38600, lr = 0.001
I0216 16:13:54.993752 24778 solver.cpp:237] Iteration 38800, loss = 0.150876
I0216 16:13:54.993814 24778 solver.cpp:253]     Train net output #0: loss = 0.150876 (* 1 = 0.150876 loss)
I0216 16:13:54.993831 24778 sgd_solver.cpp:106] Iteration 38800, lr = 0.001
I0216 16:13:59.636605 24778 solver.cpp:341] Iteration 39000, Testing net (#0)
I0216 16:14:00.537317 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7254
I0216 16:14:00.537375 24778 solver.cpp:409]     Test net output #1: loss = 0.935462 (* 1 = 0.935462 loss)
I0216 16:14:00.553966 24778 solver.cpp:237] Iteration 39000, loss = 0.266008
I0216 16:14:00.554016 24778 solver.cpp:253]     Train net output #0: loss = 0.266008 (* 1 = 0.266008 loss)
I0216 16:14:00.554028 24778 sgd_solver.cpp:106] Iteration 39000, lr = 0.001
I0216 16:14:05.839123 24778 solver.cpp:237] Iteration 39200, loss = 0.185305
I0216 16:14:05.839187 24778 solver.cpp:253]     Train net output #0: loss = 0.185305 (* 1 = 0.185305 loss)
I0216 16:14:05.839205 24778 sgd_solver.cpp:106] Iteration 39200, lr = 0.001
I0216 16:14:11.169853 24778 solver.cpp:237] Iteration 39400, loss = 0.208568
I0216 16:14:11.169924 24778 solver.cpp:253]     Train net output #0: loss = 0.208568 (* 1 = 0.208568 loss)
I0216 16:14:11.169937 24778 sgd_solver.cpp:106] Iteration 39400, lr = 0.001
I0216 16:14:16.494576 24778 solver.cpp:237] Iteration 39600, loss = 0.208714
I0216 16:14:16.494715 24778 solver.cpp:253]     Train net output #0: loss = 0.208714 (* 1 = 0.208714 loss)
I0216 16:14:16.494734 24778 sgd_solver.cpp:106] Iteration 39600, lr = 0.001
I0216 16:14:21.854542 24778 solver.cpp:237] Iteration 39800, loss = 0.163796
I0216 16:14:21.854594 24778 solver.cpp:253]     Train net output #0: loss = 0.163796 (* 1 = 0.163796 loss)
I0216 16:14:21.854604 24778 sgd_solver.cpp:106] Iteration 39800, lr = 0.001
I0216 16:14:26.871152 24778 solver.cpp:472] Snapshotting to HDF5 file examples/cifar10/cifar10_full_iter_40000.caffemodel.h5
I0216 16:14:27.698498 24778 sgd_solver.cpp:279] Snapshotting solver state to HDF5 file examples/cifar10/cifar10_full_iter_40000.solverstate.h5
I0216 16:14:27.701138 24778 solver.cpp:341] Iteration 40000, Testing net (#0)
I0216 16:14:28.981056 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7229
I0216 16:14:28.981103 24778 solver.cpp:409]     Test net output #1: loss = 0.944364 (* 1 = 0.944364 loss)
I0216 16:14:28.986686 24778 solver.cpp:237] Iteration 40000, loss = 0.248392
I0216 16:14:28.986726 24778 solver.cpp:253]     Train net output #0: loss = 0.248392 (* 1 = 0.248392 loss)
I0216 16:14:28.986733 24778 sgd_solver.cpp:106] Iteration 40000, lr = 0.001
I0216 16:14:33.982547 24778 solver.cpp:237] Iteration 40200, loss = 0.169453
I0216 16:14:33.982620 24778 solver.cpp:253]     Train net output #0: loss = 0.169453 (* 1 = 0.169453 loss)
I0216 16:14:33.982988 24778 sgd_solver.cpp:106] Iteration 40200, lr = 0.001
I0216 16:14:38.740483 24778 solver.cpp:237] Iteration 40400, loss = 0.196776
I0216 16:14:38.740545 24778 solver.cpp:253]     Train net output #0: loss = 0.196776 (* 1 = 0.196776 loss)
I0216 16:14:38.740561 24778 sgd_solver.cpp:106] Iteration 40400, lr = 0.001
I0216 16:14:43.955538 24778 solver.cpp:237] Iteration 40600, loss = 0.206276
I0216 16:14:43.955600 24778 solver.cpp:253]     Train net output #0: loss = 0.206276 (* 1 = 0.206276 loss)
I0216 16:14:43.955611 24778 sgd_solver.cpp:106] Iteration 40600, lr = 0.001
I0216 16:14:49.237879 24778 solver.cpp:237] Iteration 40800, loss = 0.177118
I0216 16:14:49.237990 24778 solver.cpp:253]     Train net output #0: loss = 0.177118 (* 1 = 0.177118 loss)
I0216 16:14:49.238009 24778 sgd_solver.cpp:106] Iteration 40800, lr = 0.001
I0216 16:14:54.437772 24778 solver.cpp:341] Iteration 41000, Testing net (#0)
I0216 16:14:55.684945 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7226
I0216 16:14:55.684998 24778 solver.cpp:409]     Test net output #1: loss = 0.95573 (* 1 = 0.95573 loss)
I0216 16:14:55.691750 24778 solver.cpp:237] Iteration 41000, loss = 0.255357
I0216 16:14:55.691803 24778 solver.cpp:253]     Train net output #0: loss = 0.255357 (* 1 = 0.255357 loss)
I0216 16:14:55.691819 24778 sgd_solver.cpp:106] Iteration 41000, lr = 0.001
I0216 16:15:00.886863 24778 solver.cpp:237] Iteration 41200, loss = 0.169704
I0216 16:15:00.886929 24778 solver.cpp:253]     Train net output #0: loss = 0.169704 (* 1 = 0.169704 loss)
I0216 16:15:00.886941 24778 sgd_solver.cpp:106] Iteration 41200, lr = 0.001
I0216 16:15:06.108165 24778 solver.cpp:237] Iteration 41400, loss = 0.18266
I0216 16:15:06.108222 24778 solver.cpp:253]     Train net output #0: loss = 0.18266 (* 1 = 0.18266 loss)
I0216 16:15:06.108233 24778 sgd_solver.cpp:106] Iteration 41400, lr = 0.001
I0216 16:15:10.871584 24778 solver.cpp:237] Iteration 41600, loss = 0.203824
I0216 16:15:10.871634 24778 solver.cpp:253]     Train net output #0: loss = 0.203824 (* 1 = 0.203824 loss)
I0216 16:15:10.871644 24778 sgd_solver.cpp:106] Iteration 41600, lr = 0.001
I0216 16:15:15.770774 24778 solver.cpp:237] Iteration 41800, loss = 0.17645
I0216 16:15:15.770836 24778 solver.cpp:253]     Train net output #0: loss = 0.176451 (* 1 = 0.176451 loss)
I0216 16:15:15.770855 24778 sgd_solver.cpp:106] Iteration 41800, lr = 0.001
I0216 16:15:20.963304 24778 solver.cpp:341] Iteration 42000, Testing net (#0)
I0216 16:15:21.958919 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7304
I0216 16:15:21.958978 24778 solver.cpp:409]     Test net output #1: loss = 0.918452 (* 1 = 0.918452 loss)
I0216 16:15:21.965586 24778 solver.cpp:237] Iteration 42000, loss = 0.271229
I0216 16:15:21.965651 24778 solver.cpp:253]     Train net output #0: loss = 0.271229 (* 1 = 0.271229 loss)
I0216 16:15:21.965667 24778 sgd_solver.cpp:106] Iteration 42000, lr = 0.001
I0216 16:15:27.261668 24778 solver.cpp:237] Iteration 42200, loss = 0.178429
I0216 16:15:27.261726 24778 solver.cpp:253]     Train net output #0: loss = 0.178429 (* 1 = 0.178429 loss)
I0216 16:15:27.261739 24778 sgd_solver.cpp:106] Iteration 42200, lr = 0.001
I0216 16:15:32.460212 24778 solver.cpp:237] Iteration 42400, loss = 0.180057
I0216 16:15:32.460268 24778 solver.cpp:253]     Train net output #0: loss = 0.180057 (* 1 = 0.180057 loss)
I0216 16:15:32.460279 24778 sgd_solver.cpp:106] Iteration 42400, lr = 0.001
I0216 16:15:37.674360 24778 solver.cpp:237] Iteration 42600, loss = 0.207861
I0216 16:15:37.674423 24778 solver.cpp:253]     Train net output #0: loss = 0.207861 (* 1 = 0.207861 loss)
I0216 16:15:37.674442 24778 sgd_solver.cpp:106] Iteration 42600, lr = 0.001
I0216 16:15:42.704231 24778 solver.cpp:237] Iteration 42800, loss = 0.176288
I0216 16:15:42.704293 24778 solver.cpp:253]     Train net output #0: loss = 0.176288 (* 1 = 0.176288 loss)
I0216 16:15:42.704537 24778 sgd_solver.cpp:106] Iteration 42800, lr = 0.001
I0216 16:15:47.490852 24778 solver.cpp:341] Iteration 43000, Testing net (#0)
I0216 16:15:48.148185 24778 solver.cpp:409]     Test net output #0: accuracy = 0.729
I0216 16:15:48.148253 24778 solver.cpp:409]     Test net output #1: loss = 0.911767 (* 1 = 0.911767 loss)
I0216 16:15:48.154392 24778 solver.cpp:237] Iteration 43000, loss = 0.270659
I0216 16:15:48.154459 24778 solver.cpp:253]     Train net output #0: loss = 0.270659 (* 1 = 0.270659 loss)
I0216 16:15:48.154475 24778 sgd_solver.cpp:106] Iteration 43000, lr = 0.001
I0216 16:15:53.074920 24778 solver.cpp:237] Iteration 43200, loss = 0.194686
I0216 16:15:53.074998 24778 solver.cpp:253]     Train net output #0: loss = 0.194686 (* 1 = 0.194686 loss)
I0216 16:15:53.075465 24778 sgd_solver.cpp:106] Iteration 43200, lr = 0.001
I0216 16:15:58.418493 24778 solver.cpp:237] Iteration 43400, loss = 0.186374
I0216 16:15:58.418561 24778 solver.cpp:253]     Train net output #0: loss = 0.186374 (* 1 = 0.186374 loss)
I0216 16:15:58.418579 24778 sgd_solver.cpp:106] Iteration 43400, lr = 0.001
I0216 16:16:03.736676 24778 solver.cpp:237] Iteration 43600, loss = 0.218261
I0216 16:16:03.736737 24778 solver.cpp:253]     Train net output #0: loss = 0.218261 (* 1 = 0.218261 loss)
I0216 16:16:03.736749 24778 sgd_solver.cpp:106] Iteration 43600, lr = 0.001
I0216 16:16:08.911588 24778 solver.cpp:237] Iteration 43800, loss = 0.160222
I0216 16:16:08.911650 24778 solver.cpp:253]     Train net output #0: loss = 0.160222 (* 1 = 0.160222 loss)
I0216 16:16:08.911669 24778 sgd_solver.cpp:106] Iteration 43800, lr = 0.001
I0216 16:16:14.267969 24778 solver.cpp:341] Iteration 44000, Testing net (#0)
I0216 16:16:15.593621 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7318
I0216 16:16:15.593698 24778 solver.cpp:409]     Test net output #1: loss = 0.903769 (* 1 = 0.903769 loss)
I0216 16:16:15.600945 24778 solver.cpp:237] Iteration 44000, loss = 0.236466
I0216 16:16:15.601001 24778 solver.cpp:253]     Train net output #0: loss = 0.236466 (* 1 = 0.236466 loss)
I0216 16:16:15.601018 24778 sgd_solver.cpp:106] Iteration 44000, lr = 0.001
I0216 16:16:20.808372 24778 solver.cpp:237] Iteration 44200, loss = 0.212866
I0216 16:16:20.808432 24778 solver.cpp:253]     Train net output #0: loss = 0.212866 (* 1 = 0.212866 loss)
I0216 16:16:20.808449 24778 sgd_solver.cpp:106] Iteration 44200, lr = 0.001
I0216 16:16:25.069653 24778 solver.cpp:237] Iteration 44400, loss = 0.216279
I0216 16:16:25.069756 24778 solver.cpp:253]     Train net output #0: loss = 0.216279 (* 1 = 0.216279 loss)
I0216 16:16:25.069766 24778 sgd_solver.cpp:106] Iteration 44400, lr = 0.001
I0216 16:16:30.229131 24778 solver.cpp:237] Iteration 44600, loss = 0.223943
I0216 16:16:30.229193 24778 solver.cpp:253]     Train net output #0: loss = 0.223943 (* 1 = 0.223943 loss)
I0216 16:16:30.229210 24778 sgd_solver.cpp:106] Iteration 44600, lr = 0.001
I0216 16:16:35.458282 24778 solver.cpp:237] Iteration 44800, loss = 0.144043
I0216 16:16:35.458338 24778 solver.cpp:253]     Train net output #0: loss = 0.144044 (* 1 = 0.144044 loss)
I0216 16:16:35.458348 24778 sgd_solver.cpp:106] Iteration 44800, lr = 0.001
I0216 16:16:40.671186 24778 solver.cpp:341] Iteration 45000, Testing net (#0)
I0216 16:16:41.871644 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7328
I0216 16:16:41.871713 24778 solver.cpp:409]     Test net output #1: loss = 0.908412 (* 1 = 0.908412 loss)
I0216 16:16:41.881597 24778 solver.cpp:237] Iteration 45000, loss = 0.225564
I0216 16:16:41.881655 24778 solver.cpp:253]     Train net output #0: loss = 0.225564 (* 1 = 0.225564 loss)
I0216 16:16:41.881672 24778 sgd_solver.cpp:106] Iteration 45000, lr = 0.001
I0216 16:16:47.121215 24778 solver.cpp:237] Iteration 45200, loss = 0.222563
I0216 16:16:47.121279 24778 solver.cpp:253]     Train net output #0: loss = 0.222563 (* 1 = 0.222563 loss)
I0216 16:16:47.121295 24778 sgd_solver.cpp:106] Iteration 45200, lr = 0.001
I0216 16:16:52.112354 24778 solver.cpp:237] Iteration 45400, loss = 0.199269
I0216 16:16:52.112411 24778 solver.cpp:253]     Train net output #0: loss = 0.199269 (* 1 = 0.199269 loss)
I0216 16:16:52.112428 24778 sgd_solver.cpp:106] Iteration 45400, lr = 0.001
I0216 16:16:57.441313 24778 solver.cpp:237] Iteration 45600, loss = 0.208871
I0216 16:16:57.441424 24778 solver.cpp:253]     Train net output #0: loss = 0.208871 (* 1 = 0.208871 loss)
I0216 16:16:57.441438 24778 sgd_solver.cpp:106] Iteration 45600, lr = 0.001
I0216 16:17:01.640892 24778 solver.cpp:237] Iteration 45800, loss = 0.149524
I0216 16:17:01.640955 24778 solver.cpp:253]     Train net output #0: loss = 0.149524 (* 1 = 0.149524 loss)
I0216 16:17:01.640971 24778 sgd_solver.cpp:106] Iteration 45800, lr = 0.001
I0216 16:17:06.732235 24778 solver.cpp:341] Iteration 46000, Testing net (#0)
I0216 16:17:07.948462 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7317
I0216 16:17:07.948506 24778 solver.cpp:409]     Test net output #1: loss = 0.924327 (* 1 = 0.924327 loss)
I0216 16:17:07.953668 24778 solver.cpp:237] Iteration 46000, loss = 0.226142
I0216 16:17:07.953714 24778 solver.cpp:253]     Train net output #0: loss = 0.226142 (* 1 = 0.226142 loss)
I0216 16:17:07.953727 24778 sgd_solver.cpp:106] Iteration 46000, lr = 0.001
I0216 16:17:13.205893 24778 solver.cpp:237] Iteration 46200, loss = 0.210162
I0216 16:17:13.205955 24778 solver.cpp:253]     Train net output #0: loss = 0.210162 (* 1 = 0.210162 loss)
I0216 16:17:13.205966 24778 sgd_solver.cpp:106] Iteration 46200, lr = 0.001
I0216 16:17:18.299669 24778 solver.cpp:237] Iteration 46400, loss = 0.173364
I0216 16:17:18.299727 24778 solver.cpp:253]     Train net output #0: loss = 0.173364 (* 1 = 0.173364 loss)
I0216 16:17:18.299738 24778 sgd_solver.cpp:106] Iteration 46400, lr = 0.001
I0216 16:17:23.521317 24778 solver.cpp:237] Iteration 46600, loss = 0.177671
I0216 16:17:23.521394 24778 solver.cpp:253]     Train net output #0: loss = 0.177671 (* 1 = 0.177671 loss)
I0216 16:17:23.521406 24778 sgd_solver.cpp:106] Iteration 46600, lr = 0.001
I0216 16:17:28.795263 24778 solver.cpp:237] Iteration 46800, loss = 0.143692
I0216 16:17:28.795400 24778 solver.cpp:253]     Train net output #0: loss = 0.143692 (* 1 = 0.143692 loss)
I0216 16:17:28.795416 24778 sgd_solver.cpp:106] Iteration 46800, lr = 0.001
I0216 16:17:33.854354 24778 solver.cpp:341] Iteration 47000, Testing net (#0)
I0216 16:17:35.076813 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7351
I0216 16:17:35.076872 24778 solver.cpp:409]     Test net output #1: loss = 0.922243 (* 1 = 0.922243 loss)
I0216 16:17:35.089808 24778 solver.cpp:237] Iteration 47000, loss = 0.204085
I0216 16:17:35.089869 24778 solver.cpp:253]     Train net output #0: loss = 0.204085 (* 1 = 0.204085 loss)
I0216 16:17:35.089884 24778 sgd_solver.cpp:106] Iteration 47000, lr = 0.001
I0216 16:17:39.561507 24778 solver.cpp:237] Iteration 47200, loss = 0.194925
I0216 16:17:39.561559 24778 solver.cpp:253]     Train net output #0: loss = 0.194925 (* 1 = 0.194925 loss)
I0216 16:17:39.561574 24778 sgd_solver.cpp:106] Iteration 47200, lr = 0.001
I0216 16:17:44.618752 24778 solver.cpp:237] Iteration 47400, loss = 0.169813
I0216 16:17:44.618820 24778 solver.cpp:253]     Train net output #0: loss = 0.169813 (* 1 = 0.169813 loss)
I0216 16:17:44.619127 24778 sgd_solver.cpp:106] Iteration 47400, lr = 0.001
I0216 16:17:49.828080 24778 solver.cpp:237] Iteration 47600, loss = 0.17153
I0216 16:17:49.828147 24778 solver.cpp:253]     Train net output #0: loss = 0.17153 (* 1 = 0.17153 loss)
I0216 16:17:49.828613 24778 sgd_solver.cpp:106] Iteration 47600, lr = 0.001
I0216 16:17:55.160815 24778 solver.cpp:237] Iteration 47800, loss = 0.131176
I0216 16:17:55.160857 24778 solver.cpp:253]     Train net output #0: loss = 0.131176 (* 1 = 0.131176 loss)
I0216 16:17:55.160866 24778 sgd_solver.cpp:106] Iteration 47800, lr = 0.001
I0216 16:18:00.347656 24778 solver.cpp:341] Iteration 48000, Testing net (#0)
I0216 16:18:01.397100 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7322
I0216 16:18:01.397155 24778 solver.cpp:409]     Test net output #1: loss = 0.934762 (* 1 = 0.934762 loss)
I0216 16:18:01.409790 24778 solver.cpp:237] Iteration 48000, loss = 0.222948
I0216 16:18:01.409840 24778 solver.cpp:253]     Train net output #0: loss = 0.222948 (* 1 = 0.222948 loss)
I0216 16:18:01.409852 24778 sgd_solver.cpp:106] Iteration 48000, lr = 0.001
I0216 16:18:06.588618 24778 solver.cpp:237] Iteration 48200, loss = 0.193953
I0216 16:18:06.588680 24778 solver.cpp:253]     Train net output #0: loss = 0.193953 (* 1 = 0.193953 loss)
I0216 16:18:06.588690 24778 sgd_solver.cpp:106] Iteration 48200, lr = 0.001
I0216 16:18:11.812042 24778 solver.cpp:237] Iteration 48400, loss = 0.164506
I0216 16:18:11.812114 24778 solver.cpp:253]     Train net output #0: loss = 0.164506 (* 1 = 0.164506 loss)
I0216 16:18:11.812435 24778 sgd_solver.cpp:106] Iteration 48400, lr = 0.001
I0216 16:18:16.222825 24778 solver.cpp:237] Iteration 48600, loss = 0.174215
I0216 16:18:16.222877 24778 solver.cpp:253]     Train net output #0: loss = 0.174215 (* 1 = 0.174215 loss)
I0216 16:18:16.222890 24778 sgd_solver.cpp:106] Iteration 48600, lr = 0.001
I0216 16:18:21.312306 24778 solver.cpp:237] Iteration 48800, loss = 0.129475
I0216 16:18:21.312351 24778 solver.cpp:253]     Train net output #0: loss = 0.129475 (* 1 = 0.129475 loss)
I0216 16:18:21.312360 24778 sgd_solver.cpp:106] Iteration 48800, lr = 0.001
I0216 16:18:26.660827 24778 solver.cpp:341] Iteration 49000, Testing net (#0)
I0216 16:18:27.902384 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7355
I0216 16:18:27.902426 24778 solver.cpp:409]     Test net output #1: loss = 0.910594 (* 1 = 0.910594 loss)
I0216 16:18:27.910780 24778 solver.cpp:237] Iteration 49000, loss = 0.234317
I0216 16:18:27.910818 24778 solver.cpp:253]     Train net output #0: loss = 0.234317 (* 1 = 0.234317 loss)
I0216 16:18:27.910826 24778 sgd_solver.cpp:106] Iteration 49000, lr = 0.001
I0216 16:18:33.208678 24778 solver.cpp:237] Iteration 49200, loss = 0.246311
I0216 16:18:33.208792 24778 solver.cpp:253]     Train net output #0: loss = 0.246311 (* 1 = 0.246311 loss)
I0216 16:18:33.209260 24778 sgd_solver.cpp:106] Iteration 49200, lr = 0.001
I0216 16:18:38.486958 24778 solver.cpp:237] Iteration 49400, loss = 0.166466
I0216 16:18:38.487000 24778 solver.cpp:253]     Train net output #0: loss = 0.166466 (* 1 = 0.166466 loss)
I0216 16:18:38.487009 24778 sgd_solver.cpp:106] Iteration 49400, lr = 0.001
I0216 16:18:43.732918 24778 solver.cpp:237] Iteration 49600, loss = 0.179237
I0216 16:18:43.732983 24778 solver.cpp:253]     Train net output #0: loss = 0.179237 (* 1 = 0.179237 loss)
I0216 16:18:43.733250 24778 sgd_solver.cpp:106] Iteration 49600, lr = 0.001
I0216 16:18:48.743365 24778 solver.cpp:237] Iteration 49800, loss = 0.127856
I0216 16:18:48.743429 24778 solver.cpp:253]     Train net output #0: loss = 0.127856 (* 1 = 0.127856 loss)
I0216 16:18:48.743669 24778 sgd_solver.cpp:106] Iteration 49800, lr = 0.001
I0216 16:18:52.959712 24778 solver.cpp:472] Snapshotting to HDF5 file examples/cifar10/cifar10_full_iter_50000.caffemodel.h5
I0216 16:18:53.944819 24778 sgd_solver.cpp:279] Snapshotting solver state to HDF5 file examples/cifar10/cifar10_full_iter_50000.solverstate.h5
I0216 16:18:53.948472 24778 solver.cpp:341] Iteration 50000, Testing net (#0)
I0216 16:18:55.101835 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7482
I0216 16:18:55.101897 24778 solver.cpp:409]     Test net output #1: loss = 0.86677 (* 1 = 0.86677 loss)
I0216 16:18:55.119526 24778 solver.cpp:237] Iteration 50000, loss = 0.218553
I0216 16:18:55.119580 24778 solver.cpp:253]     Train net output #0: loss = 0.218553 (* 1 = 0.218553 loss)
I0216 16:18:55.119590 24778 sgd_solver.cpp:106] Iteration 50000, lr = 0.001
I0216 16:19:00.066375 24778 solver.cpp:237] Iteration 50200, loss = 0.261808
I0216 16:19:00.066412 24778 solver.cpp:253]     Train net output #0: loss = 0.261808 (* 1 = 0.261808 loss)
I0216 16:19:00.066421 24778 sgd_solver.cpp:106] Iteration 50200, lr = 0.001
I0216 16:19:05.408834 24778 solver.cpp:237] Iteration 50400, loss = 0.190532
I0216 16:19:05.408937 24778 solver.cpp:253]     Train net output #0: loss = 0.190532 (* 1 = 0.190532 loss)
I0216 16:19:05.408951 24778 sgd_solver.cpp:106] Iteration 50400, lr = 0.001
I0216 16:19:10.663810 24778 solver.cpp:237] Iteration 50600, loss = 0.170893
I0216 16:19:10.663877 24778 solver.cpp:253]     Train net output #0: loss = 0.170893 (* 1 = 0.170893 loss)
I0216 16:19:10.664306 24778 sgd_solver.cpp:106] Iteration 50600, lr = 0.001
I0216 16:19:15.893694 24778 solver.cpp:237] Iteration 50800, loss = 0.122664
I0216 16:19:15.893743 24778 solver.cpp:253]     Train net output #0: loss = 0.122664 (* 1 = 0.122664 loss)
I0216 16:19:15.894052 24778 sgd_solver.cpp:106] Iteration 50800, lr = 0.001
I0216 16:19:21.162864 24778 solver.cpp:341] Iteration 51000, Testing net (#0)
I0216 16:19:22.462891 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7511
I0216 16:19:22.462939 24778 solver.cpp:409]     Test net output #1: loss = 0.848707 (* 1 = 0.848707 loss)
I0216 16:19:22.481627 24778 solver.cpp:237] Iteration 51000, loss = 0.227662
I0216 16:19:22.481688 24778 solver.cpp:253]     Train net output #0: loss = 0.227662 (* 1 = 0.227662 loss)
I0216 16:19:22.481700 24778 sgd_solver.cpp:106] Iteration 51000, lr = 0.001
I0216 16:19:26.838001 24778 solver.cpp:237] Iteration 51200, loss = 0.186254
I0216 16:19:26.838063 24778 solver.cpp:253]     Train net output #0: loss = 0.186254 (* 1 = 0.186254 loss)
I0216 16:19:26.838080 24778 sgd_solver.cpp:106] Iteration 51200, lr = 0.001
I0216 16:19:31.567055 24778 solver.cpp:237] Iteration 51400, loss = 0.216778
I0216 16:19:31.567116 24778 solver.cpp:253]     Train net output #0: loss = 0.216778 (* 1 = 0.216778 loss)
I0216 16:19:31.567131 24778 sgd_solver.cpp:106] Iteration 51400, lr = 0.001
I0216 16:19:36.798312 24778 solver.cpp:237] Iteration 51600, loss = 0.162354
I0216 16:19:36.798449 24778 solver.cpp:253]     Train net output #0: loss = 0.162355 (* 1 = 0.162355 loss)
I0216 16:19:36.798476 24778 sgd_solver.cpp:106] Iteration 51600, lr = 0.001
I0216 16:19:42.112920 24778 solver.cpp:237] Iteration 51800, loss = 0.123426
I0216 16:19:42.112984 24778 solver.cpp:253]     Train net output #0: loss = 0.123426 (* 1 = 0.123426 loss)
I0216 16:19:42.113001 24778 sgd_solver.cpp:106] Iteration 51800, lr = 0.001
I0216 16:19:47.245165 24778 solver.cpp:341] Iteration 52000, Testing net (#0)
I0216 16:19:48.526018 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7521
I0216 16:19:48.526075 24778 solver.cpp:409]     Test net output #1: loss = 0.840096 (* 1 = 0.840096 loss)
I0216 16:19:48.532049 24778 solver.cpp:237] Iteration 52000, loss = 0.226005
I0216 16:19:48.532083 24778 solver.cpp:253]     Train net output #0: loss = 0.226005 (* 1 = 0.226005 loss)
I0216 16:19:48.532089 24778 sgd_solver.cpp:106] Iteration 52000, lr = 0.001
I0216 16:19:53.720022 24778 solver.cpp:237] Iteration 52200, loss = 0.16057
I0216 16:19:53.720082 24778 solver.cpp:253]     Train net output #0: loss = 0.16057 (* 1 = 0.16057 loss)
I0216 16:19:53.720099 24778 sgd_solver.cpp:106] Iteration 52200, lr = 0.001
I0216 16:19:58.873487 24778 solver.cpp:237] Iteration 52400, loss = 0.179042
I0216 16:19:58.873548 24778 solver.cpp:253]     Train net output #0: loss = 0.179042 (* 1 = 0.179042 loss)
I0216 16:19:58.873795 24778 sgd_solver.cpp:106] Iteration 52400, lr = 0.001
I0216 16:20:03.540395 24778 solver.cpp:237] Iteration 52600, loss = 0.150544
I0216 16:20:03.540455 24778 solver.cpp:253]     Train net output #0: loss = 0.150544 (* 1 = 0.150544 loss)
I0216 16:20:03.540472 24778 sgd_solver.cpp:106] Iteration 52600, lr = 0.001
I0216 16:20:08.308127 24778 solver.cpp:237] Iteration 52800, loss = 0.11961
I0216 16:20:08.308229 24778 solver.cpp:253]     Train net output #0: loss = 0.11961 (* 1 = 0.11961 loss)
I0216 16:20:08.308243 24778 sgd_solver.cpp:106] Iteration 52800, lr = 0.001
I0216 16:20:13.448612 24778 solver.cpp:341] Iteration 53000, Testing net (#0)
I0216 16:20:14.739307 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7544
I0216 16:20:14.739359 24778 solver.cpp:409]     Test net output #1: loss = 0.829909 (* 1 = 0.829909 loss)
I0216 16:20:14.745144 24778 solver.cpp:237] Iteration 53000, loss = 0.201762
I0216 16:20:14.745192 24778 solver.cpp:253]     Train net output #0: loss = 0.201762 (* 1 = 0.201762 loss)
I0216 16:20:14.745200 24778 sgd_solver.cpp:106] Iteration 53000, lr = 0.001
I0216 16:20:19.832592 24778 solver.cpp:237] Iteration 53200, loss = 0.142193
I0216 16:20:19.832656 24778 solver.cpp:253]     Train net output #0: loss = 0.142193 (* 1 = 0.142193 loss)
I0216 16:20:19.832671 24778 sgd_solver.cpp:106] Iteration 53200, lr = 0.001
I0216 16:20:25.110523 24778 solver.cpp:237] Iteration 53400, loss = 0.15506
I0216 16:20:25.110587 24778 solver.cpp:253]     Train net output #0: loss = 0.155061 (* 1 = 0.155061 loss)
I0216 16:20:25.110605 24778 sgd_solver.cpp:106] Iteration 53400, lr = 0.001
I0216 16:20:30.396523 24778 solver.cpp:237] Iteration 53600, loss = 0.13645
I0216 16:20:30.396589 24778 solver.cpp:253]     Train net output #0: loss = 0.13645 (* 1 = 0.13645 loss)
I0216 16:20:30.396607 24778 sgd_solver.cpp:106] Iteration 53600, lr = 0.001
I0216 16:20:35.658768 24778 solver.cpp:237] Iteration 53800, loss = 0.122406
I0216 16:20:35.658835 24778 solver.cpp:253]     Train net output #0: loss = 0.122406 (* 1 = 0.122406 loss)
I0216 16:20:35.659090 24778 sgd_solver.cpp:106] Iteration 53800, lr = 0.001
I0216 16:20:40.284986 24778 solver.cpp:341] Iteration 54000, Testing net (#0)
I0216 16:20:41.175734 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7629
I0216 16:20:41.176143 24778 solver.cpp:409]     Test net output #1: loss = 0.811041 (* 1 = 0.811041 loss)
I0216 16:20:41.184159 24778 solver.cpp:237] Iteration 54000, loss = 0.165165
I0216 16:20:41.184219 24778 solver.cpp:253]     Train net output #0: loss = 0.165165 (* 1 = 0.165165 loss)
I0216 16:20:41.184236 24778 sgd_solver.cpp:106] Iteration 54000, lr = 0.001
I0216 16:20:46.082797 24778 solver.cpp:237] Iteration 54200, loss = 0.139777
I0216 16:20:46.082875 24778 solver.cpp:253]     Train net output #0: loss = 0.139777 (* 1 = 0.139777 loss)
I0216 16:20:46.082886 24778 sgd_solver.cpp:106] Iteration 54200, lr = 0.001
I0216 16:20:51.433676 24778 solver.cpp:237] Iteration 54400, loss = 0.135166
I0216 16:20:51.433739 24778 solver.cpp:253]     Train net output #0: loss = 0.135166 (* 1 = 0.135166 loss)
I0216 16:20:51.433758 24778 sgd_solver.cpp:106] Iteration 54400, lr = 0.001
I0216 16:20:56.478375 24778 solver.cpp:237] Iteration 54600, loss = 0.120182
I0216 16:20:56.478437 24778 solver.cpp:253]     Train net output #0: loss = 0.120182 (* 1 = 0.120182 loss)
I0216 16:20:56.478454 24778 sgd_solver.cpp:106] Iteration 54600, lr = 0.001
I0216 16:21:01.506921 24778 solver.cpp:237] Iteration 54800, loss = 0.12549
I0216 16:21:01.506978 24778 solver.cpp:253]     Train net output #0: loss = 0.12549 (* 1 = 0.12549 loss)
I0216 16:21:01.507210 24778 sgd_solver.cpp:106] Iteration 54800, lr = 0.001
I0216 16:21:06.685411 24778 solver.cpp:341] Iteration 55000, Testing net (#0)
I0216 16:21:07.970733 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7684
I0216 16:21:07.970790 24778 solver.cpp:409]     Test net output #1: loss = 0.790465 (* 1 = 0.790465 loss)
I0216 16:21:07.981055 24778 solver.cpp:237] Iteration 55000, loss = 0.143064
I0216 16:21:07.981112 24778 solver.cpp:253]     Train net output #0: loss = 0.143064 (* 1 = 0.143064 loss)
I0216 16:21:07.981127 24778 sgd_solver.cpp:106] Iteration 55000, lr = 0.001
I0216 16:21:13.164923 24778 solver.cpp:237] Iteration 55200, loss = 0.148101
I0216 16:21:13.165038 24778 solver.cpp:253]     Train net output #0: loss = 0.148101 (* 1 = 0.148101 loss)
I0216 16:21:13.165047 24778 sgd_solver.cpp:106] Iteration 55200, lr = 0.001
I0216 16:21:17.643141 24778 solver.cpp:237] Iteration 55400, loss = 0.12873
I0216 16:21:17.643200 24778 solver.cpp:253]     Train net output #0: loss = 0.12873 (* 1 = 0.12873 loss)
I0216 16:21:17.643216 24778 sgd_solver.cpp:106] Iteration 55400, lr = 0.001
I0216 16:21:22.830520 24778 solver.cpp:237] Iteration 55600, loss = 0.107419
I0216 16:21:22.830575 24778 solver.cpp:253]     Train net output #0: loss = 0.107419 (* 1 = 0.107419 loss)
I0216 16:21:22.830585 24778 sgd_solver.cpp:106] Iteration 55600, lr = 0.001
I0216 16:21:27.976586 24778 solver.cpp:237] Iteration 55800, loss = 0.127767
I0216 16:21:27.976629 24778 solver.cpp:253]     Train net output #0: loss = 0.127767 (* 1 = 0.127767 loss)
I0216 16:21:27.976636 24778 sgd_solver.cpp:106] Iteration 55800, lr = 0.001
I0216 16:21:33.208652 24778 solver.cpp:341] Iteration 56000, Testing net (#0)
I0216 16:21:34.356367 24778 solver.cpp:409]     Test net output #0: accuracy = 0.77
I0216 16:21:34.356434 24778 solver.cpp:409]     Test net output #1: loss = 0.784779 (* 1 = 0.784779 loss)
I0216 16:21:34.362407 24778 solver.cpp:237] Iteration 56000, loss = 0.132268
I0216 16:21:34.362463 24778 solver.cpp:253]     Train net output #0: loss = 0.132268 (* 1 = 0.132268 loss)
I0216 16:21:34.362474 24778 sgd_solver.cpp:106] Iteration 56000, lr = 0.001
I0216 16:21:39.605731 24778 solver.cpp:237] Iteration 56200, loss = 0.149784
I0216 16:21:39.605792 24778 solver.cpp:253]     Train net output #0: loss = 0.149784 (* 1 = 0.149784 loss)
I0216 16:21:39.605809 24778 sgd_solver.cpp:106] Iteration 56200, lr = 0.001
I0216 16:21:44.715437 24778 solver.cpp:237] Iteration 56400, loss = 0.119039
I0216 16:21:44.715534 24778 solver.cpp:253]     Train net output #0: loss = 0.119039 (* 1 = 0.119039 loss)
I0216 16:21:44.715546 24778 sgd_solver.cpp:106] Iteration 56400, lr = 0.001
I0216 16:21:49.996165 24778 solver.cpp:237] Iteration 56600, loss = 0.120197
I0216 16:21:49.996228 24778 solver.cpp:253]     Train net output #0: loss = 0.120197 (* 1 = 0.120197 loss)
I0216 16:21:49.996242 24778 sgd_solver.cpp:106] Iteration 56600, lr = 0.001
I0216 16:21:54.526418 24778 solver.cpp:237] Iteration 56800, loss = 0.127977
I0216 16:21:54.526463 24778 solver.cpp:253]     Train net output #0: loss = 0.127977 (* 1 = 0.127977 loss)
I0216 16:21:54.526471 24778 sgd_solver.cpp:106] Iteration 56800, lr = 0.001
I0216 16:21:59.817534 24778 solver.cpp:341] Iteration 57000, Testing net (#0)
I0216 16:22:01.132148 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7699
I0216 16:22:01.132190 24778 solver.cpp:409]     Test net output #1: loss = 0.798305 (* 1 = 0.798305 loss)
I0216 16:22:01.137923 24778 solver.cpp:237] Iteration 57000, loss = 0.128803
I0216 16:22:01.137989 24778 solver.cpp:253]     Train net output #0: loss = 0.128803 (* 1 = 0.128803 loss)
I0216 16:22:01.138008 24778 sgd_solver.cpp:106] Iteration 57000, lr = 0.001
I0216 16:22:06.477180 24778 solver.cpp:237] Iteration 57200, loss = 0.159104
I0216 16:22:06.477246 24778 solver.cpp:253]     Train net output #0: loss = 0.159104 (* 1 = 0.159104 loss)
I0216 16:22:06.477263 24778 sgd_solver.cpp:106] Iteration 57200, lr = 0.001
I0216 16:22:11.846633 24778 solver.cpp:237] Iteration 57400, loss = 0.111048
I0216 16:22:11.846709 24778 solver.cpp:253]     Train net output #0: loss = 0.111048 (* 1 = 0.111048 loss)
I0216 16:22:11.846952 24778 sgd_solver.cpp:106] Iteration 57400, lr = 0.001
I0216 16:22:17.051215 24778 solver.cpp:237] Iteration 57600, loss = 0.121886
I0216 16:22:17.051342 24778 solver.cpp:253]     Train net output #0: loss = 0.121886 (* 1 = 0.121886 loss)
I0216 16:22:17.051738 24778 sgd_solver.cpp:106] Iteration 57600, lr = 0.001
I0216 16:22:22.264166 24778 solver.cpp:237] Iteration 57800, loss = 0.121806
I0216 16:22:22.264225 24778 solver.cpp:253]     Train net output #0: loss = 0.121806 (* 1 = 0.121806 loss)
I0216 16:22:22.264241 24778 sgd_solver.cpp:106] Iteration 57800, lr = 0.001
I0216 16:22:27.395881 24778 solver.cpp:341] Iteration 58000, Testing net (#0)
I0216 16:22:28.581261 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7638
I0216 16:22:28.581321 24778 solver.cpp:409]     Test net output #1: loss = 0.823081 (* 1 = 0.823081 loss)
I0216 16:22:28.588919 24778 solver.cpp:237] Iteration 58000, loss = 0.138977
I0216 16:22:28.588979 24778 solver.cpp:253]     Train net output #0: loss = 0.138977 (* 1 = 0.138977 loss)
I0216 16:22:28.588994 24778 sgd_solver.cpp:106] Iteration 58000, lr = 0.001
I0216 16:22:33.235980 24778 solver.cpp:237] Iteration 58200, loss = 0.179569
I0216 16:22:33.236042 24778 solver.cpp:253]     Train net output #0: loss = 0.179569 (* 1 = 0.179569 loss)
I0216 16:22:33.236060 24778 sgd_solver.cpp:106] Iteration 58200, lr = 0.001
I0216 16:22:38.255101 24778 solver.cpp:237] Iteration 58400, loss = 0.114789
I0216 16:22:38.255164 24778 solver.cpp:253]     Train net output #0: loss = 0.114789 (* 1 = 0.114789 loss)
I0216 16:22:38.255177 24778 sgd_solver.cpp:106] Iteration 58400, lr = 0.001
I0216 16:22:43.497628 24778 solver.cpp:237] Iteration 58600, loss = 0.117056
I0216 16:22:43.497689 24778 solver.cpp:253]     Train net output #0: loss = 0.117057 (* 1 = 0.117057 loss)
I0216 16:22:43.497702 24778 sgd_solver.cpp:106] Iteration 58600, lr = 0.001
I0216 16:22:48.430686 24778 solver.cpp:237] Iteration 58800, loss = 0.143953
I0216 16:22:48.430819 24778 solver.cpp:253]     Train net output #0: loss = 0.143953 (* 1 = 0.143953 loss)
I0216 16:22:48.430836 24778 sgd_solver.cpp:106] Iteration 58800, lr = 0.001
I0216 16:22:53.696195 24778 solver.cpp:341] Iteration 59000, Testing net (#0)
I0216 16:22:54.878589 24778 solver.cpp:409]     Test net output #0: accuracy = 0.756
I0216 16:22:54.878653 24778 solver.cpp:409]     Test net output #1: loss = 0.88634 (* 1 = 0.88634 loss)
I0216 16:22:54.887645 24778 solver.cpp:237] Iteration 59000, loss = 0.182229
I0216 16:22:54.887684 24778 solver.cpp:253]     Train net output #0: loss = 0.182229 (* 1 = 0.182229 loss)
I0216 16:22:54.887691 24778 sgd_solver.cpp:106] Iteration 59000, lr = 0.001
I0216 16:23:00.135550 24778 solver.cpp:237] Iteration 59200, loss = 0.189052
I0216 16:23:00.135592 24778 solver.cpp:253]     Train net output #0: loss = 0.189052 (* 1 = 0.189052 loss)
I0216 16:23:00.135601 24778 sgd_solver.cpp:106] Iteration 59200, lr = 0.001
I0216 16:23:05.225271 24778 solver.cpp:237] Iteration 59400, loss = 0.116231
I0216 16:23:05.225332 24778 solver.cpp:253]     Train net output #0: loss = 0.116231 (* 1 = 0.116231 loss)
I0216 16:23:05.225358 24778 sgd_solver.cpp:106] Iteration 59400, lr = 0.001
I0216 16:23:09.970924 24778 solver.cpp:237] Iteration 59600, loss = 0.122886
I0216 16:23:09.970986 24778 solver.cpp:253]     Train net output #0: loss = 0.122887 (* 1 = 0.122887 loss)
I0216 16:23:09.971004 24778 sgd_solver.cpp:106] Iteration 59600, lr = 0.001
I0216 16:23:15.172765 24778 solver.cpp:237] Iteration 59800, loss = 0.142973
I0216 16:23:15.172823 24778 solver.cpp:253]     Train net output #0: loss = 0.142973 (* 1 = 0.142973 loss)
I0216 16:23:15.172839 24778 sgd_solver.cpp:106] Iteration 59800, lr = 0.001
I0216 16:23:20.379555 24778 solver.cpp:472] Snapshotting to HDF5 file examples/cifar10/cifar10_full_iter_60000.caffemodel.h5
I0216 16:23:20.983655 24778 sgd_solver.cpp:279] Snapshotting solver state to HDF5 file examples/cifar10/cifar10_full_iter_60000.solverstate.h5
I0216 16:23:21.004003 24778 solver.cpp:321] Iteration 60000, loss = 0.219002
I0216 16:23:21.004041 24778 solver.cpp:341] Iteration 60000, Testing net (#0)
I0216 16:23:22.161900 24778 solver.cpp:409]     Test net output #0: accuracy = 0.7483
I0216 16:23:22.161964 24778 solver.cpp:409]     Test net output #1: loss = 0.929465 (* 1 = 0.929465 loss)
I0216 16:23:22.161979 24778 solver.cpp:326] Optimization Done.
I0216 16:23:22.161988 24778 caffe.cpp:215] Optimization Done.
I0216 16:23:22.849369 25531 caffe.cpp:184] Using GPUs 0
I0216 16:23:23.147037 25531 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 1000
base_lr: 0.0001
display: 200
max_iter: 65000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.004
snapshot: 5000
snapshot_prefix: "examples/cifar10/cifar10_full"
solver_mode: GPU
device_id: 0
net: "examples/cifar10/cifar10_full_train_test.prototxt"
snapshot_format: HDF5
I0216 16:23:23.147152 25531 solver.cpp:91] Creating training net from net file: examples/cifar10/cifar10_full_train_test.prototxt
I0216 16:23:23.147541 25531 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0216 16:23:23.147560 25531 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0216 16:23:23.147658 25531 net.cpp:49] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TRAIN
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0216 16:23:23.147740 25531 layer_factory.hpp:77] Creating layer cifar
I0216 16:23:23.154260 25531 net.cpp:106] Creating Layer cifar
I0216 16:23:23.154314 25531 net.cpp:411] cifar -> data
I0216 16:23:23.154369 25531 net.cpp:411] cifar -> label
I0216 16:23:23.154399 25531 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0216 16:23:23.155218 25535 db_lmdb.cpp:38] Opened lmdb examples/cifar10/cifar10_train_lmdb
I0216 16:23:23.178167 25531 data_layer.cpp:41] output data size: 100,3,32,32
I0216 16:23:23.188915 25531 net.cpp:150] Setting up cifar
I0216 16:23:23.188963 25531 net.cpp:157] Top shape: 100 3 32 32 (307200)
I0216 16:23:23.188972 25531 net.cpp:157] Top shape: 100 (100)
I0216 16:23:23.188977 25531 net.cpp:165] Memory required for data: 1229200
I0216 16:23:23.188990 25531 layer_factory.hpp:77] Creating layer conv1
I0216 16:23:23.189016 25531 net.cpp:106] Creating Layer conv1
I0216 16:23:23.189025 25531 net.cpp:454] conv1 <- data
I0216 16:23:23.189040 25531 net.cpp:411] conv1 -> conv1
I0216 16:23:23.456328 25531 net.cpp:150] Setting up conv1
I0216 16:23:23.456374 25531 net.cpp:157] Top shape: 100 32 32 32 (3276800)
I0216 16:23:23.456382 25531 net.cpp:165] Memory required for data: 14336400
I0216 16:23:23.456405 25531 layer_factory.hpp:77] Creating layer pool1
I0216 16:23:23.456424 25531 net.cpp:106] Creating Layer pool1
I0216 16:23:23.456431 25531 net.cpp:454] pool1 <- conv1
I0216 16:23:23.456440 25531 net.cpp:411] pool1 -> pool1
I0216 16:23:23.457314 25531 net.cpp:150] Setting up pool1
I0216 16:23:23.457337 25531 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 16:23:23.457346 25531 net.cpp:165] Memory required for data: 17613200
I0216 16:23:23.457355 25531 layer_factory.hpp:77] Creating layer relu1
I0216 16:23:23.457365 25531 net.cpp:106] Creating Layer relu1
I0216 16:23:23.457372 25531 net.cpp:454] relu1 <- pool1
I0216 16:23:23.457381 25531 net.cpp:397] relu1 -> pool1 (in-place)
I0216 16:23:23.458118 25531 net.cpp:150] Setting up relu1
I0216 16:23:23.458135 25531 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 16:23:23.458142 25531 net.cpp:165] Memory required for data: 20890000
I0216 16:23:23.458148 25531 layer_factory.hpp:77] Creating layer norm1
I0216 16:23:23.458161 25531 net.cpp:106] Creating Layer norm1
I0216 16:23:23.458168 25531 net.cpp:454] norm1 <- pool1
I0216 16:23:23.458178 25531 net.cpp:411] norm1 -> norm1
I0216 16:23:23.459362 25531 net.cpp:150] Setting up norm1
I0216 16:23:23.459384 25531 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 16:23:23.459391 25531 net.cpp:165] Memory required for data: 24166800
I0216 16:23:23.459399 25531 layer_factory.hpp:77] Creating layer conv2
I0216 16:23:23.459416 25531 net.cpp:106] Creating Layer conv2
I0216 16:23:23.459424 25531 net.cpp:454] conv2 <- norm1
I0216 16:23:23.459449 25531 net.cpp:411] conv2 -> conv2
I0216 16:23:23.464643 25531 net.cpp:150] Setting up conv2
I0216 16:23:23.464686 25531 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 16:23:23.464694 25531 net.cpp:165] Memory required for data: 27443600
I0216 16:23:23.464715 25531 layer_factory.hpp:77] Creating layer relu2
I0216 16:23:23.464730 25531 net.cpp:106] Creating Layer relu2
I0216 16:23:23.464737 25531 net.cpp:454] relu2 <- conv2
I0216 16:23:23.464750 25531 net.cpp:397] relu2 -> conv2 (in-place)
I0216 16:23:23.465776 25531 net.cpp:150] Setting up relu2
I0216 16:23:23.465795 25531 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 16:23:23.465801 25531 net.cpp:165] Memory required for data: 30720400
I0216 16:23:23.465806 25531 layer_factory.hpp:77] Creating layer pool2
I0216 16:23:23.465817 25531 net.cpp:106] Creating Layer pool2
I0216 16:23:23.465824 25531 net.cpp:454] pool2 <- conv2
I0216 16:23:23.465832 25531 net.cpp:411] pool2 -> pool2
I0216 16:23:23.467227 25531 net.cpp:150] Setting up pool2
I0216 16:23:23.467247 25531 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0216 16:23:23.467253 25531 net.cpp:165] Memory required for data: 31539600
I0216 16:23:23.467260 25531 layer_factory.hpp:77] Creating layer norm2
I0216 16:23:23.467278 25531 net.cpp:106] Creating Layer norm2
I0216 16:23:23.467284 25531 net.cpp:454] norm2 <- pool2
I0216 16:23:23.467296 25531 net.cpp:411] norm2 -> norm2
I0216 16:23:23.468940 25531 net.cpp:150] Setting up norm2
I0216 16:23:23.468973 25531 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0216 16:23:23.468981 25531 net.cpp:165] Memory required for data: 32358800
I0216 16:23:23.468986 25531 layer_factory.hpp:77] Creating layer conv3
I0216 16:23:23.469000 25531 net.cpp:106] Creating Layer conv3
I0216 16:23:23.469007 25531 net.cpp:454] conv3 <- norm2
I0216 16:23:23.469017 25531 net.cpp:411] conv3 -> conv3
I0216 16:23:23.473415 25531 net.cpp:150] Setting up conv3
I0216 16:23:23.473459 25531 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0216 16:23:23.473466 25531 net.cpp:165] Memory required for data: 33997200
I0216 16:23:23.473486 25531 layer_factory.hpp:77] Creating layer relu3
I0216 16:23:23.473501 25531 net.cpp:106] Creating Layer relu3
I0216 16:23:23.473510 25531 net.cpp:454] relu3 <- conv3
I0216 16:23:23.473520 25531 net.cpp:397] relu3 -> conv3 (in-place)
I0216 16:23:23.474550 25531 net.cpp:150] Setting up relu3
I0216 16:23:23.474567 25531 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0216 16:23:23.474572 25531 net.cpp:165] Memory required for data: 35635600
I0216 16:23:23.474580 25531 layer_factory.hpp:77] Creating layer pool3
I0216 16:23:23.474591 25531 net.cpp:106] Creating Layer pool3
I0216 16:23:23.474597 25531 net.cpp:454] pool3 <- conv3
I0216 16:23:23.474606 25531 net.cpp:411] pool3 -> pool3
I0216 16:23:23.475986 25531 net.cpp:150] Setting up pool3
I0216 16:23:23.476007 25531 net.cpp:157] Top shape: 100 64 4 4 (102400)
I0216 16:23:23.476013 25531 net.cpp:165] Memory required for data: 36045200
I0216 16:23:23.476019 25531 layer_factory.hpp:77] Creating layer ip1
I0216 16:23:23.476032 25531 net.cpp:106] Creating Layer ip1
I0216 16:23:23.476038 25531 net.cpp:454] ip1 <- pool3
I0216 16:23:23.476048 25531 net.cpp:411] ip1 -> ip1
I0216 16:23:23.493190 25531 net.cpp:150] Setting up ip1
I0216 16:23:23.493235 25531 net.cpp:157] Top shape: 100 500 (50000)
I0216 16:23:23.493243 25531 net.cpp:165] Memory required for data: 36245200
I0216 16:23:23.493258 25531 layer_factory.hpp:77] Creating layer relu4
I0216 16:23:23.493273 25531 net.cpp:106] Creating Layer relu4
I0216 16:23:23.493283 25531 net.cpp:454] relu4 <- ip1
I0216 16:23:23.493296 25531 net.cpp:397] relu4 -> ip1 (in-place)
I0216 16:23:23.494266 25531 net.cpp:150] Setting up relu4
I0216 16:23:23.494287 25531 net.cpp:157] Top shape: 100 500 (50000)
I0216 16:23:23.494295 25531 net.cpp:165] Memory required for data: 36445200
I0216 16:23:23.494303 25531 layer_factory.hpp:77] Creating layer ip2
I0216 16:23:23.494318 25531 net.cpp:106] Creating Layer ip2
I0216 16:23:23.494326 25531 net.cpp:454] ip2 <- ip1
I0216 16:23:23.494351 25531 net.cpp:411] ip2 -> ip2
I0216 16:23:23.495146 25531 net.cpp:150] Setting up ip2
I0216 16:23:23.495164 25531 net.cpp:157] Top shape: 100 10 (1000)
I0216 16:23:23.495170 25531 net.cpp:165] Memory required for data: 36449200
I0216 16:23:23.495187 25531 layer_factory.hpp:77] Creating layer loss
I0216 16:23:23.495203 25531 net.cpp:106] Creating Layer loss
I0216 16:23:23.495210 25531 net.cpp:454] loss <- ip2
I0216 16:23:23.495218 25531 net.cpp:454] loss <- label
I0216 16:23:23.495226 25531 net.cpp:411] loss -> loss
I0216 16:23:23.495239 25531 layer_factory.hpp:77] Creating layer loss
I0216 16:23:23.496096 25531 net.cpp:150] Setting up loss
I0216 16:23:23.496112 25531 net.cpp:157] Top shape: (1)
I0216 16:23:23.496119 25531 net.cpp:160]     with loss weight 1
I0216 16:23:23.496136 25531 net.cpp:165] Memory required for data: 36449204
I0216 16:23:23.496142 25531 net.cpp:226] loss needs backward computation.
I0216 16:23:23.496150 25531 net.cpp:226] ip2 needs backward computation.
I0216 16:23:23.496156 25531 net.cpp:226] relu4 needs backward computation.
I0216 16:23:23.496161 25531 net.cpp:226] ip1 needs backward computation.
I0216 16:23:23.496167 25531 net.cpp:226] pool3 needs backward computation.
I0216 16:23:23.496173 25531 net.cpp:226] relu3 needs backward computation.
I0216 16:23:23.496178 25531 net.cpp:226] conv3 needs backward computation.
I0216 16:23:23.496183 25531 net.cpp:226] norm2 needs backward computation.
I0216 16:23:23.496203 25531 net.cpp:226] pool2 needs backward computation.
I0216 16:23:23.496209 25531 net.cpp:226] relu2 needs backward computation.
I0216 16:23:23.496214 25531 net.cpp:226] conv2 needs backward computation.
I0216 16:23:23.496219 25531 net.cpp:226] norm1 needs backward computation.
I0216 16:23:23.496225 25531 net.cpp:226] relu1 needs backward computation.
I0216 16:23:23.496230 25531 net.cpp:226] pool1 needs backward computation.
I0216 16:23:23.496235 25531 net.cpp:226] conv1 needs backward computation.
I0216 16:23:23.496242 25531 net.cpp:228] cifar does not need backward computation.
I0216 16:23:23.496247 25531 net.cpp:270] This network produces output loss
I0216 16:23:23.496263 25531 net.cpp:283] Network initialization done.
I0216 16:23:23.496937 25531 solver.cpp:181] Creating test net (#0) specified by net file: examples/cifar10/cifar10_full_train_test.prototxt
I0216 16:23:23.496989 25531 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0216 16:23:23.497170 25531 net.cpp:49] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0216 16:23:23.497305 25531 layer_factory.hpp:77] Creating layer cifar
I0216 16:23:23.497448 25531 net.cpp:106] Creating Layer cifar
I0216 16:23:23.497465 25531 net.cpp:411] cifar -> data
I0216 16:23:23.497480 25531 net.cpp:411] cifar -> label
I0216 16:23:23.497493 25531 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0216 16:23:23.498531 25537 db_lmdb.cpp:38] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0216 16:23:23.498693 25531 data_layer.cpp:41] output data size: 100,3,32,32
I0216 16:23:23.504834 25531 net.cpp:150] Setting up cifar
I0216 16:23:23.504879 25531 net.cpp:157] Top shape: 100 3 32 32 (307200)
I0216 16:23:23.504887 25531 net.cpp:157] Top shape: 100 (100)
I0216 16:23:23.504894 25531 net.cpp:165] Memory required for data: 1229200
I0216 16:23:23.504901 25531 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0216 16:23:23.504917 25531 net.cpp:106] Creating Layer label_cifar_1_split
I0216 16:23:23.504923 25531 net.cpp:454] label_cifar_1_split <- label
I0216 16:23:23.504932 25531 net.cpp:411] label_cifar_1_split -> label_cifar_1_split_0
I0216 16:23:23.504945 25531 net.cpp:411] label_cifar_1_split -> label_cifar_1_split_1
I0216 16:23:23.505074 25531 net.cpp:150] Setting up label_cifar_1_split
I0216 16:23:23.505086 25531 net.cpp:157] Top shape: 100 (100)
I0216 16:23:23.505092 25531 net.cpp:157] Top shape: 100 (100)
I0216 16:23:23.505097 25531 net.cpp:165] Memory required for data: 1230000
I0216 16:23:23.505103 25531 layer_factory.hpp:77] Creating layer conv1
I0216 16:23:23.505120 25531 net.cpp:106] Creating Layer conv1
I0216 16:23:23.505125 25531 net.cpp:454] conv1 <- data
I0216 16:23:23.505136 25531 net.cpp:411] conv1 -> conv1
I0216 16:23:23.508164 25531 net.cpp:150] Setting up conv1
I0216 16:23:23.508190 25531 net.cpp:157] Top shape: 100 32 32 32 (3276800)
I0216 16:23:23.508196 25531 net.cpp:165] Memory required for data: 14337200
I0216 16:23:23.508211 25531 layer_factory.hpp:77] Creating layer pool1
I0216 16:23:23.508224 25531 net.cpp:106] Creating Layer pool1
I0216 16:23:23.508231 25531 net.cpp:454] pool1 <- conv1
I0216 16:23:23.508239 25531 net.cpp:411] pool1 -> pool1
I0216 16:23:23.510876 25531 net.cpp:150] Setting up pool1
I0216 16:23:23.510901 25531 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 16:23:23.510911 25531 net.cpp:165] Memory required for data: 17614000
I0216 16:23:23.510918 25531 layer_factory.hpp:77] Creating layer relu1
I0216 16:23:23.510942 25531 net.cpp:106] Creating Layer relu1
I0216 16:23:23.510949 25531 net.cpp:454] relu1 <- pool1
I0216 16:23:23.510957 25531 net.cpp:397] relu1 -> pool1 (in-place)
I0216 16:23:23.511785 25531 net.cpp:150] Setting up relu1
I0216 16:23:23.511804 25531 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 16:23:23.511816 25531 net.cpp:165] Memory required for data: 20890800
I0216 16:23:23.511823 25531 layer_factory.hpp:77] Creating layer norm1
I0216 16:23:23.511837 25531 net.cpp:106] Creating Layer norm1
I0216 16:23:23.511845 25531 net.cpp:454] norm1 <- pool1
I0216 16:23:23.511854 25531 net.cpp:411] norm1 -> norm1
I0216 16:23:23.513110 25531 net.cpp:150] Setting up norm1
I0216 16:23:23.513128 25531 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 16:23:23.513134 25531 net.cpp:165] Memory required for data: 24167600
I0216 16:23:23.513139 25531 layer_factory.hpp:77] Creating layer conv2
I0216 16:23:23.513154 25531 net.cpp:106] Creating Layer conv2
I0216 16:23:23.513160 25531 net.cpp:454] conv2 <- norm1
I0216 16:23:23.513170 25531 net.cpp:411] conv2 -> conv2
I0216 16:23:23.517165 25531 net.cpp:150] Setting up conv2
I0216 16:23:23.517210 25531 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 16:23:23.517217 25531 net.cpp:165] Memory required for data: 27444400
I0216 16:23:23.517235 25531 layer_factory.hpp:77] Creating layer relu2
I0216 16:23:23.517247 25531 net.cpp:106] Creating Layer relu2
I0216 16:23:23.517269 25531 net.cpp:454] relu2 <- conv2
I0216 16:23:23.517278 25531 net.cpp:397] relu2 -> conv2 (in-place)
I0216 16:23:23.518050 25531 net.cpp:150] Setting up relu2
I0216 16:23:23.518067 25531 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 16:23:23.518074 25531 net.cpp:165] Memory required for data: 30721200
I0216 16:23:23.518079 25531 layer_factory.hpp:77] Creating layer pool2
I0216 16:23:23.518091 25531 net.cpp:106] Creating Layer pool2
I0216 16:23:23.518097 25531 net.cpp:454] pool2 <- conv2
I0216 16:23:23.518108 25531 net.cpp:411] pool2 -> pool2
I0216 16:23:23.518894 25531 net.cpp:150] Setting up pool2
I0216 16:23:23.518913 25531 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0216 16:23:23.518918 25531 net.cpp:165] Memory required for data: 31540400
I0216 16:23:23.518925 25531 layer_factory.hpp:77] Creating layer norm2
I0216 16:23:23.518937 25531 net.cpp:106] Creating Layer norm2
I0216 16:23:23.518944 25531 net.cpp:454] norm2 <- pool2
I0216 16:23:23.518952 25531 net.cpp:411] norm2 -> norm2
I0216 16:23:23.520069 25531 net.cpp:150] Setting up norm2
I0216 16:23:23.520087 25531 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0216 16:23:23.520092 25531 net.cpp:165] Memory required for data: 32359600
I0216 16:23:23.520098 25531 layer_factory.hpp:77] Creating layer conv3
I0216 16:23:23.520112 25531 net.cpp:106] Creating Layer conv3
I0216 16:23:23.520119 25531 net.cpp:454] conv3 <- norm2
I0216 16:23:23.520129 25531 net.cpp:411] conv3 -> conv3
I0216 16:23:23.525202 25531 net.cpp:150] Setting up conv3
I0216 16:23:23.525239 25531 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0216 16:23:23.525246 25531 net.cpp:165] Memory required for data: 33998000
I0216 16:23:23.525264 25531 layer_factory.hpp:77] Creating layer relu3
I0216 16:23:23.525277 25531 net.cpp:106] Creating Layer relu3
I0216 16:23:23.525285 25531 net.cpp:454] relu3 <- conv3
I0216 16:23:23.525295 25531 net.cpp:397] relu3 -> conv3 (in-place)
I0216 16:23:23.526522 25531 net.cpp:150] Setting up relu3
I0216 16:23:23.526538 25531 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0216 16:23:23.526543 25531 net.cpp:165] Memory required for data: 35636400
I0216 16:23:23.526551 25531 layer_factory.hpp:77] Creating layer pool3
I0216 16:23:23.526563 25531 net.cpp:106] Creating Layer pool3
I0216 16:23:23.526569 25531 net.cpp:454] pool3 <- conv3
I0216 16:23:23.526577 25531 net.cpp:411] pool3 -> pool3
I0216 16:23:23.528251 25531 net.cpp:150] Setting up pool3
I0216 16:23:23.528295 25531 net.cpp:157] Top shape: 100 64 4 4 (102400)
I0216 16:23:23.528301 25531 net.cpp:165] Memory required for data: 36046000
I0216 16:23:23.528309 25531 layer_factory.hpp:77] Creating layer ip1
I0216 16:23:23.528337 25531 net.cpp:106] Creating Layer ip1
I0216 16:23:23.528347 25531 net.cpp:454] ip1 <- pool3
I0216 16:23:23.528362 25531 net.cpp:411] ip1 -> ip1
I0216 16:23:23.545840 25531 net.cpp:150] Setting up ip1
I0216 16:23:23.545881 25531 net.cpp:157] Top shape: 100 500 (50000)
I0216 16:23:23.545887 25531 net.cpp:165] Memory required for data: 36246000
I0216 16:23:23.545900 25531 layer_factory.hpp:77] Creating layer relu4
I0216 16:23:23.545913 25531 net.cpp:106] Creating Layer relu4
I0216 16:23:23.545920 25531 net.cpp:454] relu4 <- ip1
I0216 16:23:23.545933 25531 net.cpp:397] relu4 -> ip1 (in-place)
I0216 16:23:23.546882 25531 net.cpp:150] Setting up relu4
I0216 16:23:23.546905 25531 net.cpp:157] Top shape: 100 500 (50000)
I0216 16:23:23.546911 25531 net.cpp:165] Memory required for data: 36446000
I0216 16:23:23.546916 25531 layer_factory.hpp:77] Creating layer ip2
I0216 16:23:23.546936 25531 net.cpp:106] Creating Layer ip2
I0216 16:23:23.546942 25531 net.cpp:454] ip2 <- ip1
I0216 16:23:23.546952 25531 net.cpp:411] ip2 -> ip2
I0216 16:23:23.547260 25531 net.cpp:150] Setting up ip2
I0216 16:23:23.547274 25531 net.cpp:157] Top shape: 100 10 (1000)
I0216 16:23:23.547279 25531 net.cpp:165] Memory required for data: 36450000
I0216 16:23:23.547294 25531 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0216 16:23:23.547305 25531 net.cpp:106] Creating Layer ip2_ip2_0_split
I0216 16:23:23.547312 25531 net.cpp:454] ip2_ip2_0_split <- ip2
I0216 16:23:23.547334 25531 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0216 16:23:23.547345 25531 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0216 16:23:23.547391 25531 net.cpp:150] Setting up ip2_ip2_0_split
I0216 16:23:23.547401 25531 net.cpp:157] Top shape: 100 10 (1000)
I0216 16:23:23.547407 25531 net.cpp:157] Top shape: 100 10 (1000)
I0216 16:23:23.547412 25531 net.cpp:165] Memory required for data: 36458000
I0216 16:23:23.547418 25531 layer_factory.hpp:77] Creating layer accuracy
I0216 16:23:23.547428 25531 net.cpp:106] Creating Layer accuracy
I0216 16:23:23.547435 25531 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I0216 16:23:23.547441 25531 net.cpp:454] accuracy <- label_cifar_1_split_0
I0216 16:23:23.547449 25531 net.cpp:411] accuracy -> accuracy
I0216 16:23:23.547463 25531 net.cpp:150] Setting up accuracy
I0216 16:23:23.547472 25531 net.cpp:157] Top shape: (1)
I0216 16:23:23.547477 25531 net.cpp:165] Memory required for data: 36458004
I0216 16:23:23.547482 25531 layer_factory.hpp:77] Creating layer loss
I0216 16:23:23.547490 25531 net.cpp:106] Creating Layer loss
I0216 16:23:23.547495 25531 net.cpp:454] loss <- ip2_ip2_0_split_1
I0216 16:23:23.547502 25531 net.cpp:454] loss <- label_cifar_1_split_1
I0216 16:23:23.547509 25531 net.cpp:411] loss -> loss
I0216 16:23:23.547520 25531 layer_factory.hpp:77] Creating layer loss
I0216 16:23:23.548523 25531 net.cpp:150] Setting up loss
I0216 16:23:23.548538 25531 net.cpp:157] Top shape: (1)
I0216 16:23:23.548545 25531 net.cpp:160]     with loss weight 1
I0216 16:23:23.548554 25531 net.cpp:165] Memory required for data: 36458008
I0216 16:23:23.548560 25531 net.cpp:226] loss needs backward computation.
I0216 16:23:23.548568 25531 net.cpp:228] accuracy does not need backward computation.
I0216 16:23:23.548573 25531 net.cpp:226] ip2_ip2_0_split needs backward computation.
I0216 16:23:23.548578 25531 net.cpp:226] ip2 needs backward computation.
I0216 16:23:23.548583 25531 net.cpp:226] relu4 needs backward computation.
I0216 16:23:23.548589 25531 net.cpp:226] ip1 needs backward computation.
I0216 16:23:23.548594 25531 net.cpp:226] pool3 needs backward computation.
I0216 16:23:23.548600 25531 net.cpp:226] relu3 needs backward computation.
I0216 16:23:23.548605 25531 net.cpp:226] conv3 needs backward computation.
I0216 16:23:23.548610 25531 net.cpp:226] norm2 needs backward computation.
I0216 16:23:23.548616 25531 net.cpp:226] pool2 needs backward computation.
I0216 16:23:23.548621 25531 net.cpp:226] relu2 needs backward computation.
I0216 16:23:23.548627 25531 net.cpp:226] conv2 needs backward computation.
I0216 16:23:23.548641 25531 net.cpp:226] norm1 needs backward computation.
I0216 16:23:23.548647 25531 net.cpp:226] relu1 needs backward computation.
I0216 16:23:23.548652 25531 net.cpp:226] pool1 needs backward computation.
I0216 16:23:23.548658 25531 net.cpp:226] conv1 needs backward computation.
I0216 16:23:23.548665 25531 net.cpp:228] label_cifar_1_split does not need backward computation.
I0216 16:23:23.548671 25531 net.cpp:228] cifar does not need backward computation.
I0216 16:23:23.548676 25531 net.cpp:270] This network produces output accuracy
I0216 16:23:23.548681 25531 net.cpp:270] This network produces output loss
I0216 16:23:23.548698 25531 net.cpp:283] Network initialization done.
I0216 16:23:23.548807 25531 solver.cpp:60] Solver scaffolding done.
I0216 16:23:23.549231 25531 caffe.cpp:202] Resuming from examples/cifar10/cifar10_full_iter_60000.solverstate.h5
I0216 16:23:23.550523 25531 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0216 16:23:23.554204 25531 caffe.cpp:212] Starting Optimization
I0216 16:23:23.554239 25531 solver.cpp:288] Solving CIFAR10_full
I0216 16:23:23.554245 25531 solver.cpp:289] Learning Rate Policy: fixed
I0216 16:23:23.554978 25531 solver.cpp:341] Iteration 60000, Testing net (#0)
I0216 16:23:24.246836 25531 blocking_queue.cpp:50] Data layer prefetch queue empty
I0216 16:23:24.696935 25531 solver.cpp:409]     Test net output #0: accuracy = 0.7483
I0216 16:23:24.696993 25531 solver.cpp:409]     Test net output #1: loss = 0.929465 (* 1 = 0.929465 loss)
I0216 16:23:24.705186 25531 solver.cpp:237] Iteration 60000, loss = 0.219002
I0216 16:23:24.705243 25531 solver.cpp:253]     Train net output #0: loss = 0.219002 (* 1 = 0.219002 loss)
I0216 16:23:24.705255 25531 sgd_solver.cpp:106] Iteration 60000, lr = 0.0001
I0216 16:23:29.634315 25531 solver.cpp:237] Iteration 60200, loss = 0.176237
I0216 16:23:29.634378 25531 solver.cpp:253]     Train net output #0: loss = 0.176237 (* 1 = 0.176237 loss)
I0216 16:23:29.634395 25531 sgd_solver.cpp:106] Iteration 60200, lr = 0.0001
I0216 16:23:34.950290 25531 solver.cpp:237] Iteration 60400, loss = 0.134159
I0216 16:23:34.950351 25531 solver.cpp:253]     Train net output #0: loss = 0.134159 (* 1 = 0.134159 loss)
I0216 16:23:34.950368 25531 sgd_solver.cpp:106] Iteration 60400, lr = 0.0001
I0216 16:23:39.970552 25531 solver.cpp:237] Iteration 60600, loss = 0.11614
I0216 16:23:39.970613 25531 solver.cpp:253]     Train net output #0: loss = 0.11614 (* 1 = 0.11614 loss)
I0216 16:23:39.970875 25531 sgd_solver.cpp:106] Iteration 60600, lr = 0.0001
I0216 16:23:44.590790 25531 solver.cpp:237] Iteration 60800, loss = 0.136918
I0216 16:23:44.590838 25531 solver.cpp:253]     Train net output #0: loss = 0.136918 (* 1 = 0.136918 loss)
I0216 16:23:44.590847 25531 sgd_solver.cpp:106] Iteration 60800, lr = 0.0001
I0216 16:23:49.949280 25531 solver.cpp:341] Iteration 61000, Testing net (#0)
I0216 16:23:51.313516 25531 solver.cpp:409]     Test net output #0: accuracy = 0.7954
I0216 16:23:51.313576 25531 solver.cpp:409]     Test net output #1: loss = 0.625074 (* 1 = 0.625074 loss)
I0216 16:23:51.321403 25531 solver.cpp:237] Iteration 61000, loss = 0.113033
I0216 16:23:51.321461 25531 solver.cpp:253]     Train net output #0: loss = 0.113033 (* 1 = 0.113033 loss)
I0216 16:23:51.321478 25531 sgd_solver.cpp:106] Iteration 61000, lr = 0.0001
I0216 16:23:56.542223 25531 solver.cpp:237] Iteration 61200, loss = 0.158177
I0216 16:23:56.542330 25531 solver.cpp:253]     Train net output #0: loss = 0.158177 (* 1 = 0.158177 loss)
I0216 16:23:56.542578 25531 sgd_solver.cpp:106] Iteration 61200, lr = 0.0001
I0216 16:24:01.859067 25531 solver.cpp:237] Iteration 61400, loss = 0.125965
I0216 16:24:01.859134 25531 solver.cpp:253]     Train net output #0: loss = 0.125965 (* 1 = 0.125965 loss)
I0216 16:24:01.859151 25531 sgd_solver.cpp:106] Iteration 61400, lr = 0.0001
I0216 16:24:07.184820 25531 solver.cpp:237] Iteration 61600, loss = 0.11326
I0216 16:24:07.184887 25531 solver.cpp:253]     Train net output #0: loss = 0.11326 (* 1 = 0.11326 loss)
I0216 16:24:07.184905 25531 sgd_solver.cpp:106] Iteration 61600, lr = 0.0001
I0216 16:24:12.483222 25531 solver.cpp:237] Iteration 61800, loss = 0.139636
I0216 16:24:12.483300 25531 solver.cpp:253]     Train net output #0: loss = 0.139636 (* 1 = 0.139636 loss)
I0216 16:24:12.483319 25531 sgd_solver.cpp:106] Iteration 61800, lr = 0.0001
I0216 16:24:17.097092 25531 solver.cpp:341] Iteration 62000, Testing net (#0)
I0216 16:24:17.914589 25531 solver.cpp:409]     Test net output #0: accuracy = 0.7959
I0216 16:24:17.914649 25531 solver.cpp:409]     Test net output #1: loss = 0.625357 (* 1 = 0.625357 loss)
I0216 16:24:17.921140 25531 solver.cpp:237] Iteration 62000, loss = 0.108655
I0216 16:24:17.921198 25531 solver.cpp:253]     Train net output #0: loss = 0.108655 (* 1 = 0.108655 loss)
I0216 16:24:17.921210 25531 sgd_solver.cpp:106] Iteration 62000, lr = 0.0001
I0216 16:24:23.209787 25531 solver.cpp:237] Iteration 62200, loss = 0.153081
I0216 16:24:23.209851 25531 solver.cpp:253]     Train net output #0: loss = 0.153081 (* 1 = 0.153081 loss)
I0216 16:24:23.209867 25531 sgd_solver.cpp:106] Iteration 62200, lr = 0.0001
I0216 16:24:28.453454 25531 solver.cpp:237] Iteration 62400, loss = 0.11939
I0216 16:24:28.453536 25531 solver.cpp:253]     Train net output #0: loss = 0.11939 (* 1 = 0.11939 loss)
I0216 16:24:28.453554 25531 sgd_solver.cpp:106] Iteration 62400, lr = 0.0001
I0216 16:24:33.717561 25531 solver.cpp:237] Iteration 62600, loss = 0.110953
I0216 16:24:33.717599 25531 solver.cpp:253]     Train net output #0: loss = 0.110953 (* 1 = 0.110953 loss)
I0216 16:24:33.717607 25531 sgd_solver.cpp:106] Iteration 62600, lr = 0.0001
I0216 16:24:39.039518 25531 solver.cpp:237] Iteration 62800, loss = 0.136893
I0216 16:24:39.039577 25531 solver.cpp:253]     Train net output #0: loss = 0.136893 (* 1 = 0.136893 loss)
I0216 16:24:39.039594 25531 sgd_solver.cpp:106] Iteration 62800, lr = 0.0001
I0216 16:24:44.387861 25531 solver.cpp:341] Iteration 63000, Testing net (#0)
I0216 16:24:45.308097 25531 solver.cpp:409]     Test net output #0: accuracy = 0.7963
I0216 16:24:45.308152 25531 solver.cpp:409]     Test net output #1: loss = 0.626548 (* 1 = 0.626548 loss)
I0216 16:24:45.314374 25531 solver.cpp:237] Iteration 63000, loss = 0.104536
I0216 16:24:45.314415 25531 solver.cpp:253]     Train net output #0: loss = 0.104536 (* 1 = 0.104536 loss)
I0216 16:24:45.314422 25531 sgd_solver.cpp:106] Iteration 63000, lr = 0.0001
I0216 16:24:50.556399 25531 solver.cpp:237] Iteration 63200, loss = 0.148265
I0216 16:24:50.556459 25531 solver.cpp:253]     Train net output #0: loss = 0.148265 (* 1 = 0.148265 loss)
I0216 16:24:50.556476 25531 sgd_solver.cpp:106] Iteration 63200, lr = 0.0001
I0216 16:24:55.038431 25531 solver.cpp:237] Iteration 63400, loss = 0.113762
I0216 16:24:55.038477 25531 solver.cpp:253]     Train net output #0: loss = 0.113762 (* 1 = 0.113762 loss)
I0216 16:24:55.038486 25531 sgd_solver.cpp:106] Iteration 63400, lr = 0.0001
I0216 16:25:00.328742 25531 solver.cpp:237] Iteration 63600, loss = 0.108241
I0216 16:25:00.328881 25531 solver.cpp:253]     Train net output #0: loss = 0.108241 (* 1 = 0.108241 loss)
I0216 16:25:00.328899 25531 sgd_solver.cpp:106] Iteration 63600, lr = 0.0001
I0216 16:25:05.441377 25531 solver.cpp:237] Iteration 63800, loss = 0.133543
I0216 16:25:05.441440 25531 solver.cpp:253]     Train net output #0: loss = 0.133543 (* 1 = 0.133543 loss)
I0216 16:25:05.441671 25531 sgd_solver.cpp:106] Iteration 63800, lr = 0.0001
I0216 16:25:10.622936 25531 solver.cpp:341] Iteration 64000, Testing net (#0)
I0216 16:25:11.765481 25531 solver.cpp:409]     Test net output #0: accuracy = 0.7972
I0216 16:25:11.765527 25531 solver.cpp:409]     Test net output #1: loss = 0.627521 (* 1 = 0.627521 loss)
I0216 16:25:11.771831 25531 solver.cpp:237] Iteration 64000, loss = 0.10135
I0216 16:25:11.771874 25531 solver.cpp:253]     Train net output #0: loss = 0.10135 (* 1 = 0.10135 loss)
I0216 16:25:11.771885 25531 sgd_solver.cpp:106] Iteration 64000, lr = 0.0001
I0216 16:25:17.046376 25531 solver.cpp:237] Iteration 64200, loss = 0.144507
I0216 16:25:17.046423 25531 solver.cpp:253]     Train net output #0: loss = 0.144507 (* 1 = 0.144507 loss)
I0216 16:25:17.046736 25531 sgd_solver.cpp:106] Iteration 64200, lr = 0.0001
I0216 16:25:22.162014 25531 solver.cpp:237] Iteration 64400, loss = 0.109534
I0216 16:25:22.162080 25531 solver.cpp:253]     Train net output #0: loss = 0.109534 (* 1 = 0.109534 loss)
I0216 16:25:22.162317 25531 sgd_solver.cpp:106] Iteration 64400, lr = 0.0001
I0216 16:25:27.348135 25531 solver.cpp:237] Iteration 64600, loss = 0.10551
I0216 16:25:27.348192 25531 solver.cpp:253]     Train net output #0: loss = 0.10551 (* 1 = 0.10551 loss)
I0216 16:25:27.348208 25531 sgd_solver.cpp:106] Iteration 64600, lr = 0.0001
I0216 16:25:31.880892 25531 solver.cpp:237] Iteration 64800, loss = 0.130547
I0216 16:25:31.880998 25531 solver.cpp:253]     Train net output #0: loss = 0.130547 (* 1 = 0.130547 loss)
I0216 16:25:31.881011 25531 sgd_solver.cpp:106] Iteration 64800, lr = 0.0001
I0216 16:25:37.056044 25531 solver.cpp:472] Snapshotting to HDF5 file examples/cifar10/cifar10_full_iter_65000.caffemodel.h5
I0216 16:25:37.712494 25531 sgd_solver.cpp:279] Snapshotting solver state to HDF5 file examples/cifar10/cifar10_full_iter_65000.solverstate.h5
I0216 16:25:37.727452 25531 solver.cpp:321] Iteration 65000, loss = 0.0987541
I0216 16:25:37.727501 25531 solver.cpp:341] Iteration 65000, Testing net (#0)
I0216 16:25:38.881768 25531 solver.cpp:409]     Test net output #0: accuracy = 0.7974
I0216 16:25:38.881821 25531 solver.cpp:409]     Test net output #1: loss = 0.628257 (* 1 = 0.628257 loss)
I0216 16:25:38.881835 25531 solver.cpp:326] Optimization Done.
I0216 16:25:38.881842 25531 caffe.cpp:215] Optimization Done.
I0216 16:25:39.558850 25606 caffe.cpp:184] Using GPUs 0
I0216 16:25:39.901021 25606 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 1000
base_lr: 1e-05
display: 200
max_iter: 70000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.004
snapshot: 5000
snapshot_prefix: "examples/cifar10/cifar10_full"
solver_mode: GPU
device_id: 0
net: "examples/cifar10/cifar10_full_train_test.prototxt"
snapshot_format: HDF5
I0216 16:25:39.901175 25606 solver.cpp:91] Creating training net from net file: examples/cifar10/cifar10_full_train_test.prototxt
I0216 16:25:39.901835 25606 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0216 16:25:39.901870 25606 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0216 16:25:39.902048 25606 net.cpp:49] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TRAIN
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0216 16:25:39.902179 25606 layer_factory.hpp:77] Creating layer cifar
I0216 16:25:39.902859 25606 net.cpp:106] Creating Layer cifar
I0216 16:25:39.902911 25606 net.cpp:411] cifar -> data
I0216 16:25:39.902978 25606 net.cpp:411] cifar -> label
I0216 16:25:39.903028 25606 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0216 16:25:39.903882 25611 db_lmdb.cpp:38] Opened lmdb examples/cifar10/cifar10_train_lmdb
I0216 16:25:39.935693 25606 data_layer.cpp:41] output data size: 100,3,32,32
I0216 16:25:39.946092 25606 net.cpp:150] Setting up cifar
I0216 16:25:39.946144 25606 net.cpp:157] Top shape: 100 3 32 32 (307200)
I0216 16:25:39.946157 25606 net.cpp:157] Top shape: 100 (100)
I0216 16:25:39.946166 25606 net.cpp:165] Memory required for data: 1229200
I0216 16:25:39.946182 25606 layer_factory.hpp:77] Creating layer conv1
I0216 16:25:39.946213 25606 net.cpp:106] Creating Layer conv1
I0216 16:25:39.946225 25606 net.cpp:454] conv1 <- data
I0216 16:25:39.946244 25606 net.cpp:411] conv1 -> conv1
I0216 16:25:40.266083 25606 net.cpp:150] Setting up conv1
I0216 16:25:40.266124 25606 net.cpp:157] Top shape: 100 32 32 32 (3276800)
I0216 16:25:40.266130 25606 net.cpp:165] Memory required for data: 14336400
I0216 16:25:40.266149 25606 layer_factory.hpp:77] Creating layer pool1
I0216 16:25:40.266161 25606 net.cpp:106] Creating Layer pool1
I0216 16:25:40.266166 25606 net.cpp:454] pool1 <- conv1
I0216 16:25:40.266173 25606 net.cpp:411] pool1 -> pool1
I0216 16:25:40.267069 25606 net.cpp:150] Setting up pool1
I0216 16:25:40.267088 25606 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 16:25:40.267093 25606 net.cpp:165] Memory required for data: 17613200
I0216 16:25:40.267097 25606 layer_factory.hpp:77] Creating layer relu1
I0216 16:25:40.267105 25606 net.cpp:106] Creating Layer relu1
I0216 16:25:40.267109 25606 net.cpp:454] relu1 <- pool1
I0216 16:25:40.267117 25606 net.cpp:397] relu1 -> pool1 (in-place)
I0216 16:25:40.268458 25606 net.cpp:150] Setting up relu1
I0216 16:25:40.268479 25606 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 16:25:40.268483 25606 net.cpp:165] Memory required for data: 20890000
I0216 16:25:40.268488 25606 layer_factory.hpp:77] Creating layer norm1
I0216 16:25:40.268506 25606 net.cpp:106] Creating Layer norm1
I0216 16:25:40.268510 25606 net.cpp:454] norm1 <- pool1
I0216 16:25:40.268517 25606 net.cpp:411] norm1 -> norm1
I0216 16:25:40.270138 25606 net.cpp:150] Setting up norm1
I0216 16:25:40.270159 25606 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 16:25:40.270174 25606 net.cpp:165] Memory required for data: 24166800
I0216 16:25:40.270180 25606 layer_factory.hpp:77] Creating layer conv2
I0216 16:25:40.270192 25606 net.cpp:106] Creating Layer conv2
I0216 16:25:40.270197 25606 net.cpp:454] conv2 <- norm1
I0216 16:25:40.270205 25606 net.cpp:411] conv2 -> conv2
I0216 16:25:40.274781 25606 net.cpp:150] Setting up conv2
I0216 16:25:40.274837 25606 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 16:25:40.274847 25606 net.cpp:165] Memory required for data: 27443600
I0216 16:25:40.274869 25606 layer_factory.hpp:77] Creating layer relu2
I0216 16:25:40.274885 25606 net.cpp:106] Creating Layer relu2
I0216 16:25:40.274893 25606 net.cpp:454] relu2 <- conv2
I0216 16:25:40.274904 25606 net.cpp:397] relu2 -> conv2 (in-place)
I0216 16:25:40.275928 25606 net.cpp:150] Setting up relu2
I0216 16:25:40.275975 25606 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 16:25:40.275987 25606 net.cpp:165] Memory required for data: 30720400
I0216 16:25:40.275998 25606 layer_factory.hpp:77] Creating layer pool2
I0216 16:25:40.276015 25606 net.cpp:106] Creating Layer pool2
I0216 16:25:40.276026 25606 net.cpp:454] pool2 <- conv2
I0216 16:25:40.276042 25606 net.cpp:411] pool2 -> pool2
I0216 16:25:40.277083 25606 net.cpp:150] Setting up pool2
I0216 16:25:40.277133 25606 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0216 16:25:40.277145 25606 net.cpp:165] Memory required for data: 31539600
I0216 16:25:40.277156 25606 layer_factory.hpp:77] Creating layer norm2
I0216 16:25:40.277182 25606 net.cpp:106] Creating Layer norm2
I0216 16:25:40.277194 25606 net.cpp:454] norm2 <- pool2
I0216 16:25:40.277209 25606 net.cpp:411] norm2 -> norm2
I0216 16:25:40.278661 25606 net.cpp:150] Setting up norm2
I0216 16:25:40.278718 25606 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0216 16:25:40.278730 25606 net.cpp:165] Memory required for data: 32358800
I0216 16:25:40.278740 25606 layer_factory.hpp:77] Creating layer conv3
I0216 16:25:40.278761 25606 net.cpp:106] Creating Layer conv3
I0216 16:25:40.278774 25606 net.cpp:454] conv3 <- norm2
I0216 16:25:40.278789 25606 net.cpp:411] conv3 -> conv3
I0216 16:25:40.284128 25606 net.cpp:150] Setting up conv3
I0216 16:25:40.284183 25606 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0216 16:25:40.284196 25606 net.cpp:165] Memory required for data: 33997200
I0216 16:25:40.284220 25606 layer_factory.hpp:77] Creating layer relu3
I0216 16:25:40.284240 25606 net.cpp:106] Creating Layer relu3
I0216 16:25:40.284250 25606 net.cpp:454] relu3 <- conv3
I0216 16:25:40.284260 25606 net.cpp:397] relu3 -> conv3 (in-place)
I0216 16:25:40.285230 25606 net.cpp:150] Setting up relu3
I0216 16:25:40.285253 25606 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0216 16:25:40.285261 25606 net.cpp:165] Memory required for data: 35635600
I0216 16:25:40.285269 25606 layer_factory.hpp:77] Creating layer pool3
I0216 16:25:40.285280 25606 net.cpp:106] Creating Layer pool3
I0216 16:25:40.285287 25606 net.cpp:454] pool3 <- conv3
I0216 16:25:40.285300 25606 net.cpp:411] pool3 -> pool3
I0216 16:25:40.286620 25606 net.cpp:150] Setting up pool3
I0216 16:25:40.286653 25606 net.cpp:157] Top shape: 100 64 4 4 (102400)
I0216 16:25:40.286667 25606 net.cpp:165] Memory required for data: 36045200
I0216 16:25:40.286677 25606 layer_factory.hpp:77] Creating layer ip1
I0216 16:25:40.286692 25606 net.cpp:106] Creating Layer ip1
I0216 16:25:40.286701 25606 net.cpp:454] ip1 <- pool3
I0216 16:25:40.286713 25606 net.cpp:411] ip1 -> ip1
I0216 16:25:40.304601 25606 net.cpp:150] Setting up ip1
I0216 16:25:40.304651 25606 net.cpp:157] Top shape: 100 500 (50000)
I0216 16:25:40.304662 25606 net.cpp:165] Memory required for data: 36245200
I0216 16:25:40.304678 25606 layer_factory.hpp:77] Creating layer relu4
I0216 16:25:40.304694 25606 net.cpp:106] Creating Layer relu4
I0216 16:25:40.304704 25606 net.cpp:454] relu4 <- ip1
I0216 16:25:40.304719 25606 net.cpp:397] relu4 -> ip1 (in-place)
I0216 16:25:40.305809 25606 net.cpp:150] Setting up relu4
I0216 16:25:40.305830 25606 net.cpp:157] Top shape: 100 500 (50000)
I0216 16:25:40.305852 25606 net.cpp:165] Memory required for data: 36445200
I0216 16:25:40.305863 25606 layer_factory.hpp:77] Creating layer ip2
I0216 16:25:40.305883 25606 net.cpp:106] Creating Layer ip2
I0216 16:25:40.305892 25606 net.cpp:454] ip2 <- ip1
I0216 16:25:40.305904 25606 net.cpp:411] ip2 -> ip2
I0216 16:25:40.306774 25606 net.cpp:150] Setting up ip2
I0216 16:25:40.306797 25606 net.cpp:157] Top shape: 100 10 (1000)
I0216 16:25:40.306805 25606 net.cpp:165] Memory required for data: 36449200
I0216 16:25:40.306823 25606 layer_factory.hpp:77] Creating layer loss
I0216 16:25:40.306840 25606 net.cpp:106] Creating Layer loss
I0216 16:25:40.306850 25606 net.cpp:454] loss <- ip2
I0216 16:25:40.306859 25606 net.cpp:454] loss <- label
I0216 16:25:40.306872 25606 net.cpp:411] loss -> loss
I0216 16:25:40.306890 25606 layer_factory.hpp:77] Creating layer loss
I0216 16:25:40.307970 25606 net.cpp:150] Setting up loss
I0216 16:25:40.307989 25606 net.cpp:157] Top shape: (1)
I0216 16:25:40.307999 25606 net.cpp:160]     with loss weight 1
I0216 16:25:40.308022 25606 net.cpp:165] Memory required for data: 36449204
I0216 16:25:40.308030 25606 net.cpp:226] loss needs backward computation.
I0216 16:25:40.308039 25606 net.cpp:226] ip2 needs backward computation.
I0216 16:25:40.308046 25606 net.cpp:226] relu4 needs backward computation.
I0216 16:25:40.308053 25606 net.cpp:226] ip1 needs backward computation.
I0216 16:25:40.308060 25606 net.cpp:226] pool3 needs backward computation.
I0216 16:25:40.308068 25606 net.cpp:226] relu3 needs backward computation.
I0216 16:25:40.308074 25606 net.cpp:226] conv3 needs backward computation.
I0216 16:25:40.308080 25606 net.cpp:226] norm2 needs backward computation.
I0216 16:25:40.308105 25606 net.cpp:226] pool2 needs backward computation.
I0216 16:25:40.308117 25606 net.cpp:226] relu2 needs backward computation.
I0216 16:25:40.308125 25606 net.cpp:226] conv2 needs backward computation.
I0216 16:25:40.308132 25606 net.cpp:226] norm1 needs backward computation.
I0216 16:25:40.308138 25606 net.cpp:226] relu1 needs backward computation.
I0216 16:25:40.308145 25606 net.cpp:226] pool1 needs backward computation.
I0216 16:25:40.308152 25606 net.cpp:226] conv1 needs backward computation.
I0216 16:25:40.308161 25606 net.cpp:228] cifar does not need backward computation.
I0216 16:25:40.308166 25606 net.cpp:270] This network produces output loss
I0216 16:25:40.308184 25606 net.cpp:283] Network initialization done.
I0216 16:25:40.308874 25606 solver.cpp:181] Creating test net (#0) specified by net file: examples/cifar10/cifar10_full_train_test.prototxt
I0216 16:25:40.308928 25606 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0216 16:25:40.309128 25606 net.cpp:49] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0216 16:25:40.309295 25606 layer_factory.hpp:77] Creating layer cifar
I0216 16:25:40.309447 25606 net.cpp:106] Creating Layer cifar
I0216 16:25:40.309468 25606 net.cpp:411] cifar -> data
I0216 16:25:40.309489 25606 net.cpp:411] cifar -> label
I0216 16:25:40.309506 25606 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0216 16:25:40.310407 25613 db_lmdb.cpp:38] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0216 16:25:40.310583 25606 data_layer.cpp:41] output data size: 100,3,32,32
I0216 16:25:40.314259 25606 net.cpp:150] Setting up cifar
I0216 16:25:40.314323 25606 net.cpp:157] Top shape: 100 3 32 32 (307200)
I0216 16:25:40.314340 25606 net.cpp:157] Top shape: 100 (100)
I0216 16:25:40.314349 25606 net.cpp:165] Memory required for data: 1229200
I0216 16:25:40.314359 25606 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0216 16:25:40.314376 25606 net.cpp:106] Creating Layer label_cifar_1_split
I0216 16:25:40.314386 25606 net.cpp:454] label_cifar_1_split <- label
I0216 16:25:40.314399 25606 net.cpp:411] label_cifar_1_split -> label_cifar_1_split_0
I0216 16:25:40.314414 25606 net.cpp:411] label_cifar_1_split -> label_cifar_1_split_1
I0216 16:25:40.314527 25606 net.cpp:150] Setting up label_cifar_1_split
I0216 16:25:40.314543 25606 net.cpp:157] Top shape: 100 (100)
I0216 16:25:40.314553 25606 net.cpp:157] Top shape: 100 (100)
I0216 16:25:40.314558 25606 net.cpp:165] Memory required for data: 1230000
I0216 16:25:40.314565 25606 layer_factory.hpp:77] Creating layer conv1
I0216 16:25:40.314597 25606 net.cpp:106] Creating Layer conv1
I0216 16:25:40.314611 25606 net.cpp:454] conv1 <- data
I0216 16:25:40.314638 25606 net.cpp:411] conv1 -> conv1
I0216 16:25:40.326876 25606 net.cpp:150] Setting up conv1
I0216 16:25:40.326927 25606 net.cpp:157] Top shape: 100 32 32 32 (3276800)
I0216 16:25:40.326938 25606 net.cpp:165] Memory required for data: 14337200
I0216 16:25:40.326961 25606 layer_factory.hpp:77] Creating layer pool1
I0216 16:25:40.326982 25606 net.cpp:106] Creating Layer pool1
I0216 16:25:40.326994 25606 net.cpp:454] pool1 <- conv1
I0216 16:25:40.327008 25606 net.cpp:411] pool1 -> pool1
I0216 16:25:40.329377 25606 net.cpp:150] Setting up pool1
I0216 16:25:40.329406 25606 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 16:25:40.329414 25606 net.cpp:165] Memory required for data: 17614000
I0216 16:25:40.329422 25606 layer_factory.hpp:77] Creating layer relu1
I0216 16:25:40.329435 25606 net.cpp:106] Creating Layer relu1
I0216 16:25:40.329442 25606 net.cpp:454] relu1 <- pool1
I0216 16:25:40.329453 25606 net.cpp:397] relu1 -> pool1 (in-place)
I0216 16:25:40.330881 25606 net.cpp:150] Setting up relu1
I0216 16:25:40.330914 25606 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 16:25:40.330921 25606 net.cpp:165] Memory required for data: 20890800
I0216 16:25:40.330930 25606 layer_factory.hpp:77] Creating layer norm1
I0216 16:25:40.330946 25606 net.cpp:106] Creating Layer norm1
I0216 16:25:40.330953 25606 net.cpp:454] norm1 <- pool1
I0216 16:25:40.330965 25606 net.cpp:411] norm1 -> norm1
I0216 16:25:40.332761 25606 net.cpp:150] Setting up norm1
I0216 16:25:40.332793 25606 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 16:25:40.332804 25606 net.cpp:165] Memory required for data: 24167600
I0216 16:25:40.332813 25606 layer_factory.hpp:77] Creating layer conv2
I0216 16:25:40.332831 25606 net.cpp:106] Creating Layer conv2
I0216 16:25:40.332840 25606 net.cpp:454] conv2 <- norm1
I0216 16:25:40.332855 25606 net.cpp:411] conv2 -> conv2
I0216 16:25:40.337813 25606 net.cpp:150] Setting up conv2
I0216 16:25:40.337852 25606 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 16:25:40.337862 25606 net.cpp:165] Memory required for data: 27444400
I0216 16:25:40.337880 25606 layer_factory.hpp:77] Creating layer relu2
I0216 16:25:40.337898 25606 net.cpp:106] Creating Layer relu2
I0216 16:25:40.337924 25606 net.cpp:454] relu2 <- conv2
I0216 16:25:40.337935 25606 net.cpp:397] relu2 -> conv2 (in-place)
I0216 16:25:40.338793 25606 net.cpp:150] Setting up relu2
I0216 16:25:40.338814 25606 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 16:25:40.338819 25606 net.cpp:165] Memory required for data: 30721200
I0216 16:25:40.338827 25606 layer_factory.hpp:77] Creating layer pool2
I0216 16:25:40.338846 25606 net.cpp:106] Creating Layer pool2
I0216 16:25:40.338861 25606 net.cpp:454] pool2 <- conv2
I0216 16:25:40.338874 25606 net.cpp:411] pool2 -> pool2
I0216 16:25:40.339730 25606 net.cpp:150] Setting up pool2
I0216 16:25:40.339748 25606 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0216 16:25:40.339756 25606 net.cpp:165] Memory required for data: 31540400
I0216 16:25:40.339761 25606 layer_factory.hpp:77] Creating layer norm2
I0216 16:25:40.339778 25606 net.cpp:106] Creating Layer norm2
I0216 16:25:40.339786 25606 net.cpp:454] norm2 <- pool2
I0216 16:25:40.339793 25606 net.cpp:411] norm2 -> norm2
I0216 16:25:40.341087 25606 net.cpp:150] Setting up norm2
I0216 16:25:40.341106 25606 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0216 16:25:40.341114 25606 net.cpp:165] Memory required for data: 32359600
I0216 16:25:40.341120 25606 layer_factory.hpp:77] Creating layer conv3
I0216 16:25:40.341135 25606 net.cpp:106] Creating Layer conv3
I0216 16:25:40.341142 25606 net.cpp:454] conv3 <- norm2
I0216 16:25:40.341155 25606 net.cpp:411] conv3 -> conv3
I0216 16:25:40.349073 25606 net.cpp:150] Setting up conv3
I0216 16:25:40.349117 25606 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0216 16:25:40.349123 25606 net.cpp:165] Memory required for data: 33998000
I0216 16:25:40.349144 25606 layer_factory.hpp:77] Creating layer relu3
I0216 16:25:40.349159 25606 net.cpp:106] Creating Layer relu3
I0216 16:25:40.349166 25606 net.cpp:454] relu3 <- conv3
I0216 16:25:40.349176 25606 net.cpp:397] relu3 -> conv3 (in-place)
I0216 16:25:40.350417 25606 net.cpp:150] Setting up relu3
I0216 16:25:40.350450 25606 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0216 16:25:40.350458 25606 net.cpp:165] Memory required for data: 35636400
I0216 16:25:40.350466 25606 layer_factory.hpp:77] Creating layer pool3
I0216 16:25:40.350481 25606 net.cpp:106] Creating Layer pool3
I0216 16:25:40.350489 25606 net.cpp:454] pool3 <- conv3
I0216 16:25:40.350518 25606 net.cpp:411] pool3 -> pool3
I0216 16:25:40.351837 25606 net.cpp:150] Setting up pool3
I0216 16:25:40.351867 25606 net.cpp:157] Top shape: 100 64 4 4 (102400)
I0216 16:25:40.351874 25606 net.cpp:165] Memory required for data: 36046000
I0216 16:25:40.351882 25606 layer_factory.hpp:77] Creating layer ip1
I0216 16:25:40.351897 25606 net.cpp:106] Creating Layer ip1
I0216 16:25:40.351903 25606 net.cpp:454] ip1 <- pool3
I0216 16:25:40.351917 25606 net.cpp:411] ip1 -> ip1
I0216 16:25:40.370398 25606 net.cpp:150] Setting up ip1
I0216 16:25:40.370442 25606 net.cpp:157] Top shape: 100 500 (50000)
I0216 16:25:40.370450 25606 net.cpp:165] Memory required for data: 36246000
I0216 16:25:40.370463 25606 layer_factory.hpp:77] Creating layer relu4
I0216 16:25:40.370477 25606 net.cpp:106] Creating Layer relu4
I0216 16:25:40.370486 25606 net.cpp:454] relu4 <- ip1
I0216 16:25:40.370498 25606 net.cpp:397] relu4 -> ip1 (in-place)
I0216 16:25:40.371522 25606 net.cpp:150] Setting up relu4
I0216 16:25:40.371549 25606 net.cpp:157] Top shape: 100 500 (50000)
I0216 16:25:40.371557 25606 net.cpp:165] Memory required for data: 36446000
I0216 16:25:40.371564 25606 layer_factory.hpp:77] Creating layer ip2
I0216 16:25:40.371579 25606 net.cpp:106] Creating Layer ip2
I0216 16:25:40.371587 25606 net.cpp:454] ip2 <- ip1
I0216 16:25:40.371598 25606 net.cpp:411] ip2 -> ip2
I0216 16:25:40.371948 25606 net.cpp:150] Setting up ip2
I0216 16:25:40.371961 25606 net.cpp:157] Top shape: 100 10 (1000)
I0216 16:25:40.371968 25606 net.cpp:165] Memory required for data: 36450000
I0216 16:25:40.371984 25606 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0216 16:25:40.372000 25606 net.cpp:106] Creating Layer ip2_ip2_0_split
I0216 16:25:40.372006 25606 net.cpp:454] ip2_ip2_0_split <- ip2
I0216 16:25:40.372035 25606 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0216 16:25:40.372046 25606 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0216 16:25:40.372097 25606 net.cpp:150] Setting up ip2_ip2_0_split
I0216 16:25:40.372107 25606 net.cpp:157] Top shape: 100 10 (1000)
I0216 16:25:40.372115 25606 net.cpp:157] Top shape: 100 10 (1000)
I0216 16:25:40.372122 25606 net.cpp:165] Memory required for data: 36458000
I0216 16:25:40.372128 25606 layer_factory.hpp:77] Creating layer accuracy
I0216 16:25:40.372138 25606 net.cpp:106] Creating Layer accuracy
I0216 16:25:40.372145 25606 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I0216 16:25:40.372153 25606 net.cpp:454] accuracy <- label_cifar_1_split_0
I0216 16:25:40.372161 25606 net.cpp:411] accuracy -> accuracy
I0216 16:25:40.372174 25606 net.cpp:150] Setting up accuracy
I0216 16:25:40.372184 25606 net.cpp:157] Top shape: (1)
I0216 16:25:40.372190 25606 net.cpp:165] Memory required for data: 36458004
I0216 16:25:40.372195 25606 layer_factory.hpp:77] Creating layer loss
I0216 16:25:40.372206 25606 net.cpp:106] Creating Layer loss
I0216 16:25:40.372213 25606 net.cpp:454] loss <- ip2_ip2_0_split_1
I0216 16:25:40.372220 25606 net.cpp:454] loss <- label_cifar_1_split_1
I0216 16:25:40.372228 25606 net.cpp:411] loss -> loss
I0216 16:25:40.372241 25606 layer_factory.hpp:77] Creating layer loss
I0216 16:25:40.373358 25606 net.cpp:150] Setting up loss
I0216 16:25:40.373378 25606 net.cpp:157] Top shape: (1)
I0216 16:25:40.373384 25606 net.cpp:160]     with loss weight 1
I0216 16:25:40.373395 25606 net.cpp:165] Memory required for data: 36458008
I0216 16:25:40.373402 25606 net.cpp:226] loss needs backward computation.
I0216 16:25:40.373409 25606 net.cpp:228] accuracy does not need backward computation.
I0216 16:25:40.373415 25606 net.cpp:226] ip2_ip2_0_split needs backward computation.
I0216 16:25:40.373421 25606 net.cpp:226] ip2 needs backward computation.
I0216 16:25:40.373427 25606 net.cpp:226] relu4 needs backward computation.
I0216 16:25:40.373433 25606 net.cpp:226] ip1 needs backward computation.
I0216 16:25:40.373440 25606 net.cpp:226] pool3 needs backward computation.
I0216 16:25:40.373445 25606 net.cpp:226] relu3 needs backward computation.
I0216 16:25:40.373451 25606 net.cpp:226] conv3 needs backward computation.
I0216 16:25:40.373467 25606 net.cpp:226] norm2 needs backward computation.
I0216 16:25:40.373474 25606 net.cpp:226] pool2 needs backward computation.
I0216 16:25:40.373481 25606 net.cpp:226] relu2 needs backward computation.
I0216 16:25:40.373486 25606 net.cpp:226] conv2 needs backward computation.
I0216 16:25:40.373492 25606 net.cpp:226] norm1 needs backward computation.
I0216 16:25:40.373497 25606 net.cpp:226] relu1 needs backward computation.
I0216 16:25:40.373503 25606 net.cpp:226] pool1 needs backward computation.
I0216 16:25:40.373509 25606 net.cpp:226] conv1 needs backward computation.
I0216 16:25:40.373517 25606 net.cpp:228] label_cifar_1_split does not need backward computation.
I0216 16:25:40.373522 25606 net.cpp:228] cifar does not need backward computation.
I0216 16:25:40.373528 25606 net.cpp:270] This network produces output accuracy
I0216 16:25:40.373534 25606 net.cpp:270] This network produces output loss
I0216 16:25:40.373554 25606 net.cpp:283] Network initialization done.
I0216 16:25:40.373667 25606 solver.cpp:60] Solver scaffolding done.
I0216 16:25:40.374140 25606 caffe.cpp:202] Resuming from examples/cifar10/cifar10_full_iter_65000.solverstate.h5
I0216 16:25:40.375541 25606 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0216 16:25:40.379634 25606 caffe.cpp:212] Starting Optimization
I0216 16:25:40.379669 25606 solver.cpp:288] Solving CIFAR10_full
I0216 16:25:40.379676 25606 solver.cpp:289] Learning Rate Policy: fixed
I0216 16:25:40.380467 25606 solver.cpp:341] Iteration 65000, Testing net (#0)
I0216 16:25:41.657789 25606 solver.cpp:409]     Test net output #0: accuracy = 0.7974
I0216 16:25:41.657831 25606 solver.cpp:409]     Test net output #1: loss = 0.628257 (* 1 = 0.628257 loss)
I0216 16:25:41.668102 25606 solver.cpp:237] Iteration 65000, loss = 0.0987541
I0216 16:25:41.668164 25606 solver.cpp:253]     Train net output #0: loss = 0.0987541 (* 1 = 0.0987541 loss)
I0216 16:25:41.668171 25606 sgd_solver.cpp:106] Iteration 65000, lr = 1e-05
I0216 16:25:47.019780 25606 solver.cpp:237] Iteration 65200, loss = 0.130071
I0216 16:25:47.019842 25606 solver.cpp:253]     Train net output #0: loss = 0.130071 (* 1 = 0.130071 loss)
I0216 16:25:47.019855 25606 sgd_solver.cpp:106] Iteration 65200, lr = 1e-05
I0216 16:25:52.234092 25606 solver.cpp:237] Iteration 65400, loss = 0.0887235
I0216 16:25:52.234143 25606 solver.cpp:253]     Train net output #0: loss = 0.0887235 (* 1 = 0.0887235 loss)
I0216 16:25:52.234452 25606 sgd_solver.cpp:106] Iteration 65400, lr = 1e-05
I0216 16:25:57.530004 25606 solver.cpp:237] Iteration 65600, loss = 0.104354
I0216 16:25:57.530071 25606 solver.cpp:253]     Train net output #0: loss = 0.104354 (* 1 = 0.104354 loss)
I0216 16:25:57.530089 25606 sgd_solver.cpp:106] Iteration 65600, lr = 1e-05
I0216 16:26:02.719303 25606 solver.cpp:237] Iteration 65800, loss = 0.120927
I0216 16:26:02.719349 25606 solver.cpp:253]     Train net output #0: loss = 0.120927 (* 1 = 0.120927 loss)
I0216 16:26:02.719588 25606 sgd_solver.cpp:106] Iteration 65800, lr = 1e-05
I0216 16:26:07.451661 25606 solver.cpp:341] Iteration 66000, Testing net (#0)
I0216 16:26:08.703609 25606 solver.cpp:409]     Test net output #0: accuracy = 0.7983
I0216 16:26:08.703665 25606 solver.cpp:409]     Test net output #1: loss = 0.619809 (* 1 = 0.619809 loss)
I0216 16:26:08.720798 25606 solver.cpp:237] Iteration 66000, loss = 0.0982148
I0216 16:26:08.720854 25606 solver.cpp:253]     Train net output #0: loss = 0.0982148 (* 1 = 0.0982148 loss)
I0216 16:26:08.720865 25606 sgd_solver.cpp:106] Iteration 66000, lr = 1e-05
I0216 16:26:14.014312 25606 solver.cpp:237] Iteration 66200, loss = 0.126035
I0216 16:26:14.014698 25606 solver.cpp:253]     Train net output #0: loss = 0.126035 (* 1 = 0.126035 loss)
I0216 16:26:14.029475 25606 sgd_solver.cpp:106] Iteration 66200, lr = 1e-05
I0216 16:26:19.361958 25606 solver.cpp:237] Iteration 66400, loss = 0.0944709
I0216 16:26:19.362017 25606 solver.cpp:253]     Train net output #0: loss = 0.0944709 (* 1 = 0.0944709 loss)
I0216 16:26:19.362033 25606 sgd_solver.cpp:106] Iteration 66400, lr = 1e-05
I0216 16:26:24.692605 25606 solver.cpp:237] Iteration 66600, loss = 0.102304
I0216 16:26:24.692672 25606 solver.cpp:253]     Train net output #0: loss = 0.102304 (* 1 = 0.102304 loss)
I0216 16:26:24.692690 25606 sgd_solver.cpp:106] Iteration 66600, lr = 1e-05
I0216 16:26:30.018403 25606 solver.cpp:237] Iteration 66800, loss = 0.120921
I0216 16:26:30.018466 25606 solver.cpp:253]     Train net output #0: loss = 0.120921 (* 1 = 0.120921 loss)
I0216 16:26:30.018486 25606 sgd_solver.cpp:106] Iteration 66800, lr = 1e-05
I0216 16:26:35.208415 25606 solver.cpp:341] Iteration 67000, Testing net (#0)
I0216 16:26:36.447319 25606 solver.cpp:409]     Test net output #0: accuracy = 0.799
I0216 16:26:36.447360 25606 solver.cpp:409]     Test net output #1: loss = 0.618988 (* 1 = 0.618988 loss)
I0216 16:26:36.454123 25606 solver.cpp:237] Iteration 67000, loss = 0.0981657
I0216 16:26:36.454159 25606 solver.cpp:253]     Train net output #0: loss = 0.0981657 (* 1 = 0.0981657 loss)
I0216 16:26:36.454166 25606 sgd_solver.cpp:106] Iteration 67000, lr = 1e-05
I0216 16:26:40.942594 25606 solver.cpp:237] Iteration 67200, loss = 0.12474
I0216 16:26:40.942667 25606 solver.cpp:253]     Train net output #0: loss = 0.12474 (* 1 = 0.12474 loss)
I0216 16:26:40.942683 25606 sgd_solver.cpp:106] Iteration 67200, lr = 1e-05
I0216 16:26:46.108068 25606 solver.cpp:237] Iteration 67400, loss = 0.0977238
I0216 16:26:46.108152 25606 solver.cpp:253]     Train net output #0: loss = 0.0977238 (* 1 = 0.0977238 loss)
I0216 16:26:46.108161 25606 sgd_solver.cpp:106] Iteration 67400, lr = 1e-05
I0216 16:26:51.411512 25606 solver.cpp:237] Iteration 67600, loss = 0.101196
I0216 16:26:51.411556 25606 solver.cpp:253]     Train net output #0: loss = 0.101196 (* 1 = 0.101196 loss)
I0216 16:26:51.411562 25606 sgd_solver.cpp:106] Iteration 67600, lr = 1e-05
I0216 16:26:56.752813 25606 solver.cpp:237] Iteration 67800, loss = 0.121798
I0216 16:26:56.752876 25606 solver.cpp:253]     Train net output #0: loss = 0.121798 (* 1 = 0.121798 loss)
I0216 16:26:56.752894 25606 sgd_solver.cpp:106] Iteration 67800, lr = 1e-05
I0216 16:27:02.080869 25606 solver.cpp:341] Iteration 68000, Testing net (#0)
I0216 16:27:03.222443 25606 solver.cpp:409]     Test net output #0: accuracy = 0.7988
I0216 16:27:03.222501 25606 solver.cpp:409]     Test net output #1: loss = 0.618622 (* 1 = 0.618622 loss)
I0216 16:27:03.228457 25606 solver.cpp:237] Iteration 68000, loss = 0.0979559
I0216 16:27:03.228502 25606 solver.cpp:253]     Train net output #0: loss = 0.0979559 (* 1 = 0.0979559 loss)
I0216 16:27:03.228513 25606 sgd_solver.cpp:106] Iteration 68000, lr = 1e-05
I0216 16:27:08.397378 25606 solver.cpp:237] Iteration 68200, loss = 0.124265
I0216 16:27:08.397441 25606 solver.cpp:253]     Train net output #0: loss = 0.124265 (* 1 = 0.124265 loss)
I0216 16:27:08.397459 25606 sgd_solver.cpp:106] Iteration 68200, lr = 1e-05
I0216 16:27:13.560643 25606 solver.cpp:237] Iteration 68400, loss = 0.0996154
I0216 16:27:13.560708 25606 solver.cpp:253]     Train net output #0: loss = 0.0996154 (* 1 = 0.0996154 loss)
I0216 16:27:13.561096 25606 sgd_solver.cpp:106] Iteration 68400, lr = 1e-05
I0216 16:27:18.231406 25606 solver.cpp:237] Iteration 68600, loss = 0.100657
I0216 16:27:18.231539 25606 solver.cpp:253]     Train net output #0: loss = 0.100657 (* 1 = 0.100657 loss)
I0216 16:27:18.231559 25606 sgd_solver.cpp:106] Iteration 68600, lr = 1e-05
I0216 16:27:23.550040 25606 solver.cpp:237] Iteration 68800, loss = 0.122408
I0216 16:27:23.550096 25606 solver.cpp:253]     Train net output #0: loss = 0.122408 (* 1 = 0.122408 loss)
I0216 16:27:23.550108 25606 sgd_solver.cpp:106] Iteration 68800, lr = 1e-05
I0216 16:27:28.670744 25606 solver.cpp:341] Iteration 69000, Testing net (#0)
I0216 16:27:29.771960 25606 solver.cpp:409]     Test net output #0: accuracy = 0.7985
I0216 16:27:29.772022 25606 solver.cpp:409]     Test net output #1: loss = 0.618375 (* 1 = 0.618375 loss)
I0216 16:27:29.784253 25606 solver.cpp:237] Iteration 69000, loss = 0.097702
I0216 16:27:29.784312 25606 solver.cpp:253]     Train net output #0: loss = 0.097702 (* 1 = 0.097702 loss)
I0216 16:27:29.784343 25606 sgd_solver.cpp:106] Iteration 69000, lr = 1e-05
I0216 16:27:34.943743 25606 solver.cpp:237] Iteration 69200, loss = 0.124096
I0216 16:27:34.943814 25606 solver.cpp:253]     Train net output #0: loss = 0.124096 (* 1 = 0.124096 loss)
I0216 16:27:34.943830 25606 sgd_solver.cpp:106] Iteration 69200, lr = 1e-05
I0216 16:27:40.249382 25606 solver.cpp:237] Iteration 69400, loss = 0.100676
I0216 16:27:40.249426 25606 solver.cpp:253]     Train net output #0: loss = 0.100676 (* 1 = 0.100676 loss)
I0216 16:27:40.249435 25606 sgd_solver.cpp:106] Iteration 69400, lr = 1e-05
I0216 16:27:45.568738 25606 solver.cpp:237] Iteration 69600, loss = 0.10029
I0216 16:27:45.568781 25606 solver.cpp:253]     Train net output #0: loss = 0.10029 (* 1 = 0.10029 loss)
I0216 16:27:45.568789 25606 sgd_solver.cpp:106] Iteration 69600, lr = 1e-05
I0216 16:27:50.638485 25606 solver.cpp:237] Iteration 69800, loss = 0.122717
I0216 16:27:50.638592 25606 solver.cpp:253]     Train net output #0: loss = 0.122717 (* 1 = 0.122717 loss)
I0216 16:27:50.638605 25606 sgd_solver.cpp:106] Iteration 69800, lr = 1e-05
I0216 16:27:55.352682 25606 solver.cpp:472] Snapshotting to HDF5 file examples/cifar10/cifar10_full_iter_70000.caffemodel.h5
I0216 16:27:56.350297 25606 sgd_solver.cpp:279] Snapshotting solver state to HDF5 file examples/cifar10/cifar10_full_iter_70000.solverstate.h5
I0216 16:27:56.360047 25606 solver.cpp:321] Iteration 70000, loss = 0.0974508
I0216 16:27:56.360103 25606 solver.cpp:341] Iteration 70000, Testing net (#0)
I0216 16:27:57.633460 25606 solver.cpp:409]     Test net output #0: accuracy = 0.7988
I0216 16:27:57.633512 25606 solver.cpp:409]     Test net output #1: loss = 0.618197 (* 1 = 0.618197 loss)
I0216 16:27:57.633528 25606 solver.cpp:326] Optimization Done.
I0216 16:27:57.633535 25606 caffe.cpp:215] Optimization Done.
