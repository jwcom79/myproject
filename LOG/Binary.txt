name: "TestNetwork"
layer {
  name: "data"
  type: "HDF5Data"
  top: "data"
  top: "targets"
  phase: TRAIN
  hdf5_data_param {
    source: "src/caffe/test/test_data/solver_data_list.txt"
    batch_size: 4
  }
}
layer {
  name: "innerprod"
  type: "InnerProduct"
  bottom: "data"
  top: "innerprod"
  param {
    name: "weights"
  }
  param {
    name: "bias"
  }
  blobs {
    data: 1.0296971
    data: 0.63725638
    data: 1.3190306
    data: 1.2101091
    data: -1.2610097
    data: -0.110719
    data: -1.6982716
    data: 0.21727844
    data: 0.86093217
    data: -0.798694
    data: 1.157517
    data: -2.1385913
    data: 0.20715339
    data: 0.6759277
    data: 1.1031897
    data: 0.67151815
    data: 1.6012645
    data: -0.30685183
    data: -0.40943995
    data: -0.91969776
    data: -0.85468984
    data: 1.5952466
    data: 0.42440042
    data: 0.73762667
    data: -0.62307221
    data: 1.0788504
    data: -0.85092324
    data: 0.77048445
    data: 0.5581075
    data: -0.23958181
    data: 1.8194798
    data: 0.12836054
    data: -0.22422458
    data: -1.0440754
    data: 0.98555672
    data: 1.1786196
    data: -0.67444837
    data: -0.50025177
    data: 0.91418147
    data: -0.28722182
    data: 0.53535086
    data: -1.1114866
    data: -0.86988348
    data: -1.3623548
    data: -1.5948479
    data: -1.1523193
    data: 0.11375036
    data: 0.47434089
    data: 0.85189056
    data: 0.90686083
    data: 0.056884989
    data: -1.3653963
    data: -0.65831864
    data: -0.21233316
    data: -1.3871814
    data: 0.50881869
    data: 2.4033923
    data: -1.0470457
    data: -3.226367
    data: 0.372214
    data: 0.49426022
    data: -0.36498743
    data: 0.58187729
    data: 0.19941124
    data: -0.11393771
    data: 0.12746724
    data: -0.69553041
    data: 1.3599869
    data: -1.4481767
    data: 0.19737372
    data: 1.5002041
    data: -0.832536
    data: 1.057431
    data: 0.22988804
    data: -0.86364943
    data: 1.0123435
    data: 0.81041658
    data: -0.18438703
    data: -1.1301457
    data: -1.0351299
    data: -0.54757279
    data: -0.90866709
    data: 0.31824505
    data: -0.350268
    data: 0.1475751
    data: 0.94874531
    data: -0.027293749
    data: -1.0494252
    data: 1.8260466
    data: 0.56890249
    data: -0.044247538
    data: -0.87254536
    data: 0.54910231
    data: 2.0009089
    data: -1.2526169
    data: -1.4567786
    data: -1.1435884
    data: 0.67153382
    data: 0.27749544
    data: 0.6089834
    data: 1.557479
    data: 0.48587888
    data: 0.27018303
    data: 1.4834111
    data: 0.79260629
    data: 0.4167344
    data: 1.4216465
    data: 0.63745642
    data: 1.0938305
    data: 0.60030681
    data: 0.34387422
    data: -0.49996486
    data: 0.47621298
    data: -0.45391145
    data: 0.93650985
    data: -0.040977169
    data: -0.34677729
    data: -0.77411652
    data: 2.0181711
    data: -0.75523525
    data: 0.44999161
    data: -0.56656843
    data: -0.630213
    data: -0.87553912
    data: 1.3165998
    data: 0.90422016
    data: -1.0083041
    data: 0.00867955
    data: 0.905362
    data: 1.4382089
    data: 1.0503848
    data: -0.039665025
    data: 1.9878466
    data: 0.62816894
    data: -1.6967704
    data: 0.0050531588
    data: 0.6784488
    data: 0.038033761
    data: -0.71351075
    data: 1.6663215
    data: 0.49186054
    data: -0.83885729
    data: 1.1795866
    data: 0.30593821
    data: 1.1036385
    data: -1.2305186
    data: -1.0251743
    data: -0.30691695
    data: 0.079730928
    data: 2.2787473
    data: -1.9105616
    data: -1.1965876
    data: 0.84434044
    data: 1.2364089
    data: 0.35304049
    data: -0.32176176
    data: 0.27236739
    data: -0.88299435
    data: -0.58450764
    data: -0.54064316
    data: -0.83154666
    data: -0.17179288
    data: 0.03970493
    data: -0.47441459
    data: 2.4636135
    data: -0.27445635
    data: 0.83808208
    data: 2.1499729
    data: -0.00708787
    data: 0.45372456
    data: -0.34061328
    data: 0.014153632
    data: -1.5666567
    data: -0.68736219
    data: 0.7729882
    data: -1.9658935
    data: -0.0061686309
    data: 0.60622972
    data: -0.20175366
    data: 0.47568876
    data: -1.0220023
    data: 0.83638942
    data: -0.29032171
    data: -0.47292563
    data: -0.94849807
    data: 0.84111065
    data: -0.028708937
    data: -0.26449314
    data: -0.10773574
    data: 0.96646816
    data: 0.25656298
    data: -1.280459
    data: 1.3509187
    data: -2.3971286
    data: -1.8420578
    data: 0.028289052
    data: 0.1234161
    data: -1.0714803
    data: -1.9940426
    data: -0.36244047
    data: 0.62303662
    data: -0.47770113
    data: 0.43300825
    data: -1.9540169
    data: 0.40166152
    data: 0.86027956
    data: 2.0013013
    data: 0.44444159
    data: -0.82958609
    data: -1.473029
    data: 1.1445155
    data: 0.24484992
    data: 0.64942735
    data: 0.82102382
    data: 0.55945128
    data: 0.24433976
    data: -0.036530383
    data: -0.0007720124
    data: 0.12543897
    data: 1.1397034
    data: 0.47441357
    data: 0.36542237
    data: 0.17320046
    data: -0.48229265
    data: -1.1085783
    data: 1.662872
    data: -0.29127079
    data: -0.66352749
    data: 1.1907728
    data: -0.50822335
    data: -0.26049984
    data: 0.23552594
    data: -0.23443517
    data: 0.27924943
    data: 0.040193073
    data: -0.343174
    data: 0.88874221
    data: 0.7070387
    data: 0.57862705
    data: -0.18788549
    data: 0.8237164
    data: -0.078584865
    data: -0.59103054
    data: 0.0065648607
    data: -0.49263224
    data: -0.11313888
    data: -1.8423942
    data: 0.024361202
    data: -1.3292749
    data: 1.8126658
    data: -0.0086610829
    data: -0.53661549
    data: 0.29277739
    data: -0.30103356
    data: -0.38534498
    data: -0.90543193
    data: 0.66537887
    data: 0.74119347
    data: 0.61811489
    data: 1.2730935
    data: -0.517945
    data: 0.98107952
    data: -1.0528356
    data: -0.83176553
    data: -0.528956
    data: -1.370316
    data: -0.20518355
    data: 0.36011851
    data: -0.21049121
    data: -0.17324948
    data: -0.62730789
    data: -0.39151174
    data: -0.19592449
    data: -0.28026471
    data: -0.03440173
    data: 0.6612885
    data: -0.62165886
    data: -1.2248136
    data: 0.42236188
    data: 0.138322
    data: -0.39461598
    data: 0.59803742
    data: 1.8286878
    data: 0.78057122
    data: -0.12881765
    data: 2.4103491
    data: 0.64990842
    data: -1.862945
    data: 0.0985047
    data: -0.73266643
    data: -0.51012397
    data: 1.6989361
    data: -0.42618835
    data: 1.0543215
    data: 0.6919629
    data: -0.033879515
    data: 0.33451155
    data: -0.30114543
    data: 1.2618041
    data: -0.025588695
    shape {
      dim: 1
      dim: 300
    }
  }
  blobs {
    data: 0.45467621
    shape {
      dim: 1
    }
  }
  phase: TRAIN
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "gaussian"
      std: 1
    }
    bias_filler {
      type: "gaussian"
      std: 1
    }
  }
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "innerprod"
  bottom: "targets"
  loss_weight: 1
  phase: TRAIN
}
