I0218 17:37:32.689738 26599 caffe.cpp:184] Using GPUs 0
I0218 17:37:32.914718 26599 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 1000
base_lr: 0.001
display: 200
max_iter: 60000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.004
snapshot: 10000
snapshot_prefix: "examples/cifar10/cifar10_full"
solver_mode: GPU
device_id: 0
net: "examples/cifar10/cifar10_full_train_test.prototxt"
snapshot_format: HDF5
I0218 17:37:32.914882 26599 solver.cpp:91] Creating training net from net file: examples/cifar10/cifar10_full_train_test.prototxt
I0218 17:37:32.915568 26599 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0218 17:37:32.915608 26599 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0218 17:37:32.915788 26599 net.cpp:49] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TRAIN
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0218 17:37:32.915912 26599 layer_factory.hpp:77] Creating layer cifar
I0218 17:37:32.916561 26599 net.cpp:106] Creating Layer cifar
I0218 17:37:32.916596 26599 net.cpp:411] cifar -> data
I0218 17:37:32.916642 26599 net.cpp:411] cifar -> label
I0218 17:37:32.916671 26599 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0218 17:37:32.917589 26603 db_lmdb.cpp:38] Opened lmdb examples/cifar10/cifar10_train_lmdb
I0218 17:37:32.929760 26599 data_layer.cpp:41] output data size: 100,3,32,32
I0218 17:37:32.933118 26599 net.cpp:150] Setting up cifar
I0218 17:37:32.933161 26599 net.cpp:157] Top shape: 100 3 32 32 (307200)
I0218 17:37:32.933176 26599 net.cpp:157] Top shape: 100 (100)
I0218 17:37:32.933184 26599 net.cpp:165] Memory required for data: 1229200
I0218 17:37:32.933198 26599 layer_factory.hpp:77] Creating layer conv1
I0218 17:37:32.933224 26599 net.cpp:106] Creating Layer conv1
I0218 17:37:32.933238 26599 net.cpp:454] conv1 <- data
I0218 17:37:32.933257 26599 net.cpp:411] conv1 -> conv1
I0218 17:37:33.102164 26599 net.cpp:150] Setting up conv1
I0218 17:37:33.102216 26599 net.cpp:157] Top shape: 100 32 32 32 (3276800)
I0218 17:37:33.102229 26599 net.cpp:165] Memory required for data: 14336400
I0218 17:37:33.102258 26599 layer_factory.hpp:77] Creating layer pool1
I0218 17:37:33.102280 26599 net.cpp:106] Creating Layer pool1
I0218 17:37:33.102293 26599 net.cpp:454] pool1 <- conv1
I0218 17:37:33.102305 26599 net.cpp:411] pool1 -> pool1
I0218 17:37:33.103148 26599 net.cpp:150] Setting up pool1
I0218 17:37:33.103170 26599 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0218 17:37:33.103180 26599 net.cpp:165] Memory required for data: 17613200
I0218 17:37:33.103188 26599 layer_factory.hpp:77] Creating layer relu1
I0218 17:37:33.103200 26599 net.cpp:106] Creating Layer relu1
I0218 17:37:33.103209 26599 net.cpp:454] relu1 <- pool1
I0218 17:37:33.103219 26599 net.cpp:397] relu1 -> pool1 (in-place)
I0218 17:37:33.103988 26599 net.cpp:150] Setting up relu1
I0218 17:37:33.104012 26599 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0218 17:37:33.104022 26599 net.cpp:165] Memory required for data: 20890000
I0218 17:37:33.104032 26599 layer_factory.hpp:77] Creating layer norm1
I0218 17:37:33.104049 26599 net.cpp:106] Creating Layer norm1
I0218 17:37:33.104060 26599 net.cpp:454] norm1 <- pool1
I0218 17:37:33.104073 26599 net.cpp:411] norm1 -> norm1
I0218 17:37:33.105279 26599 net.cpp:150] Setting up norm1
I0218 17:37:33.105304 26599 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0218 17:37:33.105314 26599 net.cpp:165] Memory required for data: 24166800
I0218 17:37:33.105322 26599 layer_factory.hpp:77] Creating layer conv2
I0218 17:37:33.105337 26599 net.cpp:106] Creating Layer conv2
I0218 17:37:33.105345 26599 net.cpp:454] conv2 <- norm1
I0218 17:37:33.105355 26599 net.cpp:411] conv2 -> conv2
I0218 17:37:33.109539 26599 net.cpp:150] Setting up conv2
I0218 17:37:33.109580 26599 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0218 17:37:33.109593 26599 net.cpp:165] Memory required for data: 27443600
I0218 17:37:33.109614 26599 layer_factory.hpp:77] Creating layer relu2
I0218 17:37:33.109629 26599 net.cpp:106] Creating Layer relu2
I0218 17:37:33.109642 26599 net.cpp:454] relu2 <- conv2
I0218 17:37:33.109655 26599 net.cpp:397] relu2 -> conv2 (in-place)
I0218 17:37:33.110524 26599 net.cpp:150] Setting up relu2
I0218 17:37:33.110545 26599 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0218 17:37:33.110555 26599 net.cpp:165] Memory required for data: 30720400
I0218 17:37:33.110563 26599 layer_factory.hpp:77] Creating layer pool2
I0218 17:37:33.110577 26599 net.cpp:106] Creating Layer pool2
I0218 17:37:33.110586 26599 net.cpp:454] pool2 <- conv2
I0218 17:37:33.110596 26599 net.cpp:411] pool2 -> pool2
I0218 17:37:33.111471 26599 net.cpp:150] Setting up pool2
I0218 17:37:33.111493 26599 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0218 17:37:33.111503 26599 net.cpp:165] Memory required for data: 31539600
I0218 17:37:33.111511 26599 layer_factory.hpp:77] Creating layer norm2
I0218 17:37:33.111531 26599 net.cpp:106] Creating Layer norm2
I0218 17:37:33.111541 26599 net.cpp:454] norm2 <- pool2
I0218 17:37:33.111554 26599 net.cpp:411] norm2 -> norm2
I0218 17:37:33.112818 26599 net.cpp:150] Setting up norm2
I0218 17:37:33.112850 26599 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0218 17:37:33.112860 26599 net.cpp:165] Memory required for data: 32358800
I0218 17:37:33.112867 26599 layer_factory.hpp:77] Creating layer conv3
I0218 17:37:33.112882 26599 net.cpp:106] Creating Layer conv3
I0218 17:37:33.112891 26599 net.cpp:454] conv3 <- norm2
I0218 17:37:33.112915 26599 net.cpp:411] conv3 -> conv3
I0218 17:37:33.117707 26599 net.cpp:150] Setting up conv3
I0218 17:37:33.117754 26599 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0218 17:37:33.117765 26599 net.cpp:165] Memory required for data: 33997200
I0218 17:37:33.117789 26599 layer_factory.hpp:77] Creating layer relu3
I0218 17:37:33.117806 26599 net.cpp:106] Creating Layer relu3
I0218 17:37:33.117817 26599 net.cpp:454] relu3 <- conv3
I0218 17:37:33.117831 26599 net.cpp:397] relu3 -> conv3 (in-place)
I0218 17:37:33.118708 26599 net.cpp:150] Setting up relu3
I0218 17:37:33.118731 26599 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0218 17:37:33.118741 26599 net.cpp:165] Memory required for data: 35635600
I0218 17:37:33.118748 26599 layer_factory.hpp:77] Creating layer pool3
I0218 17:37:33.118764 26599 net.cpp:106] Creating Layer pool3
I0218 17:37:33.118774 26599 net.cpp:454] pool3 <- conv3
I0218 17:37:33.118785 26599 net.cpp:411] pool3 -> pool3
I0218 17:37:33.119686 26599 net.cpp:150] Setting up pool3
I0218 17:37:33.119706 26599 net.cpp:157] Top shape: 100 64 4 4 (102400)
I0218 17:37:33.119716 26599 net.cpp:165] Memory required for data: 36045200
I0218 17:37:33.119724 26599 layer_factory.hpp:77] Creating layer ip1
I0218 17:37:33.119742 26599 net.cpp:106] Creating Layer ip1
I0218 17:37:33.119753 26599 net.cpp:454] ip1 <- pool3
I0218 17:37:33.119767 26599 net.cpp:411] ip1 -> ip1
I0218 17:37:33.137353 26599 net.cpp:150] Setting up ip1
I0218 17:37:33.137405 26599 net.cpp:157] Top shape: 100 500 (50000)
I0218 17:37:33.137418 26599 net.cpp:165] Memory required for data: 36245200
I0218 17:37:33.137435 26599 layer_factory.hpp:77] Creating layer relu4
I0218 17:37:33.137454 26599 net.cpp:106] Creating Layer relu4
I0218 17:37:33.137465 26599 net.cpp:454] relu4 <- ip1
I0218 17:37:33.137481 26599 net.cpp:397] relu4 -> ip1 (in-place)
I0218 17:37:33.138490 26599 net.cpp:150] Setting up relu4
I0218 17:37:33.138512 26599 net.cpp:157] Top shape: 100 500 (50000)
I0218 17:37:33.138521 26599 net.cpp:165] Memory required for data: 36445200
I0218 17:37:33.138530 26599 layer_factory.hpp:77] Creating layer ip2
I0218 17:37:33.138546 26599 net.cpp:106] Creating Layer ip2
I0218 17:37:33.138556 26599 net.cpp:454] ip2 <- ip1
I0218 17:37:33.138568 26599 net.cpp:411] ip2 -> ip2
I0218 17:37:33.139401 26599 net.cpp:150] Setting up ip2
I0218 17:37:33.139427 26599 net.cpp:157] Top shape: 100 10 (1000)
I0218 17:37:33.139436 26599 net.cpp:165] Memory required for data: 36449200
I0218 17:37:33.139456 26599 layer_factory.hpp:77] Creating layer loss
I0218 17:37:33.139472 26599 net.cpp:106] Creating Layer loss
I0218 17:37:33.139483 26599 net.cpp:454] loss <- ip2
I0218 17:37:33.139492 26599 net.cpp:454] loss <- label
I0218 17:37:33.139506 26599 net.cpp:411] loss -> loss
I0218 17:37:33.139524 26599 layer_factory.hpp:77] Creating layer loss
I0218 17:37:33.140544 26599 net.cpp:150] Setting up loss
I0218 17:37:33.140563 26599 net.cpp:157] Top shape: (1)
I0218 17:37:33.140569 26599 net.cpp:160]     with loss weight 1
I0218 17:37:33.140588 26599 net.cpp:165] Memory required for data: 36449204
I0218 17:37:33.140594 26599 net.cpp:226] loss needs backward computation.
I0218 17:37:33.140601 26599 net.cpp:226] ip2 needs backward computation.
I0218 17:37:33.140607 26599 net.cpp:226] relu4 needs backward computation.
I0218 17:37:33.140614 26599 net.cpp:226] ip1 needs backward computation.
I0218 17:37:33.140619 26599 net.cpp:226] pool3 needs backward computation.
I0218 17:37:33.140626 26599 net.cpp:226] relu3 needs backward computation.
I0218 17:37:33.140632 26599 net.cpp:226] conv3 needs backward computation.
I0218 17:37:33.140638 26599 net.cpp:226] norm2 needs backward computation.
I0218 17:37:33.140662 26599 net.cpp:226] pool2 needs backward computation.
I0218 17:37:33.140669 26599 net.cpp:226] relu2 needs backward computation.
I0218 17:37:33.140674 26599 net.cpp:226] conv2 needs backward computation.
I0218 17:37:33.140681 26599 net.cpp:226] norm1 needs backward computation.
I0218 17:37:33.140689 26599 net.cpp:226] relu1 needs backward computation.
I0218 17:37:33.140696 26599 net.cpp:226] pool1 needs backward computation.
I0218 17:37:33.140702 26599 net.cpp:226] conv1 needs backward computation.
I0218 17:37:33.140708 26599 net.cpp:228] cifar does not need backward computation.
I0218 17:37:33.140713 26599 net.cpp:270] This network produces output loss
I0218 17:37:33.140732 26599 net.cpp:283] Network initialization done.
I0218 17:37:33.141427 26599 solver.cpp:181] Creating test net (#0) specified by net file: examples/cifar10/cifar10_full_train_test.prototxt
I0218 17:37:33.141484 26599 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0218 17:37:33.141683 26599 net.cpp:49] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0218 17:37:33.141829 26599 layer_factory.hpp:77] Creating layer cifar
I0218 17:37:33.141978 26599 net.cpp:106] Creating Layer cifar
I0218 17:37:33.141998 26599 net.cpp:411] cifar -> data
I0218 17:37:33.142019 26599 net.cpp:411] cifar -> label
I0218 17:37:33.142036 26599 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0218 17:37:33.143056 26605 db_lmdb.cpp:38] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0218 17:37:33.143220 26599 data_layer.cpp:41] output data size: 100,3,32,32
I0218 17:37:33.146412 26599 net.cpp:150] Setting up cifar
I0218 17:37:33.146461 26599 net.cpp:157] Top shape: 100 3 32 32 (307200)
I0218 17:37:33.146474 26599 net.cpp:157] Top shape: 100 (100)
I0218 17:37:33.146481 26599 net.cpp:165] Memory required for data: 1229200
I0218 17:37:33.146491 26599 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0218 17:37:33.146508 26599 net.cpp:106] Creating Layer label_cifar_1_split
I0218 17:37:33.146517 26599 net.cpp:454] label_cifar_1_split <- label
I0218 17:37:33.146529 26599 net.cpp:411] label_cifar_1_split -> label_cifar_1_split_0
I0218 17:37:33.146546 26599 net.cpp:411] label_cifar_1_split -> label_cifar_1_split_1
I0218 17:37:33.146622 26599 net.cpp:150] Setting up label_cifar_1_split
I0218 17:37:33.146639 26599 net.cpp:157] Top shape: 100 (100)
I0218 17:37:33.146649 26599 net.cpp:157] Top shape: 100 (100)
I0218 17:37:33.146656 26599 net.cpp:165] Memory required for data: 1230000
I0218 17:37:33.146664 26599 layer_factory.hpp:77] Creating layer conv1
I0218 17:37:33.146684 26599 net.cpp:106] Creating Layer conv1
I0218 17:37:33.146695 26599 net.cpp:454] conv1 <- data
I0218 17:37:33.146710 26599 net.cpp:411] conv1 -> conv1
I0218 17:37:33.150264 26599 net.cpp:150] Setting up conv1
I0218 17:37:33.150301 26599 net.cpp:157] Top shape: 100 32 32 32 (3276800)
I0218 17:37:33.150315 26599 net.cpp:165] Memory required for data: 14337200
I0218 17:37:33.150333 26599 layer_factory.hpp:77] Creating layer pool1
I0218 17:37:33.150355 26599 net.cpp:106] Creating Layer pool1
I0218 17:37:33.150369 26599 net.cpp:454] pool1 <- conv1
I0218 17:37:33.150384 26599 net.cpp:411] pool1 -> pool1
I0218 17:37:33.152011 26599 net.cpp:150] Setting up pool1
I0218 17:37:33.152040 26599 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0218 17:37:33.152051 26599 net.cpp:165] Memory required for data: 17614000
I0218 17:37:33.152060 26599 layer_factory.hpp:77] Creating layer relu1
I0218 17:37:33.152078 26599 net.cpp:106] Creating Layer relu1
I0218 17:37:33.152091 26599 net.cpp:454] relu1 <- pool1
I0218 17:37:33.152113 26599 net.cpp:397] relu1 -> pool1 (in-place)
I0218 17:37:33.153053 26599 net.cpp:150] Setting up relu1
I0218 17:37:33.153077 26599 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0218 17:37:33.153087 26599 net.cpp:165] Memory required for data: 20890800
I0218 17:37:33.153096 26599 layer_factory.hpp:77] Creating layer norm1
I0218 17:37:33.153118 26599 net.cpp:106] Creating Layer norm1
I0218 17:37:33.153128 26599 net.cpp:454] norm1 <- pool1
I0218 17:37:33.153144 26599 net.cpp:411] norm1 -> norm1
I0218 17:37:33.154546 26599 net.cpp:150] Setting up norm1
I0218 17:37:33.154575 26599 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0218 17:37:33.154587 26599 net.cpp:165] Memory required for data: 24167600
I0218 17:37:33.154597 26599 layer_factory.hpp:77] Creating layer conv2
I0218 17:37:33.154613 26599 net.cpp:106] Creating Layer conv2
I0218 17:37:33.154620 26599 net.cpp:454] conv2 <- norm1
I0218 17:37:33.154635 26599 net.cpp:411] conv2 -> conv2
I0218 17:37:33.158957 26599 net.cpp:150] Setting up conv2
I0218 17:37:33.158993 26599 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0218 17:37:33.159003 26599 net.cpp:165] Memory required for data: 27444400
I0218 17:37:33.159028 26599 layer_factory.hpp:77] Creating layer relu2
I0218 17:37:33.159054 26599 net.cpp:106] Creating Layer relu2
I0218 17:37:33.159075 26599 net.cpp:454] relu2 <- conv2
I0218 17:37:33.159088 26599 net.cpp:397] relu2 -> conv2 (in-place)
I0218 17:37:33.160001 26599 net.cpp:150] Setting up relu2
I0218 17:37:33.160023 26599 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0218 17:37:33.160034 26599 net.cpp:165] Memory required for data: 30721200
I0218 17:37:33.160043 26599 layer_factory.hpp:77] Creating layer pool2
I0218 17:37:33.160056 26599 net.cpp:106] Creating Layer pool2
I0218 17:37:33.160065 26599 net.cpp:454] pool2 <- conv2
I0218 17:37:33.160076 26599 net.cpp:411] pool2 -> pool2
I0218 17:37:33.160996 26599 net.cpp:150] Setting up pool2
I0218 17:37:33.161020 26599 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0218 17:37:33.161028 26599 net.cpp:165] Memory required for data: 31540400
I0218 17:37:33.161036 26599 layer_factory.hpp:77] Creating layer norm2
I0218 17:37:33.161052 26599 net.cpp:106] Creating Layer norm2
I0218 17:37:33.161062 26599 net.cpp:454] norm2 <- pool2
I0218 17:37:33.161072 26599 net.cpp:411] norm2 -> norm2
I0218 17:37:33.162420 26599 net.cpp:150] Setting up norm2
I0218 17:37:33.162444 26599 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0218 17:37:33.162453 26599 net.cpp:165] Memory required for data: 32359600
I0218 17:37:33.162461 26599 layer_factory.hpp:77] Creating layer conv3
I0218 17:37:33.162480 26599 net.cpp:106] Creating Layer conv3
I0218 17:37:33.162492 26599 net.cpp:454] conv3 <- norm2
I0218 17:37:33.162506 26599 net.cpp:411] conv3 -> conv3
I0218 17:37:33.167462 26599 net.cpp:150] Setting up conv3
I0218 17:37:33.167501 26599 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0218 17:37:33.167513 26599 net.cpp:165] Memory required for data: 33998000
I0218 17:37:33.167534 26599 layer_factory.hpp:77] Creating layer relu3
I0218 17:37:33.167558 26599 net.cpp:106] Creating Layer relu3
I0218 17:37:33.167570 26599 net.cpp:454] relu3 <- conv3
I0218 17:37:33.167582 26599 net.cpp:397] relu3 -> conv3 (in-place)
I0218 17:37:33.168464 26599 net.cpp:150] Setting up relu3
I0218 17:37:33.168488 26599 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0218 17:37:33.168498 26599 net.cpp:165] Memory required for data: 35636400
I0218 17:37:33.168505 26599 layer_factory.hpp:77] Creating layer pool3
I0218 17:37:33.168519 26599 net.cpp:106] Creating Layer pool3
I0218 17:37:33.168526 26599 net.cpp:454] pool3 <- conv3
I0218 17:37:33.168540 26599 net.cpp:411] pool3 -> pool3
I0218 17:37:33.169457 26599 net.cpp:150] Setting up pool3
I0218 17:37:33.169479 26599 net.cpp:157] Top shape: 100 64 4 4 (102400)
I0218 17:37:33.169489 26599 net.cpp:165] Memory required for data: 36046000
I0218 17:37:33.169497 26599 layer_factory.hpp:77] Creating layer ip1
I0218 17:37:33.169513 26599 net.cpp:106] Creating Layer ip1
I0218 17:37:33.169523 26599 net.cpp:454] ip1 <- pool3
I0218 17:37:33.169538 26599 net.cpp:411] ip1 -> ip1
I0218 17:37:33.187160 26599 net.cpp:150] Setting up ip1
I0218 17:37:33.187211 26599 net.cpp:157] Top shape: 100 500 (50000)
I0218 17:37:33.187223 26599 net.cpp:165] Memory required for data: 36246000
I0218 17:37:33.187240 26599 layer_factory.hpp:77] Creating layer relu4
I0218 17:37:33.187258 26599 net.cpp:106] Creating Layer relu4
I0218 17:37:33.187268 26599 net.cpp:454] relu4 <- ip1
I0218 17:37:33.187280 26599 net.cpp:397] relu4 -> ip1 (in-place)
I0218 17:37:33.188345 26599 net.cpp:150] Setting up relu4
I0218 17:37:33.188367 26599 net.cpp:157] Top shape: 100 500 (50000)
I0218 17:37:33.188377 26599 net.cpp:165] Memory required for data: 36446000
I0218 17:37:33.188385 26599 layer_factory.hpp:77] Creating layer ip2
I0218 17:37:33.188401 26599 net.cpp:106] Creating Layer ip2
I0218 17:37:33.188411 26599 net.cpp:454] ip2 <- ip1
I0218 17:37:33.188423 26599 net.cpp:411] ip2 -> ip2
I0218 17:37:33.188804 26599 net.cpp:150] Setting up ip2
I0218 17:37:33.188824 26599 net.cpp:157] Top shape: 100 10 (1000)
I0218 17:37:33.188834 26599 net.cpp:165] Memory required for data: 36450000
I0218 17:37:33.188854 26599 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0218 17:37:33.188873 26599 net.cpp:106] Creating Layer ip2_ip2_0_split
I0218 17:37:33.188894 26599 net.cpp:454] ip2_ip2_0_split <- ip2
I0218 17:37:33.188930 26599 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0218 17:37:33.188946 26599 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0218 17:37:33.189018 26599 net.cpp:150] Setting up ip2_ip2_0_split
I0218 17:37:33.189033 26599 net.cpp:157] Top shape: 100 10 (1000)
I0218 17:37:33.189043 26599 net.cpp:157] Top shape: 100 10 (1000)
I0218 17:37:33.189049 26599 net.cpp:165] Memory required for data: 36458000
I0218 17:37:33.189055 26599 layer_factory.hpp:77] Creating layer accuracy
I0218 17:37:33.189069 26599 net.cpp:106] Creating Layer accuracy
I0218 17:37:33.189079 26599 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I0218 17:37:33.189086 26599 net.cpp:454] accuracy <- label_cifar_1_split_0
I0218 17:37:33.189097 26599 net.cpp:411] accuracy -> accuracy
I0218 17:37:33.189116 26599 net.cpp:150] Setting up accuracy
I0218 17:37:33.189131 26599 net.cpp:157] Top shape: (1)
I0218 17:37:33.189139 26599 net.cpp:165] Memory required for data: 36458004
I0218 17:37:33.189147 26599 layer_factory.hpp:77] Creating layer loss
I0218 17:37:33.189157 26599 net.cpp:106] Creating Layer loss
I0218 17:37:33.189165 26599 net.cpp:454] loss <- ip2_ip2_0_split_1
I0218 17:37:33.189174 26599 net.cpp:454] loss <- label_cifar_1_split_1
I0218 17:37:33.189185 26599 net.cpp:411] loss -> loss
I0218 17:37:33.189200 26599 layer_factory.hpp:77] Creating layer loss
I0218 17:37:33.190404 26599 net.cpp:150] Setting up loss
I0218 17:37:33.190426 26599 net.cpp:157] Top shape: (1)
I0218 17:37:33.190436 26599 net.cpp:160]     with loss weight 1
I0218 17:37:33.190450 26599 net.cpp:165] Memory required for data: 36458008
I0218 17:37:33.190459 26599 net.cpp:226] loss needs backward computation.
I0218 17:37:33.190469 26599 net.cpp:228] accuracy does not need backward computation.
I0218 17:37:33.190476 26599 net.cpp:226] ip2_ip2_0_split needs backward computation.
I0218 17:37:33.190484 26599 net.cpp:226] ip2 needs backward computation.
I0218 17:37:33.190490 26599 net.cpp:226] relu4 needs backward computation.
I0218 17:37:33.190495 26599 net.cpp:226] ip1 needs backward computation.
I0218 17:37:33.190501 26599 net.cpp:226] pool3 needs backward computation.
I0218 17:37:33.190508 26599 net.cpp:226] relu3 needs backward computation.
I0218 17:37:33.190515 26599 net.cpp:226] conv3 needs backward computation.
I0218 17:37:33.190521 26599 net.cpp:226] norm2 needs backward computation.
I0218 17:37:33.190526 26599 net.cpp:226] pool2 needs backward computation.
I0218 17:37:33.190532 26599 net.cpp:226] relu2 needs backward computation.
I0218 17:37:33.190538 26599 net.cpp:226] conv2 needs backward computation.
I0218 17:37:33.190544 26599 net.cpp:226] norm1 needs backward computation.
I0218 17:37:33.190551 26599 net.cpp:226] relu1 needs backward computation.
I0218 17:37:33.190557 26599 net.cpp:226] pool1 needs backward computation.
I0218 17:37:33.190562 26599 net.cpp:226] conv1 needs backward computation.
I0218 17:37:33.190569 26599 net.cpp:228] label_cifar_1_split does not need backward computation.
I0218 17:37:33.190579 26599 net.cpp:228] cifar does not need backward computation.
I0218 17:37:33.190587 26599 net.cpp:270] This network produces output accuracy
I0218 17:37:33.190593 26599 net.cpp:270] This network produces output loss
I0218 17:37:33.190615 26599 net.cpp:283] Network initialization done.
I0218 17:37:33.190742 26599 solver.cpp:60] Solver scaffolding done.
I0218 17:37:33.191321 26599 caffe.cpp:212] Starting Optimization
I0218 17:37:33.191342 26599 solver.cpp:288] Solving CIFAR10_full
I0218 17:37:33.191350 26599 solver.cpp:289] Learning Rate Policy: fixed
I0218 17:37:33.192090 26599 solver.cpp:341] Iteration 0, Testing net (#0)
I0218 17:37:33.703804 26599 solver.cpp:409]     Test net output #0: accuracy = 0.1076
I0218 17:37:33.703856 26599 solver.cpp:409]     Test net output #1: loss = 2.30259 (* 1 = 2.30259 loss)
I0218 17:37:33.711246 26599 solver.cpp:237] Iteration 0, loss = 2.30259
I0218 17:37:33.711285 26599 solver.cpp:253]     Train net output #0: loss = 2.30259 (* 1 = 2.30259 loss)
I0218 17:37:33.711300 26599 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I0218 17:37:36.839972 26599 solver.cpp:237] Iteration 200, loss = 2.30262
I0218 17:37:36.840013 26599 solver.cpp:253]     Train net output #0: loss = 2.30262 (* 1 = 2.30262 loss)
I0218 17:37:36.840019 26599 sgd_solver.cpp:106] Iteration 200, lr = 0.001
I0218 17:37:40.024971 26599 solver.cpp:237] Iteration 400, loss = 2.30257
I0218 17:37:40.025012 26599 solver.cpp:253]     Train net output #0: loss = 2.30257 (* 1 = 2.30257 loss)
I0218 17:37:40.025018 26599 sgd_solver.cpp:106] Iteration 400, lr = 0.001
I0218 17:37:43.215983 26599 solver.cpp:237] Iteration 600, loss = 2.14908
I0218 17:37:43.216027 26599 solver.cpp:253]     Train net output #0: loss = 2.14908 (* 1 = 2.14908 loss)
I0218 17:37:43.216039 26599 sgd_solver.cpp:106] Iteration 600, lr = 0.001
I0218 17:37:46.386732 26599 solver.cpp:237] Iteration 800, loss = 1.98906
I0218 17:37:46.386770 26599 solver.cpp:253]     Train net output #0: loss = 1.98906 (* 1 = 1.98906 loss)
I0218 17:37:46.386776 26599 sgd_solver.cpp:106] Iteration 800, lr = 0.001
I0218 17:37:49.534593 26599 solver.cpp:341] Iteration 1000, Testing net (#0)
I0218 17:37:50.060127 26599 solver.cpp:409]     Test net output #0: accuracy = 0.3222
I0218 17:37:50.060163 26599 solver.cpp:409]     Test net output #1: loss = 1.83163 (* 1 = 1.83163 loss)
I0218 17:37:50.065580 26599 solver.cpp:237] Iteration 1000, loss = 1.96085
I0218 17:37:50.065618 26599 solver.cpp:253]     Train net output #0: loss = 1.96085 (* 1 = 1.96085 loss)
I0218 17:37:50.065624 26599 sgd_solver.cpp:106] Iteration 1000, lr = 0.001
I0218 17:37:53.213305 26599 solver.cpp:237] Iteration 1200, loss = 1.70555
I0218 17:37:53.213343 26599 solver.cpp:253]     Train net output #0: loss = 1.70555 (* 1 = 1.70555 loss)
I0218 17:37:53.213351 26599 sgd_solver.cpp:106] Iteration 1200, lr = 0.001
I0218 17:37:56.370179 26599 solver.cpp:237] Iteration 1400, loss = 1.42395
I0218 17:37:56.370223 26599 solver.cpp:253]     Train net output #0: loss = 1.42395 (* 1 = 1.42395 loss)
I0218 17:37:56.370229 26599 sgd_solver.cpp:106] Iteration 1400, lr = 0.001
I0218 17:37:59.557797 26599 solver.cpp:237] Iteration 1600, loss = 1.49284
I0218 17:37:59.557857 26599 solver.cpp:253]     Train net output #0: loss = 1.49284 (* 1 = 1.49284 loss)
I0218 17:37:59.557864 26599 sgd_solver.cpp:106] Iteration 1600, lr = 0.001
I0218 17:38:02.715440 26599 solver.cpp:237] Iteration 1800, loss = 1.38665
I0218 17:38:02.715507 26599 solver.cpp:253]     Train net output #0: loss = 1.38665 (* 1 = 1.38665 loss)
I0218 17:38:02.715514 26599 sgd_solver.cpp:106] Iteration 1800, lr = 0.001
I0218 17:38:05.853963 26599 solver.cpp:341] Iteration 2000, Testing net (#0)
I0218 17:38:06.383491 26599 solver.cpp:409]     Test net output #0: accuracy = 0.455
I0218 17:38:06.383532 26599 solver.cpp:409]     Test net output #1: loss = 1.49187 (* 1 = 1.49187 loss)
I0218 17:38:06.389506 26599 solver.cpp:237] Iteration 2000, loss = 1.52325
I0218 17:38:06.389534 26599 solver.cpp:253]     Train net output #0: loss = 1.52325 (* 1 = 1.52325 loss)
I0218 17:38:06.389542 26599 sgd_solver.cpp:106] Iteration 2000, lr = 0.001
I0218 17:38:09.555274 26599 solver.cpp:237] Iteration 2200, loss = 1.4678
I0218 17:38:09.555318 26599 solver.cpp:253]     Train net output #0: loss = 1.4678 (* 1 = 1.4678 loss)
I0218 17:38:09.555325 26599 sgd_solver.cpp:106] Iteration 2200, lr = 0.001
I0218 17:38:12.716190 26599 solver.cpp:237] Iteration 2400, loss = 1.20465
I0218 17:38:12.716233 26599 solver.cpp:253]     Train net output #0: loss = 1.20465 (* 1 = 1.20465 loss)
I0218 17:38:12.716239 26599 sgd_solver.cpp:106] Iteration 2400, lr = 0.001
I0218 17:38:15.870638 26599 solver.cpp:237] Iteration 2600, loss = 1.28876
I0218 17:38:15.870681 26599 solver.cpp:253]     Train net output #0: loss = 1.28876 (* 1 = 1.28876 loss)
I0218 17:38:15.870688 26599 sgd_solver.cpp:106] Iteration 2600, lr = 0.001
I0218 17:38:19.023769 26599 solver.cpp:237] Iteration 2800, loss = 1.32046
I0218 17:38:19.023813 26599 solver.cpp:253]     Train net output #0: loss = 1.32046 (* 1 = 1.32046 loss)
I0218 17:38:19.023834 26599 sgd_solver.cpp:106] Iteration 2800, lr = 0.001
I0218 17:38:22.161804 26599 solver.cpp:341] Iteration 3000, Testing net (#0)
I0218 17:38:22.697823 26599 solver.cpp:409]     Test net output #0: accuracy = 0.5245
I0218 17:38:22.697870 26599 solver.cpp:409]     Test net output #1: loss = 1.31204 (* 1 = 1.31204 loss)
I0218 17:38:22.703434 26599 solver.cpp:237] Iteration 3000, loss = 1.28151
I0218 17:38:22.703474 26599 solver.cpp:253]     Train net output #0: loss = 1.28151 (* 1 = 1.28151 loss)
I0218 17:38:22.703482 26599 sgd_solver.cpp:106] Iteration 3000, lr = 0.001
I0218 17:38:25.882972 26599 solver.cpp:237] Iteration 3200, loss = 1.28911
I0218 17:38:25.883013 26599 solver.cpp:253]     Train net output #0: loss = 1.28911 (* 1 = 1.28911 loss)
I0218 17:38:25.883020 26599 sgd_solver.cpp:106] Iteration 3200, lr = 0.001
I0218 17:38:29.032757 26599 solver.cpp:237] Iteration 3400, loss = 1.06849
I0218 17:38:29.032801 26599 solver.cpp:253]     Train net output #0: loss = 1.06849 (* 1 = 1.06849 loss)
I0218 17:38:29.032809 26599 sgd_solver.cpp:106] Iteration 3400, lr = 0.001
I0218 17:38:32.207778 26599 solver.cpp:237] Iteration 3600, loss = 1.12091
I0218 17:38:32.207821 26599 solver.cpp:253]     Train net output #0: loss = 1.12091 (* 1 = 1.12091 loss)
I0218 17:38:32.207828 26599 sgd_solver.cpp:106] Iteration 3600, lr = 0.001
I0218 17:38:35.339973 26599 solver.cpp:237] Iteration 3800, loss = 1.15772
I0218 17:38:35.340080 26599 solver.cpp:253]     Train net output #0: loss = 1.15772 (* 1 = 1.15772 loss)
I0218 17:38:35.340090 26599 sgd_solver.cpp:106] Iteration 3800, lr = 0.001
I0218 17:38:38.467830 26599 solver.cpp:341] Iteration 4000, Testing net (#0)
I0218 17:38:39.002166 26599 solver.cpp:409]     Test net output #0: accuracy = 0.5752
I0218 17:38:39.002207 26599 solver.cpp:409]     Test net output #1: loss = 1.18545 (* 1 = 1.18545 loss)
I0218 17:38:39.007583 26599 solver.cpp:237] Iteration 4000, loss = 1.11942
I0218 17:38:39.007617 26599 solver.cpp:253]     Train net output #0: loss = 1.11942 (* 1 = 1.11942 loss)
I0218 17:38:39.007624 26599 sgd_solver.cpp:106] Iteration 4000, lr = 0.001
I0218 17:38:42.158684 26599 solver.cpp:237] Iteration 4200, loss = 1.13585
I0218 17:38:42.158727 26599 solver.cpp:253]     Train net output #0: loss = 1.13585 (* 1 = 1.13585 loss)
I0218 17:38:42.158735 26599 sgd_solver.cpp:106] Iteration 4200, lr = 0.001
I0218 17:38:45.347831 26599 solver.cpp:237] Iteration 4400, loss = 0.976789
I0218 17:38:45.347874 26599 solver.cpp:253]     Train net output #0: loss = 0.976789 (* 1 = 0.976789 loss)
I0218 17:38:45.347882 26599 sgd_solver.cpp:106] Iteration 4400, lr = 0.001
I0218 17:38:48.541609 26599 solver.cpp:237] Iteration 4600, loss = 0.985437
I0218 17:38:48.541658 26599 solver.cpp:253]     Train net output #0: loss = 0.985437 (* 1 = 0.985437 loss)
I0218 17:38:48.541911 26599 sgd_solver.cpp:106] Iteration 4600, lr = 0.001
I0218 17:38:51.727885 26599 solver.cpp:237] Iteration 4800, loss = 0.998062
I0218 17:38:51.727931 26599 solver.cpp:253]     Train net output #0: loss = 0.998062 (* 1 = 0.998062 loss)
I0218 17:38:51.727941 26599 sgd_solver.cpp:106] Iteration 4800, lr = 0.001
I0218 17:38:54.892165 26599 solver.cpp:341] Iteration 5000, Testing net (#0)
I0218 17:38:55.429143 26599 solver.cpp:409]     Test net output #0: accuracy = 0.6128
I0218 17:38:55.429186 26599 solver.cpp:409]     Test net output #1: loss = 1.08905 (* 1 = 1.08905 loss)
I0218 17:38:55.434198 26599 solver.cpp:237] Iteration 5000, loss = 0.984189
I0218 17:38:55.434226 26599 solver.cpp:253]     Train net output #0: loss = 0.984189 (* 1 = 0.984189 loss)
I0218 17:38:55.434239 26599 sgd_solver.cpp:106] Iteration 5000, lr = 0.001
I0218 17:38:58.627737 26599 solver.cpp:237] Iteration 5200, loss = 1.02883
I0218 17:38:58.627790 26599 solver.cpp:253]     Train net output #0: loss = 1.02883 (* 1 = 1.02883 loss)
I0218 17:38:58.628034 26599 sgd_solver.cpp:106] Iteration 5200, lr = 0.001
I0218 17:39:01.802412 26599 solver.cpp:237] Iteration 5400, loss = 0.890833
I0218 17:39:01.802458 26599 solver.cpp:253]     Train net output #0: loss = 0.890833 (* 1 = 0.890833 loss)
I0218 17:39:01.802484 26599 sgd_solver.cpp:106] Iteration 5400, lr = 0.001
I0218 17:39:04.964567 26599 solver.cpp:237] Iteration 5600, loss = 0.874937
I0218 17:39:04.964614 26599 solver.cpp:253]     Train net output #0: loss = 0.874937 (* 1 = 0.874937 loss)
I0218 17:39:04.964627 26599 sgd_solver.cpp:106] Iteration 5600, lr = 0.001
I0218 17:39:08.129720 26599 solver.cpp:237] Iteration 5800, loss = 0.8922
I0218 17:39:08.129827 26599 solver.cpp:253]     Train net output #0: loss = 0.8922 (* 1 = 0.8922 loss)
I0218 17:39:08.129843 26599 sgd_solver.cpp:106] Iteration 5800, lr = 0.001
I0218 17:39:11.277060 26599 solver.cpp:341] Iteration 6000, Testing net (#0)
I0218 17:39:11.806078 26599 solver.cpp:409]     Test net output #0: accuracy = 0.6369
I0218 17:39:11.806120 26599 solver.cpp:409]     Test net output #1: loss = 1.01547 (* 1 = 1.01547 loss)
I0218 17:39:11.811552 26599 solver.cpp:237] Iteration 6000, loss = 0.886451
I0218 17:39:11.811589 26599 solver.cpp:253]     Train net output #0: loss = 0.886451 (* 1 = 0.886451 loss)
I0218 17:39:11.811600 26599 sgd_solver.cpp:106] Iteration 6000, lr = 0.001
I0218 17:39:14.994587 26599 solver.cpp:237] Iteration 6200, loss = 0.956443
I0218 17:39:14.994650 26599 solver.cpp:253]     Train net output #0: loss = 0.956443 (* 1 = 0.956443 loss)
I0218 17:39:14.994663 26599 sgd_solver.cpp:106] Iteration 6200, lr = 0.001
I0218 17:39:18.204521 26599 solver.cpp:237] Iteration 6400, loss = 0.828077
I0218 17:39:18.204567 26599 solver.cpp:253]     Train net output #0: loss = 0.828077 (* 1 = 0.828077 loss)
I0218 17:39:18.204579 26599 sgd_solver.cpp:106] Iteration 6400, lr = 0.001
I0218 17:39:21.428000 26599 solver.cpp:237] Iteration 6600, loss = 0.801235
I0218 17:39:21.428048 26599 solver.cpp:253]     Train net output #0: loss = 0.801235 (* 1 = 0.801235 loss)
I0218 17:39:21.428059 26599 sgd_solver.cpp:106] Iteration 6600, lr = 0.001
I0218 17:39:24.610268 26599 solver.cpp:237] Iteration 6800, loss = 0.804455
I0218 17:39:24.610313 26599 solver.cpp:253]     Train net output #0: loss = 0.804455 (* 1 = 0.804455 loss)
I0218 17:39:24.610323 26599 sgd_solver.cpp:106] Iteration 6800, lr = 0.001
I0218 17:39:27.755059 26599 solver.cpp:341] Iteration 7000, Testing net (#0)
I0218 17:39:28.279253 26599 solver.cpp:409]     Test net output #0: accuracy = 0.663
I0218 17:39:28.279300 26599 solver.cpp:409]     Test net output #1: loss = 0.951555 (* 1 = 0.951555 loss)
I0218 17:39:28.284370 26599 solver.cpp:237] Iteration 7000, loss = 0.799213
I0218 17:39:28.284405 26599 solver.cpp:253]     Train net output #0: loss = 0.799213 (* 1 = 0.799213 loss)
I0218 17:39:28.284417 26599 sgd_solver.cpp:106] Iteration 7000, lr = 0.001
I0218 17:39:31.499214 26599 solver.cpp:237] Iteration 7200, loss = 0.87968
I0218 17:39:31.499260 26599 solver.cpp:253]     Train net output #0: loss = 0.87968 (* 1 = 0.87968 loss)
I0218 17:39:31.499271 26599 sgd_solver.cpp:106] Iteration 7200, lr = 0.001
I0218 17:39:34.715832 26599 solver.cpp:237] Iteration 7400, loss = 0.779033
I0218 17:39:34.715875 26599 solver.cpp:253]     Train net output #0: loss = 0.779033 (* 1 = 0.779033 loss)
I0218 17:39:34.715886 26599 sgd_solver.cpp:106] Iteration 7400, lr = 0.001
I0218 17:39:37.919673 26599 solver.cpp:237] Iteration 7600, loss = 0.744466
I0218 17:39:37.919734 26599 solver.cpp:253]     Train net output #0: loss = 0.744466 (* 1 = 0.744466 loss)
I0218 17:39:37.919745 26599 sgd_solver.cpp:106] Iteration 7600, lr = 0.001
I0218 17:39:41.107170 26599 solver.cpp:237] Iteration 7800, loss = 0.755631
I0218 17:39:41.107254 26599 solver.cpp:253]     Train net output #0: loss = 0.755631 (* 1 = 0.755631 loss)
I0218 17:39:41.107265 26599 sgd_solver.cpp:106] Iteration 7800, lr = 0.001
I0218 17:39:44.270786 26599 solver.cpp:341] Iteration 8000, Testing net (#0)
I0218 17:39:44.819186 26599 solver.cpp:409]     Test net output #0: accuracy = 0.6845
I0218 17:39:44.819228 26599 solver.cpp:409]     Test net output #1: loss = 0.895651 (* 1 = 0.895651 loss)
I0218 17:39:44.824573 26599 solver.cpp:237] Iteration 8000, loss = 0.727456
I0218 17:39:44.824623 26599 solver.cpp:253]     Train net output #0: loss = 0.727456 (* 1 = 0.727456 loss)
I0218 17:39:44.824635 26599 sgd_solver.cpp:106] Iteration 8000, lr = 0.001
I0218 17:39:47.970538 26599 solver.cpp:237] Iteration 8200, loss = 0.811211
I0218 17:39:47.970583 26599 solver.cpp:253]     Train net output #0: loss = 0.811211 (* 1 = 0.811211 loss)
I0218 17:39:47.970594 26599 sgd_solver.cpp:106] Iteration 8200, lr = 0.001
I0218 17:39:51.125115 26599 solver.cpp:237] Iteration 8400, loss = 0.760667
I0218 17:39:51.125159 26599 solver.cpp:253]     Train net output #0: loss = 0.760667 (* 1 = 0.760667 loss)
I0218 17:39:51.125169 26599 sgd_solver.cpp:106] Iteration 8400, lr = 0.001
I0218 17:39:54.292456 26599 solver.cpp:237] Iteration 8600, loss = 0.7062
I0218 17:39:54.292501 26599 solver.cpp:253]     Train net output #0: loss = 0.7062 (* 1 = 0.7062 loss)
I0218 17:39:54.292512 26599 sgd_solver.cpp:106] Iteration 8600, lr = 0.001
I0218 17:39:57.462467 26599 solver.cpp:237] Iteration 8800, loss = 0.705156
I0218 17:39:57.462517 26599 solver.cpp:253]     Train net output #0: loss = 0.705156 (* 1 = 0.705156 loss)
I0218 17:39:57.462527 26599 sgd_solver.cpp:106] Iteration 8800, lr = 0.001
I0218 17:40:00.592718 26599 solver.cpp:341] Iteration 9000, Testing net (#0)
I0218 17:40:01.125280 26599 solver.cpp:409]     Test net output #0: accuracy = 0.701
I0218 17:40:01.125324 26599 solver.cpp:409]     Test net output #1: loss = 0.858251 (* 1 = 0.858251 loss)
I0218 17:40:01.131048 26599 solver.cpp:237] Iteration 9000, loss = 0.685833
I0218 17:40:01.131077 26599 solver.cpp:253]     Train net output #0: loss = 0.685833 (* 1 = 0.685833 loss)
I0218 17:40:01.131088 26599 sgd_solver.cpp:106] Iteration 9000, lr = 0.001
I0218 17:40:04.336206 26599 solver.cpp:237] Iteration 9200, loss = 0.770162
I0218 17:40:04.336259 26599 solver.cpp:253]     Train net output #0: loss = 0.770162 (* 1 = 0.770162 loss)
I0218 17:40:04.336590 26599 sgd_solver.cpp:106] Iteration 9200, lr = 0.001
I0218 17:40:07.545481 26599 solver.cpp:237] Iteration 9400, loss = 0.738608
I0218 17:40:07.545528 26599 solver.cpp:253]     Train net output #0: loss = 0.738608 (* 1 = 0.738608 loss)
I0218 17:40:07.545539 26599 sgd_solver.cpp:106] Iteration 9400, lr = 0.001
I0218 17:40:10.737435 26599 solver.cpp:237] Iteration 9600, loss = 0.67536
I0218 17:40:10.737479 26599 solver.cpp:253]     Train net output #0: loss = 0.67536 (* 1 = 0.67536 loss)
I0218 17:40:10.737491 26599 sgd_solver.cpp:106] Iteration 9600, lr = 0.001
I0218 17:40:13.880139 26599 solver.cpp:237] Iteration 9800, loss = 0.688141
I0218 17:40:13.880240 26599 solver.cpp:253]     Train net output #0: loss = 0.688141 (* 1 = 0.688141 loss)
I0218 17:40:13.880256 26599 sgd_solver.cpp:106] Iteration 9800, lr = 0.001
I0218 17:40:17.021052 26599 solver.cpp:472] Snapshotting to HDF5 file examples/cifar10/cifar10_full_iter_10000.caffemodel.h5
I0218 17:40:17.574177 26599 sgd_solver.cpp:279] Snapshotting solver state to HDF5 file examples/cifar10/cifar10_full_iter_10000.solverstate.h5
I0218 17:40:17.577572 26599 solver.cpp:341] Iteration 10000, Testing net (#0)
I0218 17:40:18.098752 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7183
I0218 17:40:18.098796 26599 solver.cpp:409]     Test net output #1: loss = 0.817121 (* 1 = 0.817121 loss)
I0218 17:40:18.104085 26599 solver.cpp:237] Iteration 10000, loss = 0.651001
I0218 17:40:18.104123 26599 solver.cpp:253]     Train net output #0: loss = 0.651001 (* 1 = 0.651001 loss)
I0218 17:40:18.104135 26599 sgd_solver.cpp:106] Iteration 10000, lr = 0.001
I0218 17:40:21.226577 26599 solver.cpp:237] Iteration 10200, loss = 0.737269
I0218 17:40:21.226624 26599 solver.cpp:253]     Train net output #0: loss = 0.737269 (* 1 = 0.737269 loss)
I0218 17:40:21.226635 26599 sgd_solver.cpp:106] Iteration 10200, lr = 0.001
I0218 17:40:24.378738 26599 solver.cpp:237] Iteration 10400, loss = 0.710636
I0218 17:40:24.378792 26599 solver.cpp:253]     Train net output #0: loss = 0.710636 (* 1 = 0.710636 loss)
I0218 17:40:24.378803 26599 sgd_solver.cpp:106] Iteration 10400, lr = 0.001
I0218 17:40:27.529017 26599 solver.cpp:237] Iteration 10600, loss = 0.640976
I0218 17:40:27.529064 26599 solver.cpp:253]     Train net output #0: loss = 0.640976 (* 1 = 0.640976 loss)
I0218 17:40:27.529075 26599 sgd_solver.cpp:106] Iteration 10600, lr = 0.001
I0218 17:40:30.694388 26599 solver.cpp:237] Iteration 10800, loss = 0.643823
I0218 17:40:30.694435 26599 solver.cpp:253]     Train net output #0: loss = 0.643823 (* 1 = 0.643823 loss)
I0218 17:40:30.694447 26599 sgd_solver.cpp:106] Iteration 10800, lr = 0.001
I0218 17:40:33.878111 26599 solver.cpp:341] Iteration 11000, Testing net (#0)
I0218 17:40:34.425251 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7273
I0218 17:40:34.425295 26599 solver.cpp:409]     Test net output #1: loss = 0.788498 (* 1 = 0.788498 loss)
I0218 17:40:34.431074 26599 solver.cpp:237] Iteration 11000, loss = 0.621803
I0218 17:40:34.431113 26599 solver.cpp:253]     Train net output #0: loss = 0.621803 (* 1 = 0.621803 loss)
I0218 17:40:34.431124 26599 sgd_solver.cpp:106] Iteration 11000, lr = 0.001
I0218 17:40:37.612952 26599 solver.cpp:237] Iteration 11200, loss = 0.701302
I0218 17:40:37.613016 26599 solver.cpp:253]     Train net output #0: loss = 0.701302 (* 1 = 0.701302 loss)
I0218 17:40:37.613030 26599 sgd_solver.cpp:106] Iteration 11200, lr = 0.001
I0218 17:40:40.787154 26599 solver.cpp:237] Iteration 11400, loss = 0.665284
I0218 17:40:40.787204 26599 solver.cpp:253]     Train net output #0: loss = 0.665284 (* 1 = 0.665284 loss)
I0218 17:40:40.787215 26599 sgd_solver.cpp:106] Iteration 11400, lr = 0.001
I0218 17:40:43.959417 26599 solver.cpp:237] Iteration 11600, loss = 0.591554
I0218 17:40:43.959538 26599 solver.cpp:253]     Train net output #0: loss = 0.591554 (* 1 = 0.591554 loss)
I0218 17:40:43.959554 26599 sgd_solver.cpp:106] Iteration 11600, lr = 0.001
I0218 17:40:47.155146 26599 solver.cpp:237] Iteration 11800, loss = 0.576501
I0218 17:40:47.155194 26599 solver.cpp:253]     Train net output #0: loss = 0.576501 (* 1 = 0.576501 loss)
I0218 17:40:47.155205 26599 sgd_solver.cpp:106] Iteration 11800, lr = 0.001
I0218 17:40:50.333811 26599 solver.cpp:341] Iteration 12000, Testing net (#0)
I0218 17:40:50.876354 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7368
I0218 17:40:50.876407 26599 solver.cpp:409]     Test net output #1: loss = 0.765911 (* 1 = 0.765911 loss)
I0218 17:40:50.881825 26599 solver.cpp:237] Iteration 12000, loss = 0.587658
I0218 17:40:50.881872 26599 solver.cpp:253]     Train net output #0: loss = 0.587658 (* 1 = 0.587658 loss)
I0218 17:40:50.881885 26599 sgd_solver.cpp:106] Iteration 12000, lr = 0.001
I0218 17:40:54.072024 26599 solver.cpp:237] Iteration 12200, loss = 0.642246
I0218 17:40:54.072072 26599 solver.cpp:253]     Train net output #0: loss = 0.642246 (* 1 = 0.642246 loss)
I0218 17:40:54.072083 26599 sgd_solver.cpp:106] Iteration 12200, lr = 0.001
I0218 17:40:57.239542 26599 solver.cpp:237] Iteration 12400, loss = 0.620895
I0218 17:40:57.239590 26599 solver.cpp:253]     Train net output #0: loss = 0.620895 (* 1 = 0.620895 loss)
I0218 17:40:57.239603 26599 sgd_solver.cpp:106] Iteration 12400, lr = 0.001
I0218 17:41:00.382900 26599 solver.cpp:237] Iteration 12600, loss = 0.559284
I0218 17:41:00.382982 26599 solver.cpp:253]     Train net output #0: loss = 0.559284 (* 1 = 0.559284 loss)
I0218 17:41:00.382999 26599 sgd_solver.cpp:106] Iteration 12600, lr = 0.001
I0218 17:41:03.574465 26599 solver.cpp:237] Iteration 12800, loss = 0.536712
I0218 17:41:03.574518 26599 solver.cpp:253]     Train net output #0: loss = 0.536712 (* 1 = 0.536712 loss)
I0218 17:41:03.574767 26599 sgd_solver.cpp:106] Iteration 12800, lr = 0.001
I0218 17:41:06.729338 26599 solver.cpp:341] Iteration 13000, Testing net (#0)
I0218 17:41:07.270956 26599 solver.cpp:409]     Test net output #0: accuracy = 0.741
I0218 17:41:07.271001 26599 solver.cpp:409]     Test net output #1: loss = 0.749622 (* 1 = 0.749622 loss)
I0218 17:41:07.276546 26599 solver.cpp:237] Iteration 13000, loss = 0.549644
I0218 17:41:07.276587 26599 solver.cpp:253]     Train net output #0: loss = 0.549644 (* 1 = 0.549644 loss)
I0218 17:41:07.276613 26599 sgd_solver.cpp:106] Iteration 13000, lr = 0.001
I0218 17:41:10.469971 26599 solver.cpp:237] Iteration 13200, loss = 0.606049
I0218 17:41:10.470019 26599 solver.cpp:253]     Train net output #0: loss = 0.606049 (* 1 = 0.606049 loss)
I0218 17:41:10.470031 26599 sgd_solver.cpp:106] Iteration 13200, lr = 0.001
I0218 17:41:13.690325 26599 solver.cpp:237] Iteration 13400, loss = 0.590731
I0218 17:41:13.690374 26599 solver.cpp:253]     Train net output #0: loss = 0.590731 (* 1 = 0.590731 loss)
I0218 17:41:13.690387 26599 sgd_solver.cpp:106] Iteration 13400, lr = 0.001
I0218 17:41:16.897666 26599 solver.cpp:237] Iteration 13600, loss = 0.537113
I0218 17:41:16.897773 26599 solver.cpp:253]     Train net output #0: loss = 0.537113 (* 1 = 0.537113 loss)
I0218 17:41:16.897789 26599 sgd_solver.cpp:106] Iteration 13600, lr = 0.001
I0218 17:41:20.066088 26599 solver.cpp:237] Iteration 13800, loss = 0.508076
I0218 17:41:20.066135 26599 solver.cpp:253]     Train net output #0: loss = 0.508076 (* 1 = 0.508076 loss)
I0218 17:41:20.066148 26599 sgd_solver.cpp:106] Iteration 13800, lr = 0.001
I0218 17:41:23.205669 26599 solver.cpp:341] Iteration 14000, Testing net (#0)
I0218 17:41:23.747859 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7457
I0218 17:41:23.747903 26599 solver.cpp:409]     Test net output #1: loss = 0.741837 (* 1 = 0.741837 loss)
I0218 17:41:23.753036 26599 solver.cpp:237] Iteration 14000, loss = 0.511318
I0218 17:41:23.753070 26599 solver.cpp:253]     Train net output #0: loss = 0.511318 (* 1 = 0.511318 loss)
I0218 17:41:23.753082 26599 sgd_solver.cpp:106] Iteration 14000, lr = 0.001
I0218 17:41:26.952953 26599 solver.cpp:237] Iteration 14200, loss = 0.567015
I0218 17:41:26.953001 26599 solver.cpp:253]     Train net output #0: loss = 0.567015 (* 1 = 0.567015 loss)
I0218 17:41:26.953013 26599 sgd_solver.cpp:106] Iteration 14200, lr = 0.001
I0218 17:41:30.124747 26599 solver.cpp:237] Iteration 14400, loss = 0.557286
I0218 17:41:30.124794 26599 solver.cpp:253]     Train net output #0: loss = 0.557286 (* 1 = 0.557286 loss)
I0218 17:41:30.124805 26599 sgd_solver.cpp:106] Iteration 14400, lr = 0.001
I0218 17:41:33.319869 26599 solver.cpp:237] Iteration 14600, loss = 0.514273
I0218 17:41:33.319916 26599 solver.cpp:253]     Train net output #0: loss = 0.514273 (* 1 = 0.514273 loss)
I0218 17:41:33.319927 26599 sgd_solver.cpp:106] Iteration 14600, lr = 0.001
I0218 17:41:36.519299 26599 solver.cpp:237] Iteration 14800, loss = 0.487181
I0218 17:41:36.519354 26599 solver.cpp:253]     Train net output #0: loss = 0.487181 (* 1 = 0.487181 loss)
I0218 17:41:36.519364 26599 sgd_solver.cpp:106] Iteration 14800, lr = 0.001
I0218 17:41:39.707047 26599 solver.cpp:341] Iteration 15000, Testing net (#0)
I0218 17:41:40.247599 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7479
I0218 17:41:40.247642 26599 solver.cpp:409]     Test net output #1: loss = 0.736629 (* 1 = 0.736629 loss)
I0218 17:41:40.252717 26599 solver.cpp:237] Iteration 15000, loss = 0.497013
I0218 17:41:40.252746 26599 solver.cpp:253]     Train net output #0: loss = 0.497013 (* 1 = 0.497013 loss)
I0218 17:41:40.252754 26599 sgd_solver.cpp:106] Iteration 15000, lr = 0.001
I0218 17:41:43.411141 26599 solver.cpp:237] Iteration 15200, loss = 0.529923
I0218 17:41:43.411187 26599 solver.cpp:253]     Train net output #0: loss = 0.529923 (* 1 = 0.529923 loss)
I0218 17:41:43.411195 26599 sgd_solver.cpp:106] Iteration 15200, lr = 0.001
I0218 17:41:46.579975 26599 solver.cpp:237] Iteration 15400, loss = 0.527142
I0218 17:41:46.580018 26599 solver.cpp:253]     Train net output #0: loss = 0.527142 (* 1 = 0.527142 loss)
I0218 17:41:46.580026 26599 sgd_solver.cpp:106] Iteration 15400, lr = 0.001
I0218 17:41:49.745422 26599 solver.cpp:237] Iteration 15600, loss = 0.491274
I0218 17:41:49.745504 26599 solver.cpp:253]     Train net output #0: loss = 0.491274 (* 1 = 0.491274 loss)
I0218 17:41:49.745514 26599 sgd_solver.cpp:106] Iteration 15600, lr = 0.001
I0218 17:41:52.903560 26599 solver.cpp:237] Iteration 15800, loss = 0.466804
I0218 17:41:52.903605 26599 solver.cpp:253]     Train net output #0: loss = 0.466804 (* 1 = 0.466804 loss)
I0218 17:41:52.903625 26599 sgd_solver.cpp:106] Iteration 15800, lr = 0.001
I0218 17:41:56.073952 26599 solver.cpp:341] Iteration 16000, Testing net (#0)
I0218 17:41:56.614850 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7513
I0218 17:41:56.614894 26599 solver.cpp:409]     Test net output #1: loss = 0.732639 (* 1 = 0.732639 loss)
I0218 17:41:56.620476 26599 solver.cpp:237] Iteration 16000, loss = 0.468328
I0218 17:41:56.620509 26599 solver.cpp:253]     Train net output #0: loss = 0.468328 (* 1 = 0.468328 loss)
I0218 17:41:56.620517 26599 sgd_solver.cpp:106] Iteration 16000, lr = 0.001
I0218 17:41:59.803155 26599 solver.cpp:237] Iteration 16200, loss = 0.504873
I0218 17:41:59.803196 26599 solver.cpp:253]     Train net output #0: loss = 0.504873 (* 1 = 0.504873 loss)
I0218 17:41:59.803205 26599 sgd_solver.cpp:106] Iteration 16200, lr = 0.001
I0218 17:42:02.986850 26599 solver.cpp:237] Iteration 16400, loss = 0.484237
I0218 17:42:02.986892 26599 solver.cpp:253]     Train net output #0: loss = 0.484237 (* 1 = 0.484237 loss)
I0218 17:42:02.986901 26599 sgd_solver.cpp:106] Iteration 16400, lr = 0.001
I0218 17:42:06.178740 26599 solver.cpp:237] Iteration 16600, loss = 0.471867
I0218 17:42:06.178783 26599 solver.cpp:253]     Train net output #0: loss = 0.471867 (* 1 = 0.471867 loss)
I0218 17:42:06.178791 26599 sgd_solver.cpp:106] Iteration 16600, lr = 0.001
I0218 17:42:09.345602 26599 solver.cpp:237] Iteration 16800, loss = 0.44505
I0218 17:42:09.345650 26599 solver.cpp:253]     Train net output #0: loss = 0.44505 (* 1 = 0.44505 loss)
I0218 17:42:09.345657 26599 sgd_solver.cpp:106] Iteration 16800, lr = 0.001
I0218 17:42:12.516258 26599 solver.cpp:341] Iteration 17000, Testing net (#0)
I0218 17:42:13.058711 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7529
I0218 17:42:13.058753 26599 solver.cpp:409]     Test net output #1: loss = 0.725132 (* 1 = 0.725132 loss)
I0218 17:42:13.063763 26599 solver.cpp:237] Iteration 17000, loss = 0.44057
I0218 17:42:13.063794 26599 solver.cpp:253]     Train net output #0: loss = 0.44057 (* 1 = 0.44057 loss)
I0218 17:42:13.063802 26599 sgd_solver.cpp:106] Iteration 17000, lr = 0.001
I0218 17:42:16.236209 26599 solver.cpp:237] Iteration 17200, loss = 0.492509
I0218 17:42:16.236254 26599 solver.cpp:253]     Train net output #0: loss = 0.492509 (* 1 = 0.492509 loss)
I0218 17:42:16.236261 26599 sgd_solver.cpp:106] Iteration 17200, lr = 0.001
I0218 17:42:19.405853 26599 solver.cpp:237] Iteration 17400, loss = 0.460652
I0218 17:42:19.405899 26599 solver.cpp:253]     Train net output #0: loss = 0.460652 (* 1 = 0.460652 loss)
I0218 17:42:19.405908 26599 sgd_solver.cpp:106] Iteration 17400, lr = 0.001
I0218 17:42:22.605461 26599 solver.cpp:237] Iteration 17600, loss = 0.432359
I0218 17:42:22.605562 26599 solver.cpp:253]     Train net output #0: loss = 0.432359 (* 1 = 0.432359 loss)
I0218 17:42:22.605572 26599 sgd_solver.cpp:106] Iteration 17600, lr = 0.001
I0218 17:42:25.741982 26599 solver.cpp:237] Iteration 17800, loss = 0.440483
I0218 17:42:25.742024 26599 solver.cpp:253]     Train net output #0: loss = 0.440483 (* 1 = 0.440483 loss)
I0218 17:42:25.742033 26599 sgd_solver.cpp:106] Iteration 17800, lr = 0.001
I0218 17:42:28.926043 26599 solver.cpp:341] Iteration 18000, Testing net (#0)
I0218 17:42:29.463932 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7566
I0218 17:42:29.463973 26599 solver.cpp:409]     Test net output #1: loss = 0.715554 (* 1 = 0.715554 loss)
I0218 17:42:29.469988 26599 solver.cpp:237] Iteration 18000, loss = 0.40835
I0218 17:42:29.470028 26599 solver.cpp:253]     Train net output #0: loss = 0.40835 (* 1 = 0.40835 loss)
I0218 17:42:29.470036 26599 sgd_solver.cpp:106] Iteration 18000, lr = 0.001
I0218 17:42:32.643368 26599 solver.cpp:237] Iteration 18200, loss = 0.484828
I0218 17:42:32.643412 26599 solver.cpp:253]     Train net output #0: loss = 0.484828 (* 1 = 0.484828 loss)
I0218 17:42:32.643420 26599 sgd_solver.cpp:106] Iteration 18200, lr = 0.001
I0218 17:42:35.830577 26599 solver.cpp:237] Iteration 18400, loss = 0.453647
I0218 17:42:35.830622 26599 solver.cpp:253]     Train net output #0: loss = 0.453647 (* 1 = 0.453647 loss)
I0218 17:42:35.830891 26599 sgd_solver.cpp:106] Iteration 18400, lr = 0.001
I0218 17:42:39.023944 26599 solver.cpp:237] Iteration 18600, loss = 0.417736
I0218 17:42:39.023991 26599 solver.cpp:253]     Train net output #0: loss = 0.417736 (* 1 = 0.417736 loss)
I0218 17:42:39.024003 26599 sgd_solver.cpp:106] Iteration 18600, lr = 0.001
I0218 17:42:42.191601 26599 solver.cpp:237] Iteration 18800, loss = 0.426694
I0218 17:42:42.191648 26599 solver.cpp:253]     Train net output #0: loss = 0.426694 (* 1 = 0.426694 loss)
I0218 17:42:42.191658 26599 sgd_solver.cpp:106] Iteration 18800, lr = 0.001
I0218 17:42:45.369791 26599 solver.cpp:341] Iteration 19000, Testing net (#0)
I0218 17:42:45.902246 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7549
I0218 17:42:45.902292 26599 solver.cpp:409]     Test net output #1: loss = 0.713995 (* 1 = 0.713995 loss)
I0218 17:42:45.907579 26599 solver.cpp:237] Iteration 19000, loss = 0.396738
I0218 17:42:45.907609 26599 solver.cpp:253]     Train net output #0: loss = 0.396738 (* 1 = 0.396738 loss)
I0218 17:42:45.907621 26599 sgd_solver.cpp:106] Iteration 19000, lr = 0.001
I0218 17:42:49.094197 26599 solver.cpp:237] Iteration 19200, loss = 0.46556
I0218 17:42:49.094244 26599 solver.cpp:253]     Train net output #0: loss = 0.46556 (* 1 = 0.46556 loss)
I0218 17:42:49.094254 26599 sgd_solver.cpp:106] Iteration 19200, lr = 0.001
I0218 17:42:52.304045 26599 solver.cpp:237] Iteration 19400, loss = 0.425909
I0218 17:42:52.304092 26599 solver.cpp:253]     Train net output #0: loss = 0.425909 (* 1 = 0.425909 loss)
I0218 17:42:52.304103 26599 sgd_solver.cpp:106] Iteration 19400, lr = 0.001
I0218 17:42:55.492213 26599 solver.cpp:237] Iteration 19600, loss = 0.398844
I0218 17:42:55.492322 26599 solver.cpp:253]     Train net output #0: loss = 0.398844 (* 1 = 0.398844 loss)
I0218 17:42:55.492336 26599 sgd_solver.cpp:106] Iteration 19600, lr = 0.001
I0218 17:42:58.686179 26599 solver.cpp:237] Iteration 19800, loss = 0.413785
I0218 17:42:58.686224 26599 solver.cpp:253]     Train net output #0: loss = 0.413785 (* 1 = 0.413785 loss)
I0218 17:42:58.686235 26599 sgd_solver.cpp:106] Iteration 19800, lr = 0.001
I0218 17:43:01.876355 26599 solver.cpp:472] Snapshotting to HDF5 file examples/cifar10/cifar10_full_iter_20000.caffemodel.h5
I0218 17:43:02.436815 26599 sgd_solver.cpp:279] Snapshotting solver state to HDF5 file examples/cifar10/cifar10_full_iter_20000.solverstate.h5
I0218 17:43:02.439257 26599 solver.cpp:341] Iteration 20000, Testing net (#0)
I0218 17:43:02.839519 26599 blocking_queue.cpp:50] Data layer prefetch queue empty
I0218 17:43:02.974452 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7593
I0218 17:43:02.974503 26599 solver.cpp:409]     Test net output #1: loss = 0.708053 (* 1 = 0.708053 loss)
I0218 17:43:02.979759 26599 solver.cpp:237] Iteration 20000, loss = 0.389399
I0218 17:43:02.979811 26599 solver.cpp:253]     Train net output #0: loss = 0.389399 (* 1 = 0.389399 loss)
I0218 17:43:02.979827 26599 sgd_solver.cpp:106] Iteration 20000, lr = 0.001
I0218 17:43:06.148087 26599 solver.cpp:237] Iteration 20200, loss = 0.447377
I0218 17:43:06.148149 26599 solver.cpp:253]     Train net output #0: loss = 0.447377 (* 1 = 0.447377 loss)
I0218 17:43:06.148162 26599 sgd_solver.cpp:106] Iteration 20200, lr = 0.001
I0218 17:43:09.305454 26599 solver.cpp:237] Iteration 20400, loss = 0.39356
I0218 17:43:09.305502 26599 solver.cpp:253]     Train net output #0: loss = 0.39356 (* 1 = 0.39356 loss)
I0218 17:43:09.305515 26599 sgd_solver.cpp:106] Iteration 20400, lr = 0.001
I0218 17:43:12.501917 26599 solver.cpp:237] Iteration 20600, loss = 0.397893
I0218 17:43:12.501965 26599 solver.cpp:253]     Train net output #0: loss = 0.397893 (* 1 = 0.397893 loss)
I0218 17:43:12.501976 26599 sgd_solver.cpp:106] Iteration 20600, lr = 0.001
I0218 17:43:15.689399 26599 solver.cpp:237] Iteration 20800, loss = 0.388585
I0218 17:43:15.689447 26599 solver.cpp:253]     Train net output #0: loss = 0.388585 (* 1 = 0.388585 loss)
I0218 17:43:15.689481 26599 sgd_solver.cpp:106] Iteration 20800, lr = 0.001
I0218 17:43:18.836659 26599 solver.cpp:341] Iteration 21000, Testing net (#0)
I0218 17:43:19.379418 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7601
I0218 17:43:19.379467 26599 solver.cpp:409]     Test net output #1: loss = 0.707725 (* 1 = 0.707725 loss)
I0218 17:43:19.385319 26599 solver.cpp:237] Iteration 21000, loss = 0.370089
I0218 17:43:19.385354 26599 solver.cpp:253]     Train net output #0: loss = 0.370089 (* 1 = 0.370089 loss)
I0218 17:43:19.385365 26599 sgd_solver.cpp:106] Iteration 21000, lr = 0.001
I0218 17:43:22.558442 26599 solver.cpp:237] Iteration 21200, loss = 0.434466
I0218 17:43:22.558490 26599 solver.cpp:253]     Train net output #0: loss = 0.434466 (* 1 = 0.434466 loss)
I0218 17:43:22.558500 26599 sgd_solver.cpp:106] Iteration 21200, lr = 0.001
I0218 17:43:25.728943 26599 solver.cpp:237] Iteration 21400, loss = 0.382347
I0218 17:43:25.729066 26599 solver.cpp:253]     Train net output #0: loss = 0.382347 (* 1 = 0.382347 loss)
I0218 17:43:25.729081 26599 sgd_solver.cpp:106] Iteration 21400, lr = 0.001
I0218 17:43:28.900277 26599 solver.cpp:237] Iteration 21600, loss = 0.382748
I0218 17:43:28.900326 26599 solver.cpp:253]     Train net output #0: loss = 0.382748 (* 1 = 0.382748 loss)
I0218 17:43:28.900336 26599 sgd_solver.cpp:106] Iteration 21600, lr = 0.001
I0218 17:43:32.070778 26599 solver.cpp:237] Iteration 21800, loss = 0.381906
I0218 17:43:32.070824 26599 solver.cpp:253]     Train net output #0: loss = 0.381906 (* 1 = 0.381906 loss)
I0218 17:43:32.070835 26599 sgd_solver.cpp:106] Iteration 21800, lr = 0.001
I0218 17:43:35.266412 26599 solver.cpp:341] Iteration 22000, Testing net (#0)
I0218 17:43:35.807548 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7563
I0218 17:43:35.807603 26599 solver.cpp:409]     Test net output #1: loss = 0.717434 (* 1 = 0.717434 loss)
I0218 17:43:35.813398 26599 solver.cpp:237] Iteration 22000, loss = 0.340467
I0218 17:43:35.813436 26599 solver.cpp:253]     Train net output #0: loss = 0.340467 (* 1 = 0.340467 loss)
I0218 17:43:35.813447 26599 sgd_solver.cpp:106] Iteration 22000, lr = 0.001
I0218 17:43:39.020241 26599 solver.cpp:237] Iteration 22200, loss = 0.407014
I0218 17:43:39.020287 26599 solver.cpp:253]     Train net output #0: loss = 0.407014 (* 1 = 0.407014 loss)
I0218 17:43:39.020299 26599 sgd_solver.cpp:106] Iteration 22200, lr = 0.001
I0218 17:43:42.205171 26599 solver.cpp:237] Iteration 22400, loss = 0.375841
I0218 17:43:42.205216 26599 solver.cpp:253]     Train net output #0: loss = 0.375841 (* 1 = 0.375841 loss)
I0218 17:43:42.205229 26599 sgd_solver.cpp:106] Iteration 22400, lr = 0.001
I0218 17:43:45.359685 26599 solver.cpp:237] Iteration 22600, loss = 0.368507
I0218 17:43:45.359732 26599 solver.cpp:253]     Train net output #0: loss = 0.368507 (* 1 = 0.368507 loss)
I0218 17:43:45.359745 26599 sgd_solver.cpp:106] Iteration 22600, lr = 0.001
I0218 17:43:48.516860 26599 solver.cpp:237] Iteration 22800, loss = 0.368331
I0218 17:43:48.516922 26599 solver.cpp:253]     Train net output #0: loss = 0.368331 (* 1 = 0.368331 loss)
I0218 17:43:48.516935 26599 sgd_solver.cpp:106] Iteration 22800, lr = 0.001
I0218 17:43:51.675884 26599 solver.cpp:341] Iteration 23000, Testing net (#0)
I0218 17:43:52.212759 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7563
I0218 17:43:52.212806 26599 solver.cpp:409]     Test net output #1: loss = 0.731156 (* 1 = 0.731156 loss)
I0218 17:43:52.218483 26599 solver.cpp:237] Iteration 23000, loss = 0.327558
I0218 17:43:52.218595 26599 solver.cpp:253]     Train net output #0: loss = 0.327558 (* 1 = 0.327558 loss)
I0218 17:43:52.218610 26599 sgd_solver.cpp:106] Iteration 23000, lr = 0.001
I0218 17:43:55.382575 26599 solver.cpp:237] Iteration 23200, loss = 0.384652
I0218 17:43:55.382622 26599 solver.cpp:253]     Train net output #0: loss = 0.384652 (* 1 = 0.384652 loss)
I0218 17:43:55.382632 26599 sgd_solver.cpp:106] Iteration 23200, lr = 0.001
I0218 17:43:58.576640 26599 solver.cpp:237] Iteration 23400, loss = 0.394839
I0218 17:43:58.576756 26599 solver.cpp:253]     Train net output #0: loss = 0.394839 (* 1 = 0.394839 loss)
I0218 17:43:58.576771 26599 sgd_solver.cpp:106] Iteration 23400, lr = 0.001
I0218 17:44:01.757376 26599 solver.cpp:237] Iteration 23600, loss = 0.34119
I0218 17:44:01.757421 26599 solver.cpp:253]     Train net output #0: loss = 0.34119 (* 1 = 0.34119 loss)
I0218 17:44:01.757431 26599 sgd_solver.cpp:106] Iteration 23600, lr = 0.001
I0218 17:44:04.949733 26599 solver.cpp:237] Iteration 23800, loss = 0.355909
I0218 17:44:04.949779 26599 solver.cpp:253]     Train net output #0: loss = 0.355909 (* 1 = 0.355909 loss)
I0218 17:44:04.949790 26599 sgd_solver.cpp:106] Iteration 23800, lr = 0.001
I0218 17:44:08.082813 26599 solver.cpp:341] Iteration 24000, Testing net (#0)
I0218 17:44:08.626638 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7531
I0218 17:44:08.626684 26599 solver.cpp:409]     Test net output #1: loss = 0.745152 (* 1 = 0.745152 loss)
I0218 17:44:08.632741 26599 solver.cpp:237] Iteration 24000, loss = 0.324662
I0218 17:44:08.632788 26599 solver.cpp:253]     Train net output #0: loss = 0.324662 (* 1 = 0.324662 loss)
I0218 17:44:08.632799 26599 sgd_solver.cpp:106] Iteration 24000, lr = 0.001
I0218 17:44:11.811398 26599 solver.cpp:237] Iteration 24200, loss = 0.374814
I0218 17:44:11.811445 26599 solver.cpp:253]     Train net output #0: loss = 0.374814 (* 1 = 0.374814 loss)
I0218 17:44:11.811456 26599 sgd_solver.cpp:106] Iteration 24200, lr = 0.001
I0218 17:44:14.991130 26599 solver.cpp:237] Iteration 24400, loss = 0.390126
I0218 17:44:14.991180 26599 solver.cpp:253]     Train net output #0: loss = 0.390126 (* 1 = 0.390126 loss)
I0218 17:44:14.991191 26599 sgd_solver.cpp:106] Iteration 24400, lr = 0.001
I0218 17:44:18.152243 26599 solver.cpp:237] Iteration 24600, loss = 0.318827
I0218 17:44:18.152292 26599 solver.cpp:253]     Train net output #0: loss = 0.318827 (* 1 = 0.318827 loss)
I0218 17:44:18.152303 26599 sgd_solver.cpp:106] Iteration 24600, lr = 0.001
I0218 17:44:21.344259 26599 solver.cpp:237] Iteration 24800, loss = 0.353786
I0218 17:44:21.344307 26599 solver.cpp:253]     Train net output #0: loss = 0.353786 (* 1 = 0.353786 loss)
I0218 17:44:21.344318 26599 sgd_solver.cpp:106] Iteration 24800, lr = 0.001
I0218 17:44:24.533360 26599 solver.cpp:341] Iteration 25000, Testing net (#0)
I0218 17:44:25.072484 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7529
I0218 17:44:25.072528 26599 solver.cpp:409]     Test net output #1: loss = 0.755785 (* 1 = 0.755785 loss)
I0218 17:44:25.077989 26599 solver.cpp:237] Iteration 25000, loss = 0.310945
I0218 17:44:25.078032 26599 solver.cpp:253]     Train net output #0: loss = 0.310945 (* 1 = 0.310945 loss)
I0218 17:44:25.078044 26599 sgd_solver.cpp:106] Iteration 25000, lr = 0.001
I0218 17:44:28.272449 26599 solver.cpp:237] Iteration 25200, loss = 0.379149
I0218 17:44:28.272495 26599 solver.cpp:253]     Train net output #0: loss = 0.379149 (* 1 = 0.379149 loss)
I0218 17:44:28.272506 26599 sgd_solver.cpp:106] Iteration 25200, lr = 0.001
I0218 17:44:31.417219 26599 solver.cpp:237] Iteration 25400, loss = 0.377684
I0218 17:44:31.417294 26599 solver.cpp:253]     Train net output #0: loss = 0.377684 (* 1 = 0.377684 loss)
I0218 17:44:31.417307 26599 sgd_solver.cpp:106] Iteration 25400, lr = 0.001
I0218 17:44:34.582178 26599 solver.cpp:237] Iteration 25600, loss = 0.296171
I0218 17:44:34.582229 26599 solver.cpp:253]     Train net output #0: loss = 0.296171 (* 1 = 0.296171 loss)
I0218 17:44:34.582240 26599 sgd_solver.cpp:106] Iteration 25600, lr = 0.001
I0218 17:44:37.717109 26599 solver.cpp:237] Iteration 25800, loss = 0.368216
I0218 17:44:37.717156 26599 solver.cpp:253]     Train net output #0: loss = 0.368216 (* 1 = 0.368216 loss)
I0218 17:44:37.717169 26599 sgd_solver.cpp:106] Iteration 25800, lr = 0.001
I0218 17:44:40.878418 26599 solver.cpp:341] Iteration 26000, Testing net (#0)
I0218 17:44:41.414965 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7509
I0218 17:44:41.415010 26599 solver.cpp:409]     Test net output #1: loss = 0.767536 (* 1 = 0.767536 loss)
I0218 17:44:41.420708 26599 solver.cpp:237] Iteration 26000, loss = 0.318899
I0218 17:44:41.420750 26599 solver.cpp:253]     Train net output #0: loss = 0.318899 (* 1 = 0.318899 loss)
I0218 17:44:41.420763 26599 sgd_solver.cpp:106] Iteration 26000, lr = 0.001
I0218 17:44:44.620478 26599 solver.cpp:237] Iteration 26200, loss = 0.365757
I0218 17:44:44.620523 26599 solver.cpp:253]     Train net output #0: loss = 0.365757 (* 1 = 0.365757 loss)
I0218 17:44:44.620530 26599 sgd_solver.cpp:106] Iteration 26200, lr = 0.001
I0218 17:44:47.829475 26599 solver.cpp:237] Iteration 26400, loss = 0.326975
I0218 17:44:47.829520 26599 solver.cpp:253]     Train net output #0: loss = 0.326975 (* 1 = 0.326975 loss)
I0218 17:44:47.829529 26599 sgd_solver.cpp:106] Iteration 26400, lr = 0.001
I0218 17:44:51.038878 26599 solver.cpp:237] Iteration 26600, loss = 0.282682
I0218 17:44:51.038930 26599 solver.cpp:253]     Train net output #0: loss = 0.282682 (* 1 = 0.282682 loss)
I0218 17:44:51.038938 26599 sgd_solver.cpp:106] Iteration 26600, lr = 0.001
I0218 17:44:54.211498 26599 solver.cpp:237] Iteration 26800, loss = 0.342218
I0218 17:44:54.211544 26599 solver.cpp:253]     Train net output #0: loss = 0.342218 (* 1 = 0.342218 loss)
I0218 17:44:54.211552 26599 sgd_solver.cpp:106] Iteration 26800, lr = 0.001
I0218 17:44:57.339402 26599 solver.cpp:341] Iteration 27000, Testing net (#0)
I0218 17:44:57.870988 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7436
I0218 17:44:57.871029 26599 solver.cpp:409]     Test net output #1: loss = 0.794582 (* 1 = 0.794582 loss)
I0218 17:44:57.876281 26599 solver.cpp:237] Iteration 27000, loss = 0.321147
I0218 17:44:57.876313 26599 solver.cpp:253]     Train net output #0: loss = 0.321147 (* 1 = 0.321147 loss)
I0218 17:44:57.876320 26599 sgd_solver.cpp:106] Iteration 27000, lr = 0.001
I0218 17:45:01.007884 26599 solver.cpp:237] Iteration 27200, loss = 0.325989
I0218 17:45:01.007928 26599 solver.cpp:253]     Train net output #0: loss = 0.325989 (* 1 = 0.325989 loss)
I0218 17:45:01.007936 26599 sgd_solver.cpp:106] Iteration 27200, lr = 0.001
I0218 17:45:04.160166 26599 solver.cpp:237] Iteration 27400, loss = 0.277938
I0218 17:45:04.160272 26599 solver.cpp:253]     Train net output #0: loss = 0.277938 (* 1 = 0.277938 loss)
I0218 17:45:04.160284 26599 sgd_solver.cpp:106] Iteration 27400, lr = 0.001
I0218 17:45:07.314121 26599 solver.cpp:237] Iteration 27600, loss = 0.281098
I0218 17:45:07.314169 26599 solver.cpp:253]     Train net output #0: loss = 0.281098 (* 1 = 0.281098 loss)
I0218 17:45:07.314178 26599 sgd_solver.cpp:106] Iteration 27600, lr = 0.001
I0218 17:45:10.465338 26599 solver.cpp:237] Iteration 27800, loss = 0.306613
I0218 17:45:10.465384 26599 solver.cpp:253]     Train net output #0: loss = 0.306613 (* 1 = 0.306613 loss)
I0218 17:45:10.465391 26599 sgd_solver.cpp:106] Iteration 27800, lr = 0.001
I0218 17:45:13.599516 26599 solver.cpp:341] Iteration 28000, Testing net (#0)
I0218 17:45:14.141245 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7379
I0218 17:45:14.141289 26599 solver.cpp:409]     Test net output #1: loss = 0.810203 (* 1 = 0.810203 loss)
I0218 17:45:14.147018 26599 solver.cpp:237] Iteration 28000, loss = 0.326515
I0218 17:45:14.147053 26599 solver.cpp:253]     Train net output #0: loss = 0.326515 (* 1 = 0.326515 loss)
I0218 17:45:14.147059 26599 sgd_solver.cpp:106] Iteration 28000, lr = 0.001
I0218 17:45:17.353360 26599 solver.cpp:237] Iteration 28200, loss = 0.283597
I0218 17:45:17.353406 26599 solver.cpp:253]     Train net output #0: loss = 0.283597 (* 1 = 0.283597 loss)
I0218 17:45:17.353415 26599 sgd_solver.cpp:106] Iteration 28200, lr = 0.001
I0218 17:45:20.475181 26599 solver.cpp:237] Iteration 28400, loss = 0.254784
I0218 17:45:20.475225 26599 solver.cpp:253]     Train net output #0: loss = 0.254784 (* 1 = 0.254784 loss)
I0218 17:45:20.475234 26599 sgd_solver.cpp:106] Iteration 28400, lr = 0.001
I0218 17:45:23.620746 26599 solver.cpp:237] Iteration 28600, loss = 0.291784
I0218 17:45:23.620803 26599 solver.cpp:253]     Train net output #0: loss = 0.291784 (* 1 = 0.291784 loss)
I0218 17:45:23.620812 26599 sgd_solver.cpp:106] Iteration 28600, lr = 0.001
I0218 17:45:26.779963 26599 solver.cpp:237] Iteration 28800, loss = 0.301251
I0218 17:45:26.780014 26599 solver.cpp:253]     Train net output #0: loss = 0.301251 (* 1 = 0.301251 loss)
I0218 17:45:26.780024 26599 sgd_solver.cpp:106] Iteration 28800, lr = 0.001
I0218 17:45:29.952790 26599 solver.cpp:341] Iteration 29000, Testing net (#0)
I0218 17:45:30.496986 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7369
I0218 17:45:30.497028 26599 solver.cpp:409]     Test net output #1: loss = 0.829771 (* 1 = 0.829771 loss)
I0218 17:45:30.502506 26599 solver.cpp:237] Iteration 29000, loss = 0.349265
I0218 17:45:30.502540 26599 solver.cpp:253]     Train net output #0: loss = 0.349265 (* 1 = 0.349265 loss)
I0218 17:45:30.502548 26599 sgd_solver.cpp:106] Iteration 29000, lr = 0.001
I0218 17:45:33.682821 26599 solver.cpp:237] Iteration 29200, loss = 0.268321
I0218 17:45:33.682874 26599 solver.cpp:253]     Train net output #0: loss = 0.268321 (* 1 = 0.268321 loss)
I0218 17:45:33.682883 26599 sgd_solver.cpp:106] Iteration 29200, lr = 0.001
I0218 17:45:36.909508 26599 solver.cpp:237] Iteration 29400, loss = 0.243301
I0218 17:45:36.909627 26599 solver.cpp:253]     Train net output #0: loss = 0.243301 (* 1 = 0.243301 loss)
I0218 17:45:36.909637 26599 sgd_solver.cpp:106] Iteration 29400, lr = 0.001
I0218 17:45:40.123190 26599 solver.cpp:237] Iteration 29600, loss = 0.290128
I0218 17:45:40.123235 26599 solver.cpp:253]     Train net output #0: loss = 0.290128 (* 1 = 0.290128 loss)
I0218 17:45:40.123244 26599 sgd_solver.cpp:106] Iteration 29600, lr = 0.001
I0218 17:45:43.279047 26599 solver.cpp:237] Iteration 29800, loss = 0.295612
I0218 17:45:43.279103 26599 solver.cpp:253]     Train net output #0: loss = 0.295612 (* 1 = 0.295612 loss)
I0218 17:45:43.279111 26599 sgd_solver.cpp:106] Iteration 29800, lr = 0.001
I0218 17:45:46.421725 26599 solver.cpp:472] Snapshotting to HDF5 file examples/cifar10/cifar10_full_iter_30000.caffemodel.h5
I0218 17:45:46.980340 26599 sgd_solver.cpp:279] Snapshotting solver state to HDF5 file examples/cifar10/cifar10_full_iter_30000.solverstate.h5
I0218 17:45:46.982734 26599 solver.cpp:341] Iteration 30000, Testing net (#0)
I0218 17:45:47.516103 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7346
I0218 17:45:47.516145 26599 solver.cpp:409]     Test net output #1: loss = 0.834624 (* 1 = 0.834624 loss)
I0218 17:45:47.521919 26599 solver.cpp:237] Iteration 30000, loss = 0.350712
I0218 17:45:47.521962 26599 solver.cpp:253]     Train net output #0: loss = 0.350712 (* 1 = 0.350712 loss)
I0218 17:45:47.521970 26599 sgd_solver.cpp:106] Iteration 30000, lr = 0.001
I0218 17:45:50.724701 26599 solver.cpp:237] Iteration 30200, loss = 0.258246
I0218 17:45:50.724751 26599 solver.cpp:253]     Train net output #0: loss = 0.258246 (* 1 = 0.258246 loss)
I0218 17:45:50.724759 26599 sgd_solver.cpp:106] Iteration 30200, lr = 0.001
I0218 17:45:53.930544 26599 solver.cpp:237] Iteration 30400, loss = 0.218961
I0218 17:45:53.930583 26599 solver.cpp:253]     Train net output #0: loss = 0.218961 (* 1 = 0.218961 loss)
I0218 17:45:53.930590 26599 sgd_solver.cpp:106] Iteration 30400, lr = 0.001
I0218 17:45:57.137653 26599 solver.cpp:237] Iteration 30600, loss = 0.281538
I0218 17:45:57.137699 26599 solver.cpp:253]     Train net output #0: loss = 0.281538 (* 1 = 0.281538 loss)
I0218 17:45:57.137707 26599 sgd_solver.cpp:106] Iteration 30600, lr = 0.001
I0218 17:46:00.341161 26599 solver.cpp:237] Iteration 30800, loss = 0.26923
I0218 17:46:00.341208 26599 solver.cpp:253]     Train net output #0: loss = 0.26923 (* 1 = 0.26923 loss)
I0218 17:46:00.341217 26599 sgd_solver.cpp:106] Iteration 30800, lr = 0.001
I0218 17:46:03.477401 26599 solver.cpp:341] Iteration 31000, Testing net (#0)
I0218 17:46:04.020210 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7294
I0218 17:46:04.020252 26599 solver.cpp:409]     Test net output #1: loss = 0.864393 (* 1 = 0.864393 loss)
I0218 17:46:04.025657 26599 solver.cpp:237] Iteration 31000, loss = 0.34302
I0218 17:46:04.025682 26599 solver.cpp:253]     Train net output #0: loss = 0.34302 (* 1 = 0.34302 loss)
I0218 17:46:04.025691 26599 sgd_solver.cpp:106] Iteration 31000, lr = 0.001
I0218 17:46:07.176713 26599 solver.cpp:237] Iteration 31200, loss = 0.261563
I0218 17:46:07.177235 26599 solver.cpp:253]     Train net output #0: loss = 0.261563 (* 1 = 0.261563 loss)
I0218 17:46:07.177247 26599 sgd_solver.cpp:106] Iteration 31200, lr = 0.001
I0218 17:46:10.359277 26599 solver.cpp:237] Iteration 31400, loss = 0.209319
I0218 17:46:10.359321 26599 solver.cpp:253]     Train net output #0: loss = 0.209319 (* 1 = 0.209319 loss)
I0218 17:46:10.359329 26599 sgd_solver.cpp:106] Iteration 31400, lr = 0.001
I0218 17:46:13.547197 26599 solver.cpp:237] Iteration 31600, loss = 0.259666
I0218 17:46:13.547241 26599 solver.cpp:253]     Train net output #0: loss = 0.259666 (* 1 = 0.259666 loss)
I0218 17:46:13.547250 26599 sgd_solver.cpp:106] Iteration 31600, lr = 0.001
I0218 17:46:16.729094 26599 solver.cpp:237] Iteration 31800, loss = 0.253458
I0218 17:46:16.729159 26599 solver.cpp:253]     Train net output #0: loss = 0.253458 (* 1 = 0.253458 loss)
I0218 17:46:16.729421 26599 sgd_solver.cpp:106] Iteration 31800, lr = 0.001
I0218 17:46:19.901082 26599 solver.cpp:341] Iteration 32000, Testing net (#0)
I0218 17:46:20.441968 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7242
I0218 17:46:20.442013 26599 solver.cpp:409]     Test net output #1: loss = 0.883181 (* 1 = 0.883181 loss)
I0218 17:46:20.447742 26599 solver.cpp:237] Iteration 32000, loss = 0.323562
I0218 17:46:20.447779 26599 solver.cpp:253]     Train net output #0: loss = 0.323562 (* 1 = 0.323562 loss)
I0218 17:46:20.447788 26599 sgd_solver.cpp:106] Iteration 32000, lr = 0.001
I0218 17:46:23.612920 26599 solver.cpp:237] Iteration 32200, loss = 0.260562
I0218 17:46:23.612975 26599 solver.cpp:253]     Train net output #0: loss = 0.260562 (* 1 = 0.260562 loss)
I0218 17:46:23.612983 26599 sgd_solver.cpp:106] Iteration 32200, lr = 0.001
I0218 17:46:26.749451 26599 solver.cpp:237] Iteration 32400, loss = 0.213589
I0218 17:46:26.749501 26599 solver.cpp:253]     Train net output #0: loss = 0.213589 (* 1 = 0.213589 loss)
I0218 17:46:26.749510 26599 sgd_solver.cpp:106] Iteration 32400, lr = 0.001
I0218 17:46:29.884569 26599 solver.cpp:237] Iteration 32600, loss = 0.257888
I0218 17:46:29.884614 26599 solver.cpp:253]     Train net output #0: loss = 0.257888 (* 1 = 0.257888 loss)
I0218 17:46:29.884621 26599 sgd_solver.cpp:106] Iteration 32600, lr = 0.001
I0218 17:46:33.078721 26599 solver.cpp:237] Iteration 32800, loss = 0.252353
I0218 17:46:33.078765 26599 solver.cpp:253]     Train net output #0: loss = 0.252354 (* 1 = 0.252354 loss)
I0218 17:46:33.078773 26599 sgd_solver.cpp:106] Iteration 32800, lr = 0.001
I0218 17:46:36.239828 26599 solver.cpp:341] Iteration 33000, Testing net (#0)
I0218 17:46:36.772984 26599 solver.cpp:409]     Test net output #0: accuracy = 0.725
I0218 17:46:36.773027 26599 solver.cpp:409]     Test net output #1: loss = 0.883726 (* 1 = 0.883726 loss)
I0218 17:46:36.778287 26599 solver.cpp:237] Iteration 33000, loss = 0.304428
I0218 17:46:36.778321 26599 solver.cpp:253]     Train net output #0: loss = 0.304428 (* 1 = 0.304428 loss)
I0218 17:46:36.778328 26599 sgd_solver.cpp:106] Iteration 33000, lr = 0.001
I0218 17:46:39.939635 26599 solver.cpp:237] Iteration 33200, loss = 0.247663
I0218 17:46:39.939714 26599 solver.cpp:253]     Train net output #0: loss = 0.247663 (* 1 = 0.247663 loss)
I0218 17:46:39.939723 26599 sgd_solver.cpp:106] Iteration 33200, lr = 0.001
I0218 17:46:43.133973 26599 solver.cpp:237] Iteration 33400, loss = 0.199634
I0218 17:46:43.134016 26599 solver.cpp:253]     Train net output #0: loss = 0.199634 (* 1 = 0.199634 loss)
I0218 17:46:43.134024 26599 sgd_solver.cpp:106] Iteration 33400, lr = 0.001
I0218 17:46:46.341951 26599 solver.cpp:237] Iteration 33600, loss = 0.258258
I0218 17:46:46.342010 26599 solver.cpp:253]     Train net output #0: loss = 0.258258 (* 1 = 0.258258 loss)
I0218 17:46:46.342038 26599 sgd_solver.cpp:106] Iteration 33600, lr = 0.001
I0218 17:46:49.503984 26599 solver.cpp:237] Iteration 33800, loss = 0.246951
I0218 17:46:49.504048 26599 solver.cpp:253]     Train net output #0: loss = 0.246951 (* 1 = 0.246951 loss)
I0218 17:46:49.504060 26599 sgd_solver.cpp:106] Iteration 33800, lr = 0.001
I0218 17:46:52.627768 26599 solver.cpp:341] Iteration 34000, Testing net (#0)
I0218 17:46:53.150468 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7247
I0218 17:46:53.150521 26599 solver.cpp:409]     Test net output #1: loss = 0.886219 (* 1 = 0.886219 loss)
I0218 17:46:53.155799 26599 solver.cpp:237] Iteration 34000, loss = 0.28326
I0218 17:46:53.155854 26599 solver.cpp:253]     Train net output #0: loss = 0.28326 (* 1 = 0.28326 loss)
I0218 17:46:53.155874 26599 sgd_solver.cpp:106] Iteration 34000, lr = 0.001
I0218 17:46:56.366430 26599 solver.cpp:237] Iteration 34200, loss = 0.24438
I0218 17:46:56.366472 26599 solver.cpp:253]     Train net output #0: loss = 0.24438 (* 1 = 0.24438 loss)
I0218 17:46:56.366479 26599 sgd_solver.cpp:106] Iteration 34200, lr = 0.001
I0218 17:46:59.548717 26599 solver.cpp:237] Iteration 34400, loss = 0.202266
I0218 17:46:59.548761 26599 solver.cpp:253]     Train net output #0: loss = 0.202266 (* 1 = 0.202266 loss)
I0218 17:46:59.548771 26599 sgd_solver.cpp:106] Iteration 34400, lr = 0.001
I0218 17:47:02.762275 26599 solver.cpp:237] Iteration 34600, loss = 0.252029
I0218 17:47:02.762317 26599 solver.cpp:253]     Train net output #0: loss = 0.252029 (* 1 = 0.252029 loss)
I0218 17:47:02.762325 26599 sgd_solver.cpp:106] Iteration 34600, lr = 0.001
I0218 17:47:05.948222 26599 solver.cpp:237] Iteration 34800, loss = 0.266446
I0218 17:47:05.948266 26599 solver.cpp:253]     Train net output #0: loss = 0.266446 (* 1 = 0.266446 loss)
I0218 17:47:05.948273 26599 sgd_solver.cpp:106] Iteration 34800, lr = 0.001
I0218 17:47:09.126772 26599 solver.cpp:341] Iteration 35000, Testing net (#0)
I0218 17:47:09.669517 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7223
I0218 17:47:09.669558 26599 solver.cpp:409]     Test net output #1: loss = 0.905768 (* 1 = 0.905768 loss)
I0218 17:47:09.674667 26599 solver.cpp:237] Iteration 35000, loss = 0.288125
I0218 17:47:09.674697 26599 solver.cpp:253]     Train net output #0: loss = 0.288125 (* 1 = 0.288125 loss)
I0218 17:47:09.674705 26599 sgd_solver.cpp:106] Iteration 35000, lr = 0.001
I0218 17:47:12.807426 26599 solver.cpp:237] Iteration 35200, loss = 0.226161
I0218 17:47:12.807529 26599 solver.cpp:253]     Train net output #0: loss = 0.226161 (* 1 = 0.226161 loss)
I0218 17:47:12.807539 26599 sgd_solver.cpp:106] Iteration 35200, lr = 0.001
I0218 17:47:15.957648 26599 solver.cpp:237] Iteration 35400, loss = 0.188193
I0218 17:47:15.957692 26599 solver.cpp:253]     Train net output #0: loss = 0.188193 (* 1 = 0.188193 loss)
I0218 17:47:15.957700 26599 sgd_solver.cpp:106] Iteration 35400, lr = 0.001
I0218 17:47:19.112146 26599 solver.cpp:237] Iteration 35600, loss = 0.265644
I0218 17:47:19.112190 26599 solver.cpp:253]     Train net output #0: loss = 0.265645 (* 1 = 0.265645 loss)
I0218 17:47:19.112197 26599 sgd_solver.cpp:106] Iteration 35600, lr = 0.001
I0218 17:47:22.296784 26599 solver.cpp:237] Iteration 35800, loss = 0.272506
I0218 17:47:22.296830 26599 solver.cpp:253]     Train net output #0: loss = 0.272507 (* 1 = 0.272507 loss)
I0218 17:47:22.296838 26599 sgd_solver.cpp:106] Iteration 35800, lr = 0.001
I0218 17:47:25.461557 26599 solver.cpp:341] Iteration 36000, Testing net (#0)
I0218 17:47:26.016855 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7183
I0218 17:47:26.016906 26599 solver.cpp:409]     Test net output #1: loss = 0.92808 (* 1 = 0.92808 loss)
I0218 17:47:26.022364 26599 solver.cpp:237] Iteration 36000, loss = 0.300882
I0218 17:47:26.022397 26599 solver.cpp:253]     Train net output #0: loss = 0.300883 (* 1 = 0.300883 loss)
I0218 17:47:26.022405 26599 sgd_solver.cpp:106] Iteration 36000, lr = 0.001
I0218 17:47:29.244593 26599 solver.cpp:237] Iteration 36200, loss = 0.214883
I0218 17:47:29.244648 26599 solver.cpp:253]     Train net output #0: loss = 0.214883 (* 1 = 0.214883 loss)
I0218 17:47:29.244657 26599 sgd_solver.cpp:106] Iteration 36200, lr = 0.001
I0218 17:47:32.448297 26599 solver.cpp:237] Iteration 36400, loss = 0.200937
I0218 17:47:32.448345 26599 solver.cpp:253]     Train net output #0: loss = 0.200937 (* 1 = 0.200937 loss)
I0218 17:47:32.448354 26599 sgd_solver.cpp:106] Iteration 36400, lr = 0.001
I0218 17:47:35.608201 26599 solver.cpp:237] Iteration 36600, loss = 0.280297
I0218 17:47:35.608247 26599 solver.cpp:253]     Train net output #0: loss = 0.280297 (* 1 = 0.280297 loss)
I0218 17:47:35.608254 26599 sgd_solver.cpp:106] Iteration 36600, lr = 0.001
I0218 17:47:38.782546 26599 solver.cpp:237] Iteration 36800, loss = 0.25067
I0218 17:47:38.782590 26599 solver.cpp:253]     Train net output #0: loss = 0.25067 (* 1 = 0.25067 loss)
I0218 17:47:38.782598 26599 sgd_solver.cpp:106] Iteration 36800, lr = 0.001
I0218 17:47:41.980579 26599 solver.cpp:341] Iteration 37000, Testing net (#0)
I0218 17:47:42.532073 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7251
I0218 17:47:42.532114 26599 solver.cpp:409]     Test net output #1: loss = 0.908884 (* 1 = 0.908884 loss)
I0218 17:47:42.537516 26599 solver.cpp:237] Iteration 37000, loss = 0.278626
I0218 17:47:42.537552 26599 solver.cpp:253]     Train net output #0: loss = 0.278626 (* 1 = 0.278626 loss)
I0218 17:47:42.537560 26599 sgd_solver.cpp:106] Iteration 37000, lr = 0.001
I0218 17:47:45.719779 26599 solver.cpp:237] Iteration 37200, loss = 0.182691
I0218 17:47:45.719899 26599 solver.cpp:253]     Train net output #0: loss = 0.182691 (* 1 = 0.182691 loss)
I0218 17:47:45.719909 26599 sgd_solver.cpp:106] Iteration 37200, lr = 0.001
I0218 17:47:48.936899 26599 solver.cpp:237] Iteration 37400, loss = 0.245565
I0218 17:47:48.936944 26599 solver.cpp:253]     Train net output #0: loss = 0.245565 (* 1 = 0.245565 loss)
I0218 17:47:48.936951 26599 sgd_solver.cpp:106] Iteration 37400, lr = 0.001
I0218 17:47:52.154734 26599 solver.cpp:237] Iteration 37600, loss = 0.295973
I0218 17:47:52.154778 26599 solver.cpp:253]     Train net output #0: loss = 0.295973 (* 1 = 0.295973 loss)
I0218 17:47:52.154786 26599 sgd_solver.cpp:106] Iteration 37600, lr = 0.001
I0218 17:47:55.316988 26599 solver.cpp:237] Iteration 37800, loss = 0.254851
I0218 17:47:55.317034 26599 solver.cpp:253]     Train net output #0: loss = 0.254851 (* 1 = 0.254851 loss)
I0218 17:47:55.317042 26599 sgd_solver.cpp:106] Iteration 37800, lr = 0.001
I0218 17:47:58.462268 26599 solver.cpp:341] Iteration 38000, Testing net (#0)
I0218 17:47:58.995769 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7319
I0218 17:47:58.995815 26599 solver.cpp:409]     Test net output #1: loss = 0.877999 (* 1 = 0.877999 loss)
I0218 17:47:59.001636 26599 solver.cpp:237] Iteration 38000, loss = 0.27728
I0218 17:47:59.001677 26599 solver.cpp:253]     Train net output #0: loss = 0.27728 (* 1 = 0.27728 loss)
I0218 17:47:59.001685 26599 sgd_solver.cpp:106] Iteration 38000, lr = 0.001
I0218 17:48:02.184135 26599 solver.cpp:237] Iteration 38200, loss = 0.175462
I0218 17:48:02.184180 26599 solver.cpp:253]     Train net output #0: loss = 0.175462 (* 1 = 0.175462 loss)
I0218 17:48:02.184187 26599 sgd_solver.cpp:106] Iteration 38200, lr = 0.001
I0218 17:48:05.398221 26599 solver.cpp:237] Iteration 38400, loss = 0.221422
I0218 17:48:05.398267 26599 solver.cpp:253]     Train net output #0: loss = 0.221422 (* 1 = 0.221422 loss)
I0218 17:48:05.398274 26599 sgd_solver.cpp:106] Iteration 38400, lr = 0.001
I0218 17:48:08.610987 26599 solver.cpp:237] Iteration 38600, loss = 0.267296
I0218 17:48:08.611032 26599 solver.cpp:253]     Train net output #0: loss = 0.267296 (* 1 = 0.267296 loss)
I0218 17:48:08.611039 26599 sgd_solver.cpp:106] Iteration 38600, lr = 0.001
I0218 17:48:11.813283 26599 solver.cpp:237] Iteration 38800, loss = 0.246944
I0218 17:48:11.813341 26599 solver.cpp:253]     Train net output #0: loss = 0.246944 (* 1 = 0.246944 loss)
I0218 17:48:11.813350 26599 sgd_solver.cpp:106] Iteration 38800, lr = 0.001
I0218 17:48:14.963107 26599 solver.cpp:341] Iteration 39000, Testing net (#0)
I0218 17:48:15.494491 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7306
I0218 17:48:15.494535 26599 solver.cpp:409]     Test net output #1: loss = 0.882 (* 1 = 0.882 loss)
I0218 17:48:15.500257 26599 solver.cpp:237] Iteration 39000, loss = 0.284174
I0218 17:48:15.500305 26599 solver.cpp:253]     Train net output #0: loss = 0.284174 (* 1 = 0.284174 loss)
I0218 17:48:15.500324 26599 sgd_solver.cpp:106] Iteration 39000, lr = 0.001
I0218 17:48:18.674484 26599 solver.cpp:237] Iteration 39200, loss = 0.184617
I0218 17:48:18.674594 26599 solver.cpp:253]     Train net output #0: loss = 0.184617 (* 1 = 0.184617 loss)
I0218 17:48:18.674604 26599 sgd_solver.cpp:106] Iteration 39200, lr = 0.001
I0218 17:48:21.842581 26599 solver.cpp:237] Iteration 39400, loss = 0.194526
I0218 17:48:21.842627 26599 solver.cpp:253]     Train net output #0: loss = 0.194526 (* 1 = 0.194526 loss)
I0218 17:48:21.842634 26599 sgd_solver.cpp:106] Iteration 39400, lr = 0.001
I0218 17:48:24.993605 26599 solver.cpp:237] Iteration 39600, loss = 0.241684
I0218 17:48:24.993649 26599 solver.cpp:253]     Train net output #0: loss = 0.241684 (* 1 = 0.241684 loss)
I0218 17:48:24.993656 26599 sgd_solver.cpp:106] Iteration 39600, lr = 0.001
I0218 17:48:28.173717 26599 solver.cpp:237] Iteration 39800, loss = 0.243383
I0218 17:48:28.173761 26599 solver.cpp:253]     Train net output #0: loss = 0.243383 (* 1 = 0.243383 loss)
I0218 17:48:28.173769 26599 sgd_solver.cpp:106] Iteration 39800, lr = 0.001
I0218 17:48:31.336259 26599 solver.cpp:472] Snapshotting to HDF5 file examples/cifar10/cifar10_full_iter_40000.caffemodel.h5
I0218 17:48:31.900396 26599 sgd_solver.cpp:279] Snapshotting solver state to HDF5 file examples/cifar10/cifar10_full_iter_40000.solverstate.h5
I0218 17:48:31.902983 26599 solver.cpp:341] Iteration 40000, Testing net (#0)
I0218 17:48:32.430405 26599 solver.cpp:409]     Test net output #0: accuracy = 0.733
I0218 17:48:32.430445 26599 solver.cpp:409]     Test net output #1: loss = 0.870089 (* 1 = 0.870089 loss)
I0218 17:48:32.435952 26599 solver.cpp:237] Iteration 40000, loss = 0.253701
I0218 17:48:32.435981 26599 solver.cpp:253]     Train net output #0: loss = 0.253701 (* 1 = 0.253701 loss)
I0218 17:48:32.435988 26599 sgd_solver.cpp:106] Iteration 40000, lr = 0.001
I0218 17:48:35.630280 26599 solver.cpp:237] Iteration 40200, loss = 0.186218
I0218 17:48:35.630323 26599 solver.cpp:253]     Train net output #0: loss = 0.186218 (* 1 = 0.186218 loss)
I0218 17:48:35.630331 26599 sgd_solver.cpp:106] Iteration 40200, lr = 0.001
I0218 17:48:38.778935 26599 solver.cpp:237] Iteration 40400, loss = 0.179576
I0218 17:48:38.778981 26599 solver.cpp:253]     Train net output #0: loss = 0.179576 (* 1 = 0.179576 loss)
I0218 17:48:38.778988 26599 sgd_solver.cpp:106] Iteration 40400, lr = 0.001
I0218 17:48:41.905414 26599 solver.cpp:237] Iteration 40600, loss = 0.225086
I0218 17:48:41.905457 26599 solver.cpp:253]     Train net output #0: loss = 0.225086 (* 1 = 0.225086 loss)
I0218 17:48:41.905465 26599 sgd_solver.cpp:106] Iteration 40600, lr = 0.001
I0218 17:48:45.055235 26599 solver.cpp:237] Iteration 40800, loss = 0.227084
I0218 17:48:45.055290 26599 solver.cpp:253]     Train net output #0: loss = 0.227084 (* 1 = 0.227084 loss)
I0218 17:48:45.055297 26599 sgd_solver.cpp:106] Iteration 40800, lr = 0.001
I0218 17:48:48.197651 26599 solver.cpp:341] Iteration 41000, Testing net (#0)
I0218 17:48:48.734251 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7361
I0218 17:48:48.734331 26599 solver.cpp:409]     Test net output #1: loss = 0.866009 (* 1 = 0.866009 loss)
I0218 17:48:48.739925 26599 solver.cpp:237] Iteration 41000, loss = 0.225892
I0218 17:48:48.739955 26599 solver.cpp:253]     Train net output #0: loss = 0.225893 (* 1 = 0.225893 loss)
I0218 17:48:48.739964 26599 sgd_solver.cpp:106] Iteration 41000, lr = 0.001
I0218 17:48:51.902281 26599 solver.cpp:237] Iteration 41200, loss = 0.176413
I0218 17:48:51.902324 26599 solver.cpp:253]     Train net output #0: loss = 0.176413 (* 1 = 0.176413 loss)
I0218 17:48:51.902345 26599 sgd_solver.cpp:106] Iteration 41200, lr = 0.001
I0218 17:48:55.065299 26599 solver.cpp:237] Iteration 41400, loss = 0.180737
I0218 17:48:55.065345 26599 solver.cpp:253]     Train net output #0: loss = 0.180737 (* 1 = 0.180737 loss)
I0218 17:48:55.065351 26599 sgd_solver.cpp:106] Iteration 41400, lr = 0.001
I0218 17:48:58.275486 26599 solver.cpp:237] Iteration 41600, loss = 0.220049
I0218 17:48:58.275529 26599 solver.cpp:253]     Train net output #0: loss = 0.220049 (* 1 = 0.220049 loss)
I0218 17:48:58.275537 26599 sgd_solver.cpp:106] Iteration 41600, lr = 0.001
I0218 17:49:01.443967 26599 solver.cpp:237] Iteration 41800, loss = 0.207173
I0218 17:49:01.444012 26599 solver.cpp:253]     Train net output #0: loss = 0.207173 (* 1 = 0.207173 loss)
I0218 17:49:01.444020 26599 sgd_solver.cpp:106] Iteration 41800, lr = 0.001
I0218 17:49:04.589696 26599 solver.cpp:341] Iteration 42000, Testing net (#0)
I0218 17:49:05.123118 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7394
I0218 17:49:05.123160 26599 solver.cpp:409]     Test net output #1: loss = 0.852748 (* 1 = 0.852748 loss)
I0218 17:49:05.128614 26599 solver.cpp:237] Iteration 42000, loss = 0.211636
I0218 17:49:05.128653 26599 solver.cpp:253]     Train net output #0: loss = 0.211636 (* 1 = 0.211636 loss)
I0218 17:49:05.128660 26599 sgd_solver.cpp:106] Iteration 42000, lr = 0.001
I0218 17:49:08.308007 26599 solver.cpp:237] Iteration 42200, loss = 0.182247
I0218 17:49:08.308050 26599 solver.cpp:253]     Train net output #0: loss = 0.182247 (* 1 = 0.182247 loss)
I0218 17:49:08.308058 26599 sgd_solver.cpp:106] Iteration 42200, lr = 0.001
I0218 17:49:11.496181 26599 solver.cpp:237] Iteration 42400, loss = 0.229815
I0218 17:49:11.496224 26599 solver.cpp:253]     Train net output #0: loss = 0.229815 (* 1 = 0.229815 loss)
I0218 17:49:11.496233 26599 sgd_solver.cpp:106] Iteration 42400, lr = 0.001
I0218 17:49:14.681617 26599 solver.cpp:237] Iteration 42600, loss = 0.241166
I0218 17:49:14.681661 26599 solver.cpp:253]     Train net output #0: loss = 0.241166 (* 1 = 0.241166 loss)
I0218 17:49:14.681669 26599 sgd_solver.cpp:106] Iteration 42600, lr = 0.001
I0218 17:49:17.871654 26599 solver.cpp:237] Iteration 42800, loss = 0.194758
I0218 17:49:17.871696 26599 solver.cpp:253]     Train net output #0: loss = 0.194758 (* 1 = 0.194758 loss)
I0218 17:49:17.871704 26599 sgd_solver.cpp:106] Iteration 42800, lr = 0.001
I0218 17:49:21.049247 26599 solver.cpp:341] Iteration 43000, Testing net (#0)
I0218 17:49:21.587249 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7423
I0218 17:49:21.587291 26599 solver.cpp:409]     Test net output #1: loss = 0.838907 (* 1 = 0.838907 loss)
I0218 17:49:21.592447 26599 solver.cpp:237] Iteration 43000, loss = 0.197412
I0218 17:49:21.592480 26599 solver.cpp:253]     Train net output #0: loss = 0.197412 (* 1 = 0.197412 loss)
I0218 17:49:21.592488 26599 sgd_solver.cpp:106] Iteration 43000, lr = 0.001
I0218 17:49:24.725224 26599 solver.cpp:237] Iteration 43200, loss = 0.187551
I0218 17:49:24.725268 26599 solver.cpp:253]     Train net output #0: loss = 0.187551 (* 1 = 0.187551 loss)
I0218 17:49:24.725276 26599 sgd_solver.cpp:106] Iteration 43200, lr = 0.001
I0218 17:49:27.878511 26599 solver.cpp:237] Iteration 43400, loss = 0.256568
I0218 17:49:27.878566 26599 solver.cpp:253]     Train net output #0: loss = 0.256568 (* 1 = 0.256568 loss)
I0218 17:49:27.878573 26599 sgd_solver.cpp:106] Iteration 43400, lr = 0.001
I0218 17:49:31.034378 26599 solver.cpp:237] Iteration 43600, loss = 0.237512
I0218 17:49:31.034422 26599 solver.cpp:253]     Train net output #0: loss = 0.237512 (* 1 = 0.237512 loss)
I0218 17:49:31.034430 26599 sgd_solver.cpp:106] Iteration 43600, lr = 0.001
I0218 17:49:34.196701 26599 solver.cpp:237] Iteration 43800, loss = 0.204206
I0218 17:49:34.196746 26599 solver.cpp:253]     Train net output #0: loss = 0.204207 (* 1 = 0.204207 loss)
I0218 17:49:34.196754 26599 sgd_solver.cpp:106] Iteration 43800, lr = 0.001
I0218 17:49:37.377823 26599 solver.cpp:341] Iteration 44000, Testing net (#0)
I0218 17:49:37.918454 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7534
I0218 17:49:37.918498 26599 solver.cpp:409]     Test net output #1: loss = 0.802967 (* 1 = 0.802967 loss)
I0218 17:49:37.924120 26599 solver.cpp:237] Iteration 44000, loss = 0.193814
I0218 17:49:37.924156 26599 solver.cpp:253]     Train net output #0: loss = 0.193814 (* 1 = 0.193814 loss)
I0218 17:49:37.924163 26599 sgd_solver.cpp:106] Iteration 44000, lr = 0.001
I0218 17:49:41.086608 26599 solver.cpp:237] Iteration 44200, loss = 0.201227
I0218 17:49:41.086655 26599 solver.cpp:253]     Train net output #0: loss = 0.201227 (* 1 = 0.201227 loss)
I0218 17:49:41.086663 26599 sgd_solver.cpp:106] Iteration 44200, lr = 0.001
I0218 17:49:44.223950 26599 solver.cpp:237] Iteration 44400, loss = 0.260007
I0218 17:49:44.223995 26599 solver.cpp:253]     Train net output #0: loss = 0.260007 (* 1 = 0.260007 loss)
I0218 17:49:44.224004 26599 sgd_solver.cpp:106] Iteration 44400, lr = 0.001
I0218 17:49:47.366585 26599 solver.cpp:237] Iteration 44600, loss = 0.211059
I0218 17:49:47.366629 26599 solver.cpp:253]     Train net output #0: loss = 0.211059 (* 1 = 0.211059 loss)
I0218 17:49:47.366637 26599 sgd_solver.cpp:106] Iteration 44600, lr = 0.001
I0218 17:49:50.519804 26599 solver.cpp:237] Iteration 44800, loss = 0.169316
I0218 17:49:50.519860 26599 solver.cpp:253]     Train net output #0: loss = 0.169316 (* 1 = 0.169316 loss)
I0218 17:49:50.519868 26599 sgd_solver.cpp:106] Iteration 44800, lr = 0.001
I0218 17:49:53.677171 26599 solver.cpp:341] Iteration 45000, Testing net (#0)
I0218 17:49:54.215363 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7559
I0218 17:49:54.215405 26599 solver.cpp:409]     Test net output #1: loss = 0.791845 (* 1 = 0.791845 loss)
I0218 17:49:54.221220 26599 solver.cpp:237] Iteration 45000, loss = 0.19243
I0218 17:49:54.221257 26599 solver.cpp:253]     Train net output #0: loss = 0.19243 (* 1 = 0.19243 loss)
I0218 17:49:54.221264 26599 sgd_solver.cpp:106] Iteration 45000, lr = 0.001
I0218 17:49:57.368991 26599 solver.cpp:237] Iteration 45200, loss = 0.224565
I0218 17:49:57.369035 26599 solver.cpp:253]     Train net output #0: loss = 0.224565 (* 1 = 0.224565 loss)
I0218 17:49:57.369041 26599 sgd_solver.cpp:106] Iteration 45200, lr = 0.001
I0218 17:50:00.539511 26599 solver.cpp:237] Iteration 45400, loss = 0.233933
I0218 17:50:00.539556 26599 solver.cpp:253]     Train net output #0: loss = 0.233933 (* 1 = 0.233933 loss)
I0218 17:50:00.539563 26599 sgd_solver.cpp:106] Iteration 45400, lr = 0.001
I0218 17:50:03.737025 26599 solver.cpp:237] Iteration 45600, loss = 0.181819
I0218 17:50:03.737069 26599 solver.cpp:253]     Train net output #0: loss = 0.181819 (* 1 = 0.181819 loss)
I0218 17:50:03.737077 26599 sgd_solver.cpp:106] Iteration 45600, lr = 0.001
I0218 17:50:06.917969 26599 solver.cpp:237] Iteration 45800, loss = 0.151987
I0218 17:50:06.918015 26599 solver.cpp:253]     Train net output #0: loss = 0.151987 (* 1 = 0.151987 loss)
I0218 17:50:06.918022 26599 sgd_solver.cpp:106] Iteration 45800, lr = 0.001
I0218 17:50:10.099330 26599 solver.cpp:341] Iteration 46000, Testing net (#0)
I0218 17:50:10.637285 26599 solver.cpp:409]     Test net output #0: accuracy = 0.758
I0218 17:50:10.637326 26599 solver.cpp:409]     Test net output #1: loss = 0.788912 (* 1 = 0.788912 loss)
I0218 17:50:10.642918 26599 solver.cpp:237] Iteration 46000, loss = 0.190483
I0218 17:50:10.642951 26599 solver.cpp:253]     Train net output #0: loss = 0.190484 (* 1 = 0.190484 loss)
I0218 17:50:10.642959 26599 sgd_solver.cpp:106] Iteration 46000, lr = 0.001
I0218 17:50:13.794880 26599 solver.cpp:237] Iteration 46200, loss = 0.220663
I0218 17:50:13.794925 26599 solver.cpp:253]     Train net output #0: loss = 0.220663 (* 1 = 0.220663 loss)
I0218 17:50:13.794934 26599 sgd_solver.cpp:106] Iteration 46200, lr = 0.001
I0218 17:50:16.957275 26599 solver.cpp:237] Iteration 46400, loss = 0.231525
I0218 17:50:16.957320 26599 solver.cpp:253]     Train net output #0: loss = 0.231525 (* 1 = 0.231525 loss)
I0218 17:50:16.957329 26599 sgd_solver.cpp:106] Iteration 46400, lr = 0.001
I0218 17:50:20.117311 26599 solver.cpp:237] Iteration 46600, loss = 0.160653
I0218 17:50:20.117353 26599 solver.cpp:253]     Train net output #0: loss = 0.160654 (* 1 = 0.160654 loss)
I0218 17:50:20.117362 26599 sgd_solver.cpp:106] Iteration 46600, lr = 0.001
I0218 17:50:23.282608 26599 solver.cpp:237] Iteration 46800, loss = 0.143989
I0218 17:50:23.282657 26599 solver.cpp:253]     Train net output #0: loss = 0.143989 (* 1 = 0.143989 loss)
I0218 17:50:23.282666 26599 sgd_solver.cpp:106] Iteration 46800, lr = 0.001
I0218 17:50:26.420167 26599 solver.cpp:341] Iteration 47000, Testing net (#0)
I0218 17:50:26.955868 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7569
I0218 17:50:26.955916 26599 solver.cpp:409]     Test net output #1: loss = 0.799174 (* 1 = 0.799174 loss)
I0218 17:50:26.961869 26599 solver.cpp:237] Iteration 47000, loss = 0.185459
I0218 17:50:26.961911 26599 solver.cpp:253]     Train net output #0: loss = 0.185459 (* 1 = 0.185459 loss)
I0218 17:50:26.961918 26599 sgd_solver.cpp:106] Iteration 47000, lr = 0.001
I0218 17:50:30.149708 26599 solver.cpp:237] Iteration 47200, loss = 0.205908
I0218 17:50:30.149751 26599 solver.cpp:253]     Train net output #0: loss = 0.205908 (* 1 = 0.205908 loss)
I0218 17:50:30.149760 26599 sgd_solver.cpp:106] Iteration 47200, lr = 0.001
I0218 17:50:33.340277 26599 solver.cpp:237] Iteration 47400, loss = 0.21834
I0218 17:50:33.340322 26599 solver.cpp:253]     Train net output #0: loss = 0.21834 (* 1 = 0.21834 loss)
I0218 17:50:33.340329 26599 sgd_solver.cpp:106] Iteration 47400, lr = 0.001
I0218 17:50:36.519356 26599 solver.cpp:237] Iteration 47600, loss = 0.135558
I0218 17:50:36.519402 26599 solver.cpp:253]     Train net output #0: loss = 0.135558 (* 1 = 0.135558 loss)
I0218 17:50:36.519415 26599 sgd_solver.cpp:106] Iteration 47600, lr = 0.001
I0218 17:50:39.706310 26599 solver.cpp:237] Iteration 47800, loss = 0.142507
I0218 17:50:39.706356 26599 solver.cpp:253]     Train net output #0: loss = 0.142507 (* 1 = 0.142507 loss)
I0218 17:50:39.706363 26599 sgd_solver.cpp:106] Iteration 47800, lr = 0.001
I0218 17:50:42.835013 26599 solver.cpp:341] Iteration 48000, Testing net (#0)
I0218 17:50:43.366713 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7522
I0218 17:50:43.366757 26599 solver.cpp:409]     Test net output #1: loss = 0.822836 (* 1 = 0.822836 loss)
I0218 17:50:43.372505 26599 solver.cpp:237] Iteration 48000, loss = 0.187101
I0218 17:50:43.372534 26599 solver.cpp:253]     Train net output #0: loss = 0.187101 (* 1 = 0.187101 loss)
I0218 17:50:43.372542 26599 sgd_solver.cpp:106] Iteration 48000, lr = 0.001
I0218 17:50:46.563180 26599 solver.cpp:237] Iteration 48200, loss = 0.185875
I0218 17:50:46.563225 26599 solver.cpp:253]     Train net output #0: loss = 0.185876 (* 1 = 0.185876 loss)
I0218 17:50:46.563233 26599 sgd_solver.cpp:106] Iteration 48200, lr = 0.001
I0218 17:50:49.743140 26599 solver.cpp:237] Iteration 48400, loss = 0.210607
I0218 17:50:49.743183 26599 solver.cpp:253]     Train net output #0: loss = 0.210607 (* 1 = 0.210607 loss)
I0218 17:50:49.743192 26599 sgd_solver.cpp:106] Iteration 48400, lr = 0.001
I0218 17:50:52.915279 26599 solver.cpp:237] Iteration 48600, loss = 0.120684
I0218 17:50:52.915329 26599 solver.cpp:253]     Train net output #0: loss = 0.120685 (* 1 = 0.120685 loss)
I0218 17:50:52.915341 26599 sgd_solver.cpp:106] Iteration 48600, lr = 0.001
I0218 17:50:56.104387 26599 solver.cpp:237] Iteration 48800, loss = 0.150959
I0218 17:50:56.104434 26599 solver.cpp:253]     Train net output #0: loss = 0.150959 (* 1 = 0.150959 loss)
I0218 17:50:56.104441 26599 sgd_solver.cpp:106] Iteration 48800, lr = 0.001
I0218 17:50:59.294672 26599 solver.cpp:341] Iteration 49000, Testing net (#0)
I0218 17:50:59.830752 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7519
I0218 17:50:59.830796 26599 solver.cpp:409]     Test net output #1: loss = 0.830454 (* 1 = 0.830454 loss)
I0218 17:50:59.835997 26599 solver.cpp:237] Iteration 49000, loss = 0.164507
I0218 17:50:59.836035 26599 solver.cpp:253]     Train net output #0: loss = 0.164507 (* 1 = 0.164507 loss)
I0218 17:50:59.836058 26599 sgd_solver.cpp:106] Iteration 49000, lr = 0.001
I0218 17:51:02.995295 26599 solver.cpp:237] Iteration 49200, loss = 0.170617
I0218 17:51:02.995338 26599 solver.cpp:253]     Train net output #0: loss = 0.170617 (* 1 = 0.170617 loss)
I0218 17:51:02.995347 26599 sgd_solver.cpp:106] Iteration 49200, lr = 0.001
I0218 17:51:06.162101 26599 solver.cpp:237] Iteration 49400, loss = 0.185759
I0218 17:51:06.162145 26599 solver.cpp:253]     Train net output #0: loss = 0.185759 (* 1 = 0.185759 loss)
I0218 17:51:06.162153 26599 sgd_solver.cpp:106] Iteration 49400, lr = 0.001
I0218 17:51:09.348978 26599 solver.cpp:237] Iteration 49600, loss = 0.117712
I0218 17:51:09.349025 26599 solver.cpp:253]     Train net output #0: loss = 0.117712 (* 1 = 0.117712 loss)
I0218 17:51:09.349037 26599 sgd_solver.cpp:106] Iteration 49600, lr = 0.001
I0218 17:51:12.514739 26599 solver.cpp:237] Iteration 49800, loss = 0.144625
I0218 17:51:12.514796 26599 solver.cpp:253]     Train net output #0: loss = 0.144625 (* 1 = 0.144625 loss)
I0218 17:51:12.514806 26599 sgd_solver.cpp:106] Iteration 49800, lr = 0.001
I0218 17:51:15.683079 26599 solver.cpp:472] Snapshotting to HDF5 file examples/cifar10/cifar10_full_iter_50000.caffemodel.h5
I0218 17:51:16.242641 26599 sgd_solver.cpp:279] Snapshotting solver state to HDF5 file examples/cifar10/cifar10_full_iter_50000.solverstate.h5
I0218 17:51:16.245295 26599 solver.cpp:341] Iteration 50000, Testing net (#0)
I0218 17:51:16.775490 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7562
I0218 17:51:16.775534 26599 solver.cpp:409]     Test net output #1: loss = 0.821149 (* 1 = 0.821149 loss)
I0218 17:51:16.780931 26599 solver.cpp:237] Iteration 50000, loss = 0.153385
I0218 17:51:16.780969 26599 solver.cpp:253]     Train net output #0: loss = 0.153385 (* 1 = 0.153385 loss)
I0218 17:51:16.780977 26599 sgd_solver.cpp:106] Iteration 50000, lr = 0.001
I0218 17:51:19.985544 26599 solver.cpp:237] Iteration 50200, loss = 0.171397
I0218 17:51:19.985591 26599 solver.cpp:253]     Train net output #0: loss = 0.171397 (* 1 = 0.171397 loss)
I0218 17:51:19.985599 26599 sgd_solver.cpp:106] Iteration 50200, lr = 0.001
I0218 17:51:23.130964 26599 solver.cpp:237] Iteration 50400, loss = 0.17654
I0218 17:51:23.131007 26599 solver.cpp:253]     Train net output #0: loss = 0.17654 (* 1 = 0.17654 loss)
I0218 17:51:23.131016 26599 sgd_solver.cpp:106] Iteration 50400, lr = 0.001
I0218 17:51:26.297263 26599 solver.cpp:237] Iteration 50600, loss = 0.119257
I0218 17:51:26.297308 26599 solver.cpp:253]     Train net output #0: loss = 0.119257 (* 1 = 0.119257 loss)
I0218 17:51:26.297317 26599 sgd_solver.cpp:106] Iteration 50600, lr = 0.001
I0218 17:51:29.466163 26599 solver.cpp:237] Iteration 50800, loss = 0.159872
I0218 17:51:29.466269 26599 solver.cpp:253]     Train net output #0: loss = 0.159872 (* 1 = 0.159872 loss)
I0218 17:51:29.466280 26599 sgd_solver.cpp:106] Iteration 50800, lr = 0.001
I0218 17:51:32.621834 26599 solver.cpp:341] Iteration 51000, Testing net (#0)
I0218 17:51:33.163426 26599 solver.cpp:409]     Test net output #0: accuracy = 0.757
I0218 17:51:33.163470 26599 solver.cpp:409]     Test net output #1: loss = 0.819799 (* 1 = 0.819799 loss)
I0218 17:51:33.168611 26599 solver.cpp:237] Iteration 51000, loss = 0.156713
I0218 17:51:33.168644 26599 solver.cpp:253]     Train net output #0: loss = 0.156713 (* 1 = 0.156713 loss)
I0218 17:51:33.168653 26599 sgd_solver.cpp:106] Iteration 51000, lr = 0.001
I0218 17:51:36.316622 26599 solver.cpp:237] Iteration 51200, loss = 0.158388
I0218 17:51:36.316668 26599 solver.cpp:253]     Train net output #0: loss = 0.158388 (* 1 = 0.158388 loss)
I0218 17:51:36.316675 26599 sgd_solver.cpp:106] Iteration 51200, lr = 0.001
I0218 17:51:39.479837 26599 solver.cpp:237] Iteration 51400, loss = 0.177055
I0218 17:51:39.479884 26599 solver.cpp:253]     Train net output #0: loss = 0.177056 (* 1 = 0.177056 loss)
I0218 17:51:39.479892 26599 sgd_solver.cpp:106] Iteration 51400, lr = 0.001
I0218 17:51:42.642829 26599 solver.cpp:237] Iteration 51600, loss = 0.119637
I0218 17:51:42.642887 26599 solver.cpp:253]     Train net output #0: loss = 0.119637 (* 1 = 0.119637 loss)
I0218 17:51:42.642896 26599 sgd_solver.cpp:106] Iteration 51600, lr = 0.001
I0218 17:51:45.815826 26599 solver.cpp:237] Iteration 51800, loss = 0.163105
I0218 17:51:45.815871 26599 solver.cpp:253]     Train net output #0: loss = 0.163106 (* 1 = 0.163106 loss)
I0218 17:51:45.815879 26599 sgd_solver.cpp:106] Iteration 51800, lr = 0.001
I0218 17:51:49.009145 26599 solver.cpp:341] Iteration 52000, Testing net (#0)
I0218 17:51:49.546051 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7578
I0218 17:51:49.546093 26599 solver.cpp:409]     Test net output #1: loss = 0.81653 (* 1 = 0.81653 loss)
I0218 17:51:49.551849 26599 solver.cpp:237] Iteration 52000, loss = 0.144821
I0218 17:51:49.551884 26599 solver.cpp:253]     Train net output #0: loss = 0.144821 (* 1 = 0.144821 loss)
I0218 17:51:49.551892 26599 sgd_solver.cpp:106] Iteration 52000, lr = 0.001
I0218 17:51:52.719951 26599 solver.cpp:237] Iteration 52200, loss = 0.140351
I0218 17:51:52.720000 26599 solver.cpp:253]     Train net output #0: loss = 0.140351 (* 1 = 0.140351 loss)
I0218 17:51:52.720007 26599 sgd_solver.cpp:106] Iteration 52200, lr = 0.001
I0218 17:51:55.888108 26599 solver.cpp:237] Iteration 52400, loss = 0.163673
I0218 17:51:55.888154 26599 solver.cpp:253]     Train net output #0: loss = 0.163673 (* 1 = 0.163673 loss)
I0218 17:51:55.888164 26599 sgd_solver.cpp:106] Iteration 52400, lr = 0.001
I0218 17:51:59.064090 26599 solver.cpp:237] Iteration 52600, loss = 0.12234
I0218 17:51:59.064134 26599 solver.cpp:253]     Train net output #0: loss = 0.12234 (* 1 = 0.12234 loss)
I0218 17:51:59.064142 26599 sgd_solver.cpp:106] Iteration 52600, lr = 0.001
I0218 17:52:02.253132 26599 solver.cpp:237] Iteration 52800, loss = 0.170942
I0218 17:52:02.253252 26599 solver.cpp:253]     Train net output #0: loss = 0.170942 (* 1 = 0.170942 loss)
I0218 17:52:02.253262 26599 sgd_solver.cpp:106] Iteration 52800, lr = 0.001
I0218 17:52:05.406911 26599 solver.cpp:341] Iteration 53000, Testing net (#0)
I0218 17:52:05.948490 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7564
I0218 17:52:05.948549 26599 solver.cpp:409]     Test net output #1: loss = 0.826506 (* 1 = 0.826506 loss)
I0218 17:52:05.954080 26599 solver.cpp:237] Iteration 53000, loss = 0.136561
I0218 17:52:05.954129 26599 solver.cpp:253]     Train net output #0: loss = 0.136561 (* 1 = 0.136561 loss)
I0218 17:52:05.954136 26599 sgd_solver.cpp:106] Iteration 53000, lr = 0.001
I0218 17:52:09.111407 26599 solver.cpp:237] Iteration 53200, loss = 0.133745
I0218 17:52:09.111465 26599 solver.cpp:253]     Train net output #0: loss = 0.133745 (* 1 = 0.133745 loss)
I0218 17:52:09.111477 26599 sgd_solver.cpp:106] Iteration 53200, lr = 0.001
I0218 17:52:12.270548 26599 solver.cpp:237] Iteration 53400, loss = 0.150082
I0218 17:52:12.270599 26599 solver.cpp:253]     Train net output #0: loss = 0.150082 (* 1 = 0.150082 loss)
I0218 17:52:12.270607 26599 sgd_solver.cpp:106] Iteration 53400, lr = 0.001
I0218 17:52:15.471000 26599 solver.cpp:237] Iteration 53600, loss = 0.136149
I0218 17:52:15.471050 26599 solver.cpp:253]     Train net output #0: loss = 0.13615 (* 1 = 0.13615 loss)
I0218 17:52:15.471057 26599 sgd_solver.cpp:106] Iteration 53600, lr = 0.001
I0218 17:52:18.663311 26599 solver.cpp:237] Iteration 53800, loss = 0.167435
I0218 17:52:18.663359 26599 solver.cpp:253]     Train net output #0: loss = 0.167435 (* 1 = 0.167435 loss)
I0218 17:52:18.663367 26599 sgd_solver.cpp:106] Iteration 53800, lr = 0.001
I0218 17:52:21.816416 26599 solver.cpp:341] Iteration 54000, Testing net (#0)
I0218 17:52:22.350100 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7572
I0218 17:52:22.350149 26599 solver.cpp:409]     Test net output #1: loss = 0.850203 (* 1 = 0.850203 loss)
I0218 17:52:22.355661 26599 solver.cpp:237] Iteration 54000, loss = 0.144876
I0218 17:52:22.355697 26599 solver.cpp:253]     Train net output #0: loss = 0.144876 (* 1 = 0.144876 loss)
I0218 17:52:22.355705 26599 sgd_solver.cpp:106] Iteration 54000, lr = 0.001
I0218 17:52:25.520952 26599 solver.cpp:237] Iteration 54200, loss = 0.122452
I0218 17:52:25.521005 26599 solver.cpp:253]     Train net output #0: loss = 0.122453 (* 1 = 0.122453 loss)
I0218 17:52:25.521335 26599 sgd_solver.cpp:106] Iteration 54200, lr = 0.001
I0218 17:52:28.698829 26599 solver.cpp:237] Iteration 54400, loss = 0.146563
I0218 17:52:28.698875 26599 solver.cpp:253]     Train net output #0: loss = 0.146563 (* 1 = 0.146563 loss)
I0218 17:52:28.698884 26599 sgd_solver.cpp:106] Iteration 54400, lr = 0.001
I0218 17:52:31.861996 26599 solver.cpp:237] Iteration 54600, loss = 0.148025
I0218 17:52:31.862046 26599 solver.cpp:253]     Train net output #0: loss = 0.148025 (* 1 = 0.148025 loss)
I0218 17:52:31.862053 26599 sgd_solver.cpp:106] Iteration 54600, lr = 0.001
I0218 17:52:35.051828 26599 solver.cpp:237] Iteration 54800, loss = 0.148408
I0218 17:52:35.051937 26599 solver.cpp:253]     Train net output #0: loss = 0.148408 (* 1 = 0.148408 loss)
I0218 17:52:35.051947 26599 sgd_solver.cpp:106] Iteration 54800, lr = 0.001
I0218 17:52:38.255225 26599 solver.cpp:341] Iteration 55000, Testing net (#0)
I0218 17:52:38.790627 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7636
I0218 17:52:38.790683 26599 solver.cpp:409]     Test net output #1: loss = 0.817463 (* 1 = 0.817463 loss)
I0218 17:52:38.796411 26599 solver.cpp:237] Iteration 55000, loss = 0.129331
I0218 17:52:38.796461 26599 solver.cpp:253]     Train net output #0: loss = 0.129331 (* 1 = 0.129331 loss)
I0218 17:52:38.796474 26599 sgd_solver.cpp:106] Iteration 55000, lr = 0.001
I0218 17:52:42.008776 26599 solver.cpp:237] Iteration 55200, loss = 0.121666
I0218 17:52:42.008824 26599 solver.cpp:253]     Train net output #0: loss = 0.121666 (* 1 = 0.121666 loss)
I0218 17:52:42.008832 26599 sgd_solver.cpp:106] Iteration 55200, lr = 0.001
I0218 17:52:45.155133 26599 solver.cpp:237] Iteration 55400, loss = 0.14489
I0218 17:52:45.155176 26599 solver.cpp:253]     Train net output #0: loss = 0.14489 (* 1 = 0.14489 loss)
I0218 17:52:45.155184 26599 sgd_solver.cpp:106] Iteration 55400, lr = 0.001
I0218 17:52:48.363442 26599 solver.cpp:237] Iteration 55600, loss = 0.147896
I0218 17:52:48.363490 26599 solver.cpp:253]     Train net output #0: loss = 0.147896 (* 1 = 0.147896 loss)
I0218 17:52:48.363498 26599 sgd_solver.cpp:106] Iteration 55600, lr = 0.001
I0218 17:52:51.530966 26599 solver.cpp:237] Iteration 55800, loss = 0.140302
I0218 17:52:51.531015 26599 solver.cpp:253]     Train net output #0: loss = 0.140302 (* 1 = 0.140302 loss)
I0218 17:52:51.531023 26599 sgd_solver.cpp:106] Iteration 55800, lr = 0.001
I0218 17:52:54.676946 26599 solver.cpp:341] Iteration 56000, Testing net (#0)
I0218 17:52:55.210777 26599 solver.cpp:409]     Test net output #0: accuracy = 0.771
I0218 17:52:55.210820 26599 solver.cpp:409]     Test net output #1: loss = 0.786496 (* 1 = 0.786496 loss)
I0218 17:52:55.216380 26599 solver.cpp:237] Iteration 56000, loss = 0.119905
I0218 17:52:55.216409 26599 solver.cpp:253]     Train net output #0: loss = 0.119905 (* 1 = 0.119905 loss)
I0218 17:52:55.216418 26599 sgd_solver.cpp:106] Iteration 56000, lr = 0.001
I0218 17:52:58.376657 26599 solver.cpp:237] Iteration 56200, loss = 0.118434
I0218 17:52:58.376705 26599 solver.cpp:253]     Train net output #0: loss = 0.118434 (* 1 = 0.118434 loss)
I0218 17:52:58.376713 26599 sgd_solver.cpp:106] Iteration 56200, lr = 0.001
I0218 17:53:01.534679 26599 solver.cpp:237] Iteration 56400, loss = 0.146108
I0218 17:53:01.534725 26599 solver.cpp:253]     Train net output #0: loss = 0.146108 (* 1 = 0.146108 loss)
I0218 17:53:01.534734 26599 sgd_solver.cpp:106] Iteration 56400, lr = 0.001
I0218 17:53:04.685650 26599 solver.cpp:237] Iteration 56600, loss = 0.145535
I0218 17:53:04.685698 26599 solver.cpp:253]     Train net output #0: loss = 0.145535 (* 1 = 0.145535 loss)
I0218 17:53:04.685706 26599 sgd_solver.cpp:106] Iteration 56600, lr = 0.001
I0218 17:53:07.830799 26599 solver.cpp:237] Iteration 56800, loss = 0.133158
I0218 17:53:07.830903 26599 solver.cpp:253]     Train net output #0: loss = 0.133158 (* 1 = 0.133158 loss)
I0218 17:53:07.830917 26599 sgd_solver.cpp:106] Iteration 56800, lr = 0.001
I0218 17:53:10.975461 26599 solver.cpp:341] Iteration 57000, Testing net (#0)
I0218 17:53:11.509215 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7693
I0218 17:53:11.509263 26599 solver.cpp:409]     Test net output #1: loss = 0.783305 (* 1 = 0.783305 loss)
I0218 17:53:11.514678 26599 solver.cpp:237] Iteration 57000, loss = 0.143947
I0218 17:53:11.514721 26599 solver.cpp:253]     Train net output #0: loss = 0.143947 (* 1 = 0.143947 loss)
I0218 17:53:11.514729 26599 sgd_solver.cpp:106] Iteration 57000, lr = 0.001
I0218 17:53:14.669687 26599 solver.cpp:237] Iteration 57200, loss = 0.112901
I0218 17:53:14.669731 26599 solver.cpp:253]     Train net output #0: loss = 0.112901 (* 1 = 0.112901 loss)
I0218 17:53:14.669739 26599 sgd_solver.cpp:106] Iteration 57200, lr = 0.001
I0218 17:53:17.823839 26599 solver.cpp:237] Iteration 57400, loss = 0.146014
I0218 17:53:17.823882 26599 solver.cpp:253]     Train net output #0: loss = 0.146014 (* 1 = 0.146014 loss)
I0218 17:53:17.823890 26599 sgd_solver.cpp:106] Iteration 57400, lr = 0.001
I0218 17:53:20.979113 26599 solver.cpp:237] Iteration 57600, loss = 0.144014
I0218 17:53:20.979159 26599 solver.cpp:253]     Train net output #0: loss = 0.144014 (* 1 = 0.144014 loss)
I0218 17:53:20.979167 26599 sgd_solver.cpp:106] Iteration 57600, lr = 0.001
I0218 17:53:24.139310 26599 solver.cpp:237] Iteration 57800, loss = 0.136619
I0218 17:53:24.139359 26599 solver.cpp:253]     Train net output #0: loss = 0.136619 (* 1 = 0.136619 loss)
I0218 17:53:24.139367 26599 sgd_solver.cpp:106] Iteration 57800, lr = 0.001
I0218 17:53:27.308936 26599 solver.cpp:341] Iteration 58000, Testing net (#0)
I0218 17:53:27.842684 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7636
I0218 17:53:27.842725 26599 solver.cpp:409]     Test net output #1: loss = 0.802378 (* 1 = 0.802378 loss)
I0218 17:53:27.847877 26599 solver.cpp:237] Iteration 58000, loss = 0.159713
I0218 17:53:27.847910 26599 solver.cpp:253]     Train net output #0: loss = 0.159714 (* 1 = 0.159714 loss)
I0218 17:53:27.847918 26599 sgd_solver.cpp:106] Iteration 58000, lr = 0.001
I0218 17:53:31.008605 26599 solver.cpp:237] Iteration 58200, loss = 0.117295
I0218 17:53:31.008652 26599 solver.cpp:253]     Train net output #0: loss = 0.117295 (* 1 = 0.117295 loss)
I0218 17:53:31.008661 26599 sgd_solver.cpp:106] Iteration 58200, lr = 0.001
I0218 17:53:34.147408 26599 solver.cpp:237] Iteration 58400, loss = 0.126665
I0218 17:53:34.147451 26599 solver.cpp:253]     Train net output #0: loss = 0.126665 (* 1 = 0.126665 loss)
I0218 17:53:34.147459 26599 sgd_solver.cpp:106] Iteration 58400, lr = 0.001
I0218 17:53:37.302918 26599 solver.cpp:237] Iteration 58600, loss = 0.153683
I0218 17:53:37.302963 26599 solver.cpp:253]     Train net output #0: loss = 0.153683 (* 1 = 0.153683 loss)
I0218 17:53:37.302970 26599 sgd_solver.cpp:106] Iteration 58600, lr = 0.001
I0218 17:53:40.455996 26599 solver.cpp:237] Iteration 58800, loss = 0.14655
I0218 17:53:40.456074 26599 solver.cpp:253]     Train net output #0: loss = 0.14655 (* 1 = 0.14655 loss)
I0218 17:53:40.456085 26599 sgd_solver.cpp:106] Iteration 58800, lr = 0.001
I0218 17:53:43.607630 26599 solver.cpp:341] Iteration 59000, Testing net (#0)
I0218 17:53:44.143189 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7607
I0218 17:53:44.143234 26599 solver.cpp:409]     Test net output #1: loss = 0.804592 (* 1 = 0.804592 loss)
I0218 17:53:44.148408 26599 solver.cpp:237] Iteration 59000, loss = 0.145122
I0218 17:53:44.148447 26599 solver.cpp:253]     Train net output #0: loss = 0.145122 (* 1 = 0.145122 loss)
I0218 17:53:44.148454 26599 sgd_solver.cpp:106] Iteration 59000, lr = 0.001
I0218 17:53:47.327164 26599 solver.cpp:237] Iteration 59200, loss = 0.118656
I0218 17:53:47.327214 26599 solver.cpp:253]     Train net output #0: loss = 0.118656 (* 1 = 0.118656 loss)
I0218 17:53:47.327222 26599 sgd_solver.cpp:106] Iteration 59200, lr = 0.001
I0218 17:53:50.505964 26599 solver.cpp:237] Iteration 59400, loss = 0.123058
I0218 17:53:50.506018 26599 solver.cpp:253]     Train net output #0: loss = 0.123058 (* 1 = 0.123058 loss)
I0218 17:53:50.506027 26599 sgd_solver.cpp:106] Iteration 59400, lr = 0.001
I0218 17:53:53.692101 26599 solver.cpp:237] Iteration 59600, loss = 0.165481
I0218 17:53:53.692147 26599 solver.cpp:253]     Train net output #0: loss = 0.165481 (* 1 = 0.165481 loss)
I0218 17:53:53.692155 26599 sgd_solver.cpp:106] Iteration 59600, lr = 0.001
I0218 17:53:56.855566 26599 solver.cpp:237] Iteration 59800, loss = 0.172928
I0218 17:53:56.855609 26599 solver.cpp:253]     Train net output #0: loss = 0.172928 (* 1 = 0.172928 loss)
I0218 17:53:56.855618 26599 sgd_solver.cpp:106] Iteration 59800, lr = 0.001
I0218 17:54:00.022753 26599 solver.cpp:472] Snapshotting to HDF5 file examples/cifar10/cifar10_full_iter_60000.caffemodel.h5
I0218 17:54:00.581339 26599 sgd_solver.cpp:279] Snapshotting solver state to HDF5 file examples/cifar10/cifar10_full_iter_60000.solverstate.h5
I0218 17:54:00.589054 26599 solver.cpp:321] Iteration 60000, loss = 0.118236
I0218 17:54:00.589087 26599 solver.cpp:341] Iteration 60000, Testing net (#0)
I0218 17:54:01.112639 26599 solver.cpp:409]     Test net output #0: accuracy = 0.7637
I0218 17:54:01.112690 26599 solver.cpp:409]     Test net output #1: loss = 0.785518 (* 1 = 0.785518 loss)
I0218 17:54:01.112701 26599 solver.cpp:326] Optimization Done.
I0218 17:54:01.112707 26599 caffe.cpp:215] Optimization Done.
I0218 17:54:01.634471 26654 caffe.cpp:184] Using GPUs 0
I0218 17:54:01.859309 26654 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 1000
base_lr: 0.0001
display: 200
max_iter: 65000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.004
snapshot: 5000
snapshot_prefix: "examples/cifar10/cifar10_full"
solver_mode: GPU
device_id: 0
net: "examples/cifar10/cifar10_full_train_test.prototxt"
snapshot_format: HDF5
I0218 17:54:01.859475 26654 solver.cpp:91] Creating training net from net file: examples/cifar10/cifar10_full_train_test.prototxt
I0218 17:54:01.860152 26654 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0218 17:54:01.860190 26654 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0218 17:54:01.860365 26654 net.cpp:49] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TRAIN
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0218 17:54:01.860501 26654 layer_factory.hpp:77] Creating layer cifar
I0218 17:54:01.861173 26654 net.cpp:106] Creating Layer cifar
I0218 17:54:01.861199 26654 net.cpp:411] cifar -> data
I0218 17:54:01.861253 26654 net.cpp:411] cifar -> label
I0218 17:54:01.861289 26654 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0218 17:54:01.861927 26658 db_lmdb.cpp:38] Opened lmdb examples/cifar10/cifar10_train_lmdb
I0218 17:54:01.874368 26654 data_layer.cpp:41] output data size: 100,3,32,32
I0218 17:54:01.877559 26654 net.cpp:150] Setting up cifar
I0218 17:54:01.877604 26654 net.cpp:157] Top shape: 100 3 32 32 (307200)
I0218 17:54:01.877621 26654 net.cpp:157] Top shape: 100 (100)
I0218 17:54:01.877634 26654 net.cpp:165] Memory required for data: 1229200
I0218 17:54:01.877652 26654 layer_factory.hpp:77] Creating layer conv1
I0218 17:54:01.877686 26654 net.cpp:106] Creating Layer conv1
I0218 17:54:01.877702 26654 net.cpp:454] conv1 <- data
I0218 17:54:01.877725 26654 net.cpp:411] conv1 -> conv1
I0218 17:54:02.043107 26654 net.cpp:150] Setting up conv1
I0218 17:54:02.043164 26654 net.cpp:157] Top shape: 100 32 32 32 (3276800)
I0218 17:54:02.043179 26654 net.cpp:165] Memory required for data: 14336400
I0218 17:54:02.043211 26654 layer_factory.hpp:77] Creating layer pool1
I0218 17:54:02.043236 26654 net.cpp:106] Creating Layer pool1
I0218 17:54:02.043249 26654 net.cpp:454] pool1 <- conv1
I0218 17:54:02.043262 26654 net.cpp:411] pool1 -> pool1
I0218 17:54:02.044108 26654 net.cpp:150] Setting up pool1
I0218 17:54:02.044133 26654 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0218 17:54:02.044145 26654 net.cpp:165] Memory required for data: 17613200
I0218 17:54:02.044154 26654 layer_factory.hpp:77] Creating layer relu1
I0218 17:54:02.044167 26654 net.cpp:106] Creating Layer relu1
I0218 17:54:02.044178 26654 net.cpp:454] relu1 <- pool1
I0218 17:54:02.044191 26654 net.cpp:397] relu1 -> pool1 (in-place)
I0218 17:54:02.044965 26654 net.cpp:150] Setting up relu1
I0218 17:54:02.044989 26654 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0218 17:54:02.045001 26654 net.cpp:165] Memory required for data: 20890000
I0218 17:54:02.045011 26654 layer_factory.hpp:77] Creating layer norm1
I0218 17:54:02.045038 26654 net.cpp:106] Creating Layer norm1
I0218 17:54:02.045053 26654 net.cpp:454] norm1 <- pool1
I0218 17:54:02.045065 26654 net.cpp:411] norm1 -> norm1
I0218 17:54:02.046252 26654 net.cpp:150] Setting up norm1
I0218 17:54:02.046279 26654 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0218 17:54:02.046293 26654 net.cpp:165] Memory required for data: 24166800
I0218 17:54:02.046303 26654 layer_factory.hpp:77] Creating layer conv2
I0218 17:54:02.046319 26654 net.cpp:106] Creating Layer conv2
I0218 17:54:02.046344 26654 net.cpp:454] conv2 <- norm1
I0218 17:54:02.046358 26654 net.cpp:411] conv2 -> conv2
I0218 17:54:02.050663 26654 net.cpp:150] Setting up conv2
I0218 17:54:02.050698 26654 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0218 17:54:02.050710 26654 net.cpp:165] Memory required for data: 27443600
I0218 17:54:02.050731 26654 layer_factory.hpp:77] Creating layer relu2
I0218 17:54:02.050751 26654 net.cpp:106] Creating Layer relu2
I0218 17:54:02.050768 26654 net.cpp:454] relu2 <- conv2
I0218 17:54:02.050782 26654 net.cpp:397] relu2 -> conv2 (in-place)
I0218 17:54:02.051663 26654 net.cpp:150] Setting up relu2
I0218 17:54:02.051687 26654 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0218 17:54:02.051697 26654 net.cpp:165] Memory required for data: 30720400
I0218 17:54:02.051707 26654 layer_factory.hpp:77] Creating layer pool2
I0218 17:54:02.051722 26654 net.cpp:106] Creating Layer pool2
I0218 17:54:02.051733 26654 net.cpp:454] pool2 <- conv2
I0218 17:54:02.051748 26654 net.cpp:411] pool2 -> pool2
I0218 17:54:02.052630 26654 net.cpp:150] Setting up pool2
I0218 17:54:02.052655 26654 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0218 17:54:02.052666 26654 net.cpp:165] Memory required for data: 31539600
I0218 17:54:02.052675 26654 layer_factory.hpp:77] Creating layer norm2
I0218 17:54:02.052693 26654 net.cpp:106] Creating Layer norm2
I0218 17:54:02.052709 26654 net.cpp:454] norm2 <- pool2
I0218 17:54:02.052724 26654 net.cpp:411] norm2 -> norm2
I0218 17:54:02.053987 26654 net.cpp:150] Setting up norm2
I0218 17:54:02.054028 26654 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0218 17:54:02.054039 26654 net.cpp:165] Memory required for data: 32358800
I0218 17:54:02.054046 26654 layer_factory.hpp:77] Creating layer conv3
I0218 17:54:02.054064 26654 net.cpp:106] Creating Layer conv3
I0218 17:54:02.054080 26654 net.cpp:454] conv3 <- norm2
I0218 17:54:02.054095 26654 net.cpp:411] conv3 -> conv3
I0218 17:54:02.058910 26654 net.cpp:150] Setting up conv3
I0218 17:54:02.058949 26654 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0218 17:54:02.058961 26654 net.cpp:165] Memory required for data: 33997200
I0218 17:54:02.058980 26654 layer_factory.hpp:77] Creating layer relu3
I0218 17:54:02.059001 26654 net.cpp:106] Creating Layer relu3
I0218 17:54:02.059016 26654 net.cpp:454] relu3 <- conv3
I0218 17:54:02.059036 26654 net.cpp:397] relu3 -> conv3 (in-place)
I0218 17:54:02.059932 26654 net.cpp:150] Setting up relu3
I0218 17:54:02.059957 26654 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0218 17:54:02.059967 26654 net.cpp:165] Memory required for data: 35635600
I0218 17:54:02.059975 26654 layer_factory.hpp:77] Creating layer pool3
I0218 17:54:02.059990 26654 net.cpp:106] Creating Layer pool3
I0218 17:54:02.060003 26654 net.cpp:454] pool3 <- conv3
I0218 17:54:02.060015 26654 net.cpp:411] pool3 -> pool3
I0218 17:54:02.060931 26654 net.cpp:150] Setting up pool3
I0218 17:54:02.060959 26654 net.cpp:157] Top shape: 100 64 4 4 (102400)
I0218 17:54:02.060976 26654 net.cpp:165] Memory required for data: 36045200
I0218 17:54:02.060986 26654 layer_factory.hpp:77] Creating layer ip1
I0218 17:54:02.061009 26654 net.cpp:106] Creating Layer ip1
I0218 17:54:02.061024 26654 net.cpp:454] ip1 <- pool3
I0218 17:54:02.061040 26654 net.cpp:411] ip1 -> ip1
I0218 17:54:02.078619 26654 net.cpp:150] Setting up ip1
I0218 17:54:02.078676 26654 net.cpp:157] Top shape: 100 500 (50000)
I0218 17:54:02.078690 26654 net.cpp:165] Memory required for data: 36245200
I0218 17:54:02.078706 26654 layer_factory.hpp:77] Creating layer relu4
I0218 17:54:02.078723 26654 net.cpp:106] Creating Layer relu4
I0218 17:54:02.078737 26654 net.cpp:454] relu4 <- ip1
I0218 17:54:02.078757 26654 net.cpp:397] relu4 -> ip1 (in-place)
I0218 17:54:02.079746 26654 net.cpp:150] Setting up relu4
I0218 17:54:02.079769 26654 net.cpp:157] Top shape: 100 500 (50000)
I0218 17:54:02.079779 26654 net.cpp:165] Memory required for data: 36445200
I0218 17:54:02.079789 26654 layer_factory.hpp:77] Creating layer ip2
I0218 17:54:02.079807 26654 net.cpp:106] Creating Layer ip2
I0218 17:54:02.079829 26654 net.cpp:454] ip2 <- ip1
I0218 17:54:02.079849 26654 net.cpp:411] ip2 -> ip2
I0218 17:54:02.080667 26654 net.cpp:150] Setting up ip2
I0218 17:54:02.080692 26654 net.cpp:157] Top shape: 100 10 (1000)
I0218 17:54:02.080701 26654 net.cpp:165] Memory required for data: 36449200
I0218 17:54:02.080721 26654 layer_factory.hpp:77] Creating layer loss
I0218 17:54:02.080741 26654 net.cpp:106] Creating Layer loss
I0218 17:54:02.080754 26654 net.cpp:454] loss <- ip2
I0218 17:54:02.080765 26654 net.cpp:454] loss <- label
I0218 17:54:02.080778 26654 net.cpp:411] loss -> loss
I0218 17:54:02.080797 26654 layer_factory.hpp:77] Creating layer loss
I0218 17:54:02.081845 26654 net.cpp:150] Setting up loss
I0218 17:54:02.081871 26654 net.cpp:157] Top shape: (1)
I0218 17:54:02.081882 26654 net.cpp:160]     with loss weight 1
I0218 17:54:02.081907 26654 net.cpp:165] Memory required for data: 36449204
I0218 17:54:02.081917 26654 net.cpp:226] loss needs backward computation.
I0218 17:54:02.081928 26654 net.cpp:226] ip2 needs backward computation.
I0218 17:54:02.081935 26654 net.cpp:226] relu4 needs backward computation.
I0218 17:54:02.081945 26654 net.cpp:226] ip1 needs backward computation.
I0218 17:54:02.081954 26654 net.cpp:226] pool3 needs backward computation.
I0218 17:54:02.081964 26654 net.cpp:226] relu3 needs backward computation.
I0218 17:54:02.081972 26654 net.cpp:226] conv3 needs backward computation.
I0218 17:54:02.081984 26654 net.cpp:226] norm2 needs backward computation.
I0218 17:54:02.082007 26654 net.cpp:226] pool2 needs backward computation.
I0218 17:54:02.082020 26654 net.cpp:226] relu2 needs backward computation.
I0218 17:54:02.082027 26654 net.cpp:226] conv2 needs backward computation.
I0218 17:54:02.082036 26654 net.cpp:226] norm1 needs backward computation.
I0218 17:54:02.082046 26654 net.cpp:226] relu1 needs backward computation.
I0218 17:54:02.082054 26654 net.cpp:226] pool1 needs backward computation.
I0218 17:54:02.082063 26654 net.cpp:226] conv1 needs backward computation.
I0218 17:54:02.082073 26654 net.cpp:228] cifar does not need backward computation.
I0218 17:54:02.082084 26654 net.cpp:270] This network produces output loss
I0218 17:54:02.082109 26654 net.cpp:283] Network initialization done.
I0218 17:54:02.082813 26654 solver.cpp:181] Creating test net (#0) specified by net file: examples/cifar10/cifar10_full_train_test.prototxt
I0218 17:54:02.082872 26654 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0218 17:54:02.083071 26654 net.cpp:49] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0218 17:54:02.083221 26654 layer_factory.hpp:77] Creating layer cifar
I0218 17:54:02.083372 26654 net.cpp:106] Creating Layer cifar
I0218 17:54:02.083400 26654 net.cpp:411] cifar -> data
I0218 17:54:02.083421 26654 net.cpp:411] cifar -> label
I0218 17:54:02.083441 26654 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0218 17:54:02.084650 26660 db_lmdb.cpp:38] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0218 17:54:02.084872 26654 data_layer.cpp:41] output data size: 100,3,32,32
I0218 17:54:02.087942 26654 net.cpp:150] Setting up cifar
I0218 17:54:02.087995 26654 net.cpp:157] Top shape: 100 3 32 32 (307200)
I0218 17:54:02.088011 26654 net.cpp:157] Top shape: 100 (100)
I0218 17:54:02.088021 26654 net.cpp:165] Memory required for data: 1229200
I0218 17:54:02.088030 26654 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0218 17:54:02.088053 26654 net.cpp:106] Creating Layer label_cifar_1_split
I0218 17:54:02.088068 26654 net.cpp:454] label_cifar_1_split <- label
I0218 17:54:02.088083 26654 net.cpp:411] label_cifar_1_split -> label_cifar_1_split_0
I0218 17:54:02.088101 26654 net.cpp:411] label_cifar_1_split -> label_cifar_1_split_1
I0218 17:54:02.088276 26654 net.cpp:150] Setting up label_cifar_1_split
I0218 17:54:02.088299 26654 net.cpp:157] Top shape: 100 (100)
I0218 17:54:02.088310 26654 net.cpp:157] Top shape: 100 (100)
I0218 17:54:02.088315 26654 net.cpp:165] Memory required for data: 1230000
I0218 17:54:02.088323 26654 layer_factory.hpp:77] Creating layer conv1
I0218 17:54:02.088346 26654 net.cpp:106] Creating Layer conv1
I0218 17:54:02.088354 26654 net.cpp:454] conv1 <- data
I0218 17:54:02.088366 26654 net.cpp:411] conv1 -> conv1
I0218 17:54:02.092185 26654 net.cpp:150] Setting up conv1
I0218 17:54:02.092221 26654 net.cpp:157] Top shape: 100 32 32 32 (3276800)
I0218 17:54:02.092233 26654 net.cpp:165] Memory required for data: 14337200
I0218 17:54:02.092252 26654 layer_factory.hpp:77] Creating layer pool1
I0218 17:54:02.092277 26654 net.cpp:106] Creating Layer pool1
I0218 17:54:02.092290 26654 net.cpp:454] pool1 <- conv1
I0218 17:54:02.092305 26654 net.cpp:411] pool1 -> pool1
I0218 17:54:02.093268 26654 net.cpp:150] Setting up pool1
I0218 17:54:02.093293 26654 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0218 17:54:02.093315 26654 net.cpp:165] Memory required for data: 17614000
I0218 17:54:02.093339 26654 layer_factory.hpp:77] Creating layer relu1
I0218 17:54:02.093355 26654 net.cpp:106] Creating Layer relu1
I0218 17:54:02.093366 26654 net.cpp:454] relu1 <- pool1
I0218 17:54:02.093379 26654 net.cpp:397] relu1 -> pool1 (in-place)
I0218 17:54:02.094267 26654 net.cpp:150] Setting up relu1
I0218 17:54:02.094293 26654 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0218 17:54:02.094305 26654 net.cpp:165] Memory required for data: 20890800
I0218 17:54:02.094317 26654 layer_factory.hpp:77] Creating layer norm1
I0218 17:54:02.094334 26654 net.cpp:106] Creating Layer norm1
I0218 17:54:02.094347 26654 net.cpp:454] norm1 <- pool1
I0218 17:54:02.094359 26654 net.cpp:411] norm1 -> norm1
I0218 17:54:02.095672 26654 net.cpp:150] Setting up norm1
I0218 17:54:02.095696 26654 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0218 17:54:02.095706 26654 net.cpp:165] Memory required for data: 24167600
I0218 17:54:02.095715 26654 layer_factory.hpp:77] Creating layer conv2
I0218 17:54:02.095737 26654 net.cpp:106] Creating Layer conv2
I0218 17:54:02.095752 26654 net.cpp:454] conv2 <- norm1
I0218 17:54:02.095767 26654 net.cpp:411] conv2 -> conv2
I0218 17:54:02.100031 26654 net.cpp:150] Setting up conv2
I0218 17:54:02.100069 26654 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0218 17:54:02.100082 26654 net.cpp:165] Memory required for data: 27444400
I0218 17:54:02.100101 26654 layer_factory.hpp:77] Creating layer relu2
I0218 17:54:02.100119 26654 net.cpp:106] Creating Layer relu2
I0218 17:54:02.100152 26654 net.cpp:454] relu2 <- conv2
I0218 17:54:02.100167 26654 net.cpp:397] relu2 -> conv2 (in-place)
I0218 17:54:02.101164 26654 net.cpp:150] Setting up relu2
I0218 17:54:02.101189 26654 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0218 17:54:02.101202 26654 net.cpp:165] Memory required for data: 30721200
I0218 17:54:02.101210 26654 layer_factory.hpp:77] Creating layer pool2
I0218 17:54:02.101225 26654 net.cpp:106] Creating Layer pool2
I0218 17:54:02.101239 26654 net.cpp:454] pool2 <- conv2
I0218 17:54:02.101256 26654 net.cpp:411] pool2 -> pool2
I0218 17:54:02.102322 26654 net.cpp:150] Setting up pool2
I0218 17:54:02.102349 26654 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0218 17:54:02.102361 26654 net.cpp:165] Memory required for data: 31540400
I0218 17:54:02.102373 26654 layer_factory.hpp:77] Creating layer norm2
I0218 17:54:02.102392 26654 net.cpp:106] Creating Layer norm2
I0218 17:54:02.102406 26654 net.cpp:454] norm2 <- pool2
I0218 17:54:02.102417 26654 net.cpp:411] norm2 -> norm2
I0218 17:54:02.103724 26654 net.cpp:150] Setting up norm2
I0218 17:54:02.103754 26654 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0218 17:54:02.103778 26654 net.cpp:165] Memory required for data: 32359600
I0218 17:54:02.103788 26654 layer_factory.hpp:77] Creating layer conv3
I0218 17:54:02.103809 26654 net.cpp:106] Creating Layer conv3
I0218 17:54:02.103824 26654 net.cpp:454] conv3 <- norm2
I0218 17:54:02.103842 26654 net.cpp:411] conv3 -> conv3
I0218 17:54:02.108889 26654 net.cpp:150] Setting up conv3
I0218 17:54:02.108932 26654 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0218 17:54:02.108943 26654 net.cpp:165] Memory required for data: 33998000
I0218 17:54:02.108968 26654 layer_factory.hpp:77] Creating layer relu3
I0218 17:54:02.108985 26654 net.cpp:106] Creating Layer relu3
I0218 17:54:02.108995 26654 net.cpp:454] relu3 <- conv3
I0218 17:54:02.109010 26654 net.cpp:397] relu3 -> conv3 (in-place)
I0218 17:54:02.109927 26654 net.cpp:150] Setting up relu3
I0218 17:54:02.109951 26654 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0218 17:54:02.109963 26654 net.cpp:165] Memory required for data: 35636400
I0218 17:54:02.109972 26654 layer_factory.hpp:77] Creating layer pool3
I0218 17:54:02.109992 26654 net.cpp:106] Creating Layer pool3
I0218 17:54:02.110005 26654 net.cpp:454] pool3 <- conv3
I0218 17:54:02.110018 26654 net.cpp:411] pool3 -> pool3
I0218 17:54:02.110920 26654 net.cpp:150] Setting up pool3
I0218 17:54:02.110944 26654 net.cpp:157] Top shape: 100 64 4 4 (102400)
I0218 17:54:02.110955 26654 net.cpp:165] Memory required for data: 36046000
I0218 17:54:02.110980 26654 layer_factory.hpp:77] Creating layer ip1
I0218 17:54:02.110999 26654 net.cpp:106] Creating Layer ip1
I0218 17:54:02.111013 26654 net.cpp:454] ip1 <- pool3
I0218 17:54:02.111028 26654 net.cpp:411] ip1 -> ip1
I0218 17:54:02.128619 26654 net.cpp:150] Setting up ip1
I0218 17:54:02.128674 26654 net.cpp:157] Top shape: 100 500 (50000)
I0218 17:54:02.128686 26654 net.cpp:165] Memory required for data: 36246000
I0218 17:54:02.128703 26654 layer_factory.hpp:77] Creating layer relu4
I0218 17:54:02.128721 26654 net.cpp:106] Creating Layer relu4
I0218 17:54:02.128733 26654 net.cpp:454] relu4 <- ip1
I0218 17:54:02.128756 26654 net.cpp:397] relu4 -> ip1 (in-place)
I0218 17:54:02.129807 26654 net.cpp:150] Setting up relu4
I0218 17:54:02.129832 26654 net.cpp:157] Top shape: 100 500 (50000)
I0218 17:54:02.129844 26654 net.cpp:165] Memory required for data: 36446000
I0218 17:54:02.129853 26654 layer_factory.hpp:77] Creating layer ip2
I0218 17:54:02.129875 26654 net.cpp:106] Creating Layer ip2
I0218 17:54:02.129892 26654 net.cpp:454] ip2 <- ip1
I0218 17:54:02.129909 26654 net.cpp:411] ip2 -> ip2
I0218 17:54:02.130288 26654 net.cpp:150] Setting up ip2
I0218 17:54:02.130312 26654 net.cpp:157] Top shape: 100 10 (1000)
I0218 17:54:02.130323 26654 net.cpp:165] Memory required for data: 36450000
I0218 17:54:02.130342 26654 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0218 17:54:02.130362 26654 net.cpp:106] Creating Layer ip2_ip2_0_split
I0218 17:54:02.130375 26654 net.cpp:454] ip2_ip2_0_split <- ip2
I0218 17:54:02.130408 26654 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0218 17:54:02.130430 26654 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0218 17:54:02.130504 26654 net.cpp:150] Setting up ip2_ip2_0_split
I0218 17:54:02.130524 26654 net.cpp:157] Top shape: 100 10 (1000)
I0218 17:54:02.130535 26654 net.cpp:157] Top shape: 100 10 (1000)
I0218 17:54:02.130547 26654 net.cpp:165] Memory required for data: 36458000
I0218 17:54:02.130558 26654 layer_factory.hpp:77] Creating layer accuracy
I0218 17:54:02.130575 26654 net.cpp:106] Creating Layer accuracy
I0218 17:54:02.130589 26654 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I0218 17:54:02.130599 26654 net.cpp:454] accuracy <- label_cifar_1_split_0
I0218 17:54:02.130614 26654 net.cpp:411] accuracy -> accuracy
I0218 17:54:02.130638 26654 net.cpp:150] Setting up accuracy
I0218 17:54:02.130656 26654 net.cpp:157] Top shape: (1)
I0218 17:54:02.130666 26654 net.cpp:165] Memory required for data: 36458004
I0218 17:54:02.130674 26654 layer_factory.hpp:77] Creating layer loss
I0218 17:54:02.130686 26654 net.cpp:106] Creating Layer loss
I0218 17:54:02.130695 26654 net.cpp:454] loss <- ip2_ip2_0_split_1
I0218 17:54:02.130704 26654 net.cpp:454] loss <- label_cifar_1_split_1
I0218 17:54:02.130714 26654 net.cpp:411] loss -> loss
I0218 17:54:02.130731 26654 layer_factory.hpp:77] Creating layer loss
I0218 17:54:02.131922 26654 net.cpp:150] Setting up loss
I0218 17:54:02.131947 26654 net.cpp:157] Top shape: (1)
I0218 17:54:02.131961 26654 net.cpp:160]     with loss weight 1
I0218 17:54:02.131978 26654 net.cpp:165] Memory required for data: 36458008
I0218 17:54:02.131986 26654 net.cpp:226] loss needs backward computation.
I0218 17:54:02.131994 26654 net.cpp:228] accuracy does not need backward computation.
I0218 17:54:02.132006 26654 net.cpp:226] ip2_ip2_0_split needs backward computation.
I0218 17:54:02.132014 26654 net.cpp:226] ip2 needs backward computation.
I0218 17:54:02.132021 26654 net.cpp:226] relu4 needs backward computation.
I0218 17:54:02.132032 26654 net.cpp:226] ip1 needs backward computation.
I0218 17:54:02.132041 26654 net.cpp:226] pool3 needs backward computation.
I0218 17:54:02.132050 26654 net.cpp:226] relu3 needs backward computation.
I0218 17:54:02.132060 26654 net.cpp:226] conv3 needs backward computation.
I0218 17:54:02.132071 26654 net.cpp:226] norm2 needs backward computation.
I0218 17:54:02.132083 26654 net.cpp:226] pool2 needs backward computation.
I0218 17:54:02.132092 26654 net.cpp:226] relu2 needs backward computation.
I0218 17:54:02.132110 26654 net.cpp:226] conv2 needs backward computation.
I0218 17:54:02.132120 26654 net.cpp:226] norm1 needs backward computation.
I0218 17:54:02.132128 26654 net.cpp:226] relu1 needs backward computation.
I0218 17:54:02.132136 26654 net.cpp:226] pool1 needs backward computation.
I0218 17:54:02.132144 26654 net.cpp:226] conv1 needs backward computation.
I0218 17:54:02.132154 26654 net.cpp:228] label_cifar_1_split does not need backward computation.
I0218 17:54:02.132165 26654 net.cpp:228] cifar does not need backward computation.
I0218 17:54:02.132174 26654 net.cpp:270] This network produces output accuracy
I0218 17:54:02.132181 26654 net.cpp:270] This network produces output loss
I0218 17:54:02.132206 26654 net.cpp:283] Network initialization done.
I0218 17:54:02.132340 26654 solver.cpp:60] Solver scaffolding done.
I0218 17:54:02.133005 26654 caffe.cpp:202] Resuming from examples/cifar10/cifar10_full_iter_60000.solverstate.h5
I0218 17:54:02.134460 26654 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0218 17:54:02.138427 26654 caffe.cpp:212] Starting Optimization
I0218 17:54:02.138473 26654 solver.cpp:288] Solving CIFAR10_full
I0218 17:54:02.138485 26654 solver.cpp:289] Learning Rate Policy: fixed
I0218 17:54:02.139258 26654 solver.cpp:341] Iteration 60000, Testing net (#0)
I0218 17:54:02.669406 26654 solver.cpp:409]     Test net output #0: accuracy = 0.7637
I0218 17:54:02.669468 26654 solver.cpp:409]     Test net output #1: loss = 0.785518 (* 1 = 0.785518 loss)
I0218 17:54:02.677676 26654 solver.cpp:237] Iteration 60000, loss = 0.118236
I0218 17:54:02.677753 26654 solver.cpp:253]     Train net output #0: loss = 0.118236 (* 1 = 0.118236 loss)
I0218 17:54:02.677772 26654 sgd_solver.cpp:106] Iteration 60000, lr = 0.0001
I0218 17:54:05.860728 26654 solver.cpp:237] Iteration 60200, loss = 0.167466
I0218 17:54:05.860776 26654 solver.cpp:253]     Train net output #0: loss = 0.167466 (* 1 = 0.167466 loss)
I0218 17:54:05.860785 26654 sgd_solver.cpp:106] Iteration 60200, lr = 0.0001
I0218 17:54:09.058621 26654 solver.cpp:237] Iteration 60400, loss = 0.116134
I0218 17:54:09.058671 26654 solver.cpp:253]     Train net output #0: loss = 0.116134 (* 1 = 0.116134 loss)
I0218 17:54:09.058681 26654 sgd_solver.cpp:106] Iteration 60400, lr = 0.0001
I0218 17:54:12.260130 26654 solver.cpp:237] Iteration 60600, loss = 0.102931
I0218 17:54:12.260176 26654 solver.cpp:253]     Train net output #0: loss = 0.102931 (* 1 = 0.102931 loss)
I0218 17:54:12.260185 26654 sgd_solver.cpp:106] Iteration 60600, lr = 0.0001
I0218 17:54:15.456704 26654 solver.cpp:237] Iteration 60800, loss = 0.161705
I0218 17:54:15.456753 26654 solver.cpp:253]     Train net output #0: loss = 0.161705 (* 1 = 0.161705 loss)
I0218 17:54:15.456761 26654 sgd_solver.cpp:106] Iteration 60800, lr = 0.0001
I0218 17:54:18.619057 26654 solver.cpp:341] Iteration 61000, Testing net (#0)
I0218 17:54:19.179229 26654 solver.cpp:409]     Test net output #0: accuracy = 0.7903
I0218 17:54:19.179272 26654 solver.cpp:409]     Test net output #1: loss = 0.62857 (* 1 = 0.62857 loss)
I0218 17:54:19.184738 26654 solver.cpp:237] Iteration 61000, loss = 0.123451
I0218 17:54:19.184772 26654 solver.cpp:253]     Train net output #0: loss = 0.123451 (* 1 = 0.123451 loss)
I0218 17:54:19.184782 26654 sgd_solver.cpp:106] Iteration 61000, lr = 0.0001
I0218 17:54:22.398669 26654 solver.cpp:237] Iteration 61200, loss = 0.147433
I0218 17:54:22.398713 26654 solver.cpp:253]     Train net output #0: loss = 0.147433 (* 1 = 0.147433 loss)
I0218 17:54:22.398722 26654 sgd_solver.cpp:106] Iteration 61200, lr = 0.0001
I0218 17:54:25.599841 26654 solver.cpp:237] Iteration 61400, loss = 0.123697
I0218 17:54:25.599892 26654 solver.cpp:253]     Train net output #0: loss = 0.123697 (* 1 = 0.123697 loss)
I0218 17:54:25.599901 26654 sgd_solver.cpp:106] Iteration 61400, lr = 0.0001
I0218 17:54:28.787860 26654 solver.cpp:237] Iteration 61600, loss = 0.0987733
I0218 17:54:28.787909 26654 solver.cpp:253]     Train net output #0: loss = 0.0987733 (* 1 = 0.0987733 loss)
I0218 17:54:28.787919 26654 sgd_solver.cpp:106] Iteration 61600, lr = 0.0001
I0218 17:54:31.948753 26654 solver.cpp:237] Iteration 61800, loss = 0.151215
I0218 17:54:31.948876 26654 solver.cpp:253]     Train net output #0: loss = 0.151215 (* 1 = 0.151215 loss)
I0218 17:54:31.948887 26654 sgd_solver.cpp:106] Iteration 61800, lr = 0.0001
I0218 17:54:35.116756 26654 solver.cpp:341] Iteration 62000, Testing net (#0)
I0218 17:54:35.652657 26654 solver.cpp:409]     Test net output #0: accuracy = 0.7917
I0218 17:54:35.652695 26654 solver.cpp:409]     Test net output #1: loss = 0.6295 (* 1 = 0.6295 loss)
I0218 17:54:35.658113 26654 solver.cpp:237] Iteration 62000, loss = 0.119423
I0218 17:54:35.658145 26654 solver.cpp:253]     Train net output #0: loss = 0.119423 (* 1 = 0.119423 loss)
I0218 17:54:35.658154 26654 sgd_solver.cpp:106] Iteration 62000, lr = 0.0001
I0218 17:54:38.832932 26654 solver.cpp:237] Iteration 62200, loss = 0.139293
I0218 17:54:38.832978 26654 solver.cpp:253]     Train net output #0: loss = 0.139293 (* 1 = 0.139293 loss)
I0218 17:54:38.832985 26654 sgd_solver.cpp:106] Iteration 62200, lr = 0.0001
I0218 17:54:41.995765 26654 solver.cpp:237] Iteration 62400, loss = 0.121708
I0218 17:54:41.995810 26654 solver.cpp:253]     Train net output #0: loss = 0.121708 (* 1 = 0.121708 loss)
I0218 17:54:41.995818 26654 sgd_solver.cpp:106] Iteration 62400, lr = 0.0001
I0218 17:54:45.157840 26654 solver.cpp:237] Iteration 62600, loss = 0.0963715
I0218 17:54:45.157886 26654 solver.cpp:253]     Train net output #0: loss = 0.0963716 (* 1 = 0.0963716 loss)
I0218 17:54:45.157893 26654 sgd_solver.cpp:106] Iteration 62600, lr = 0.0001
I0218 17:54:48.327946 26654 solver.cpp:237] Iteration 62800, loss = 0.146298
I0218 17:54:48.327991 26654 solver.cpp:253]     Train net output #0: loss = 0.146298 (* 1 = 0.146298 loss)
I0218 17:54:48.327999 26654 sgd_solver.cpp:106] Iteration 62800, lr = 0.0001
I0218 17:54:51.464202 26654 solver.cpp:341] Iteration 63000, Testing net (#0)
I0218 17:54:52.001410 26654 solver.cpp:409]     Test net output #0: accuracy = 0.7921
I0218 17:54:52.001454 26654 solver.cpp:409]     Test net output #1: loss = 0.63057 (* 1 = 0.63057 loss)
I0218 17:54:52.007040 26654 solver.cpp:237] Iteration 63000, loss = 0.115206
I0218 17:54:52.007089 26654 solver.cpp:253]     Train net output #0: loss = 0.115206 (* 1 = 0.115206 loss)
I0218 17:54:52.007098 26654 sgd_solver.cpp:106] Iteration 63000, lr = 0.0001
I0218 17:54:55.176355 26654 solver.cpp:237] Iteration 63200, loss = 0.134077
I0218 17:54:55.176401 26654 solver.cpp:253]     Train net output #0: loss = 0.134077 (* 1 = 0.134077 loss)
I0218 17:54:55.176409 26654 sgd_solver.cpp:106] Iteration 63200, lr = 0.0001
I0218 17:54:58.360607 26654 solver.cpp:237] Iteration 63400, loss = 0.118713
I0218 17:54:58.360653 26654 solver.cpp:253]     Train net output #0: loss = 0.118713 (* 1 = 0.118713 loss)
I0218 17:54:58.360661 26654 sgd_solver.cpp:106] Iteration 63400, lr = 0.0001
I0218 17:55:01.542450 26654 solver.cpp:237] Iteration 63600, loss = 0.0947294
I0218 17:55:01.542495 26654 solver.cpp:253]     Train net output #0: loss = 0.0947295 (* 1 = 0.0947295 loss)
I0218 17:55:01.542503 26654 sgd_solver.cpp:106] Iteration 63600, lr = 0.0001
I0218 17:55:04.700130 26654 solver.cpp:237] Iteration 63800, loss = 0.141258
I0218 17:55:04.700245 26654 solver.cpp:253]     Train net output #0: loss = 0.141258 (* 1 = 0.141258 loss)
I0218 17:55:04.700255 26654 sgd_solver.cpp:106] Iteration 63800, lr = 0.0001
I0218 17:55:07.860980 26654 solver.cpp:341] Iteration 64000, Testing net (#0)
I0218 17:55:08.398023 26654 solver.cpp:409]     Test net output #0: accuracy = 0.7921
I0218 17:55:08.398068 26654 solver.cpp:409]     Test net output #1: loss = 0.631149 (* 1 = 0.631149 loss)
I0218 17:55:08.403309 26654 solver.cpp:237] Iteration 64000, loss = 0.111248
I0218 17:55:08.403347 26654 solver.cpp:253]     Train net output #0: loss = 0.111248 (* 1 = 0.111248 loss)
I0218 17:55:08.403357 26654 sgd_solver.cpp:106] Iteration 64000, lr = 0.0001
I0218 17:55:11.580261 26654 solver.cpp:237] Iteration 64200, loss = 0.130546
I0218 17:55:11.580308 26654 solver.cpp:253]     Train net output #0: loss = 0.130546 (* 1 = 0.130546 loss)
I0218 17:55:11.580332 26654 sgd_solver.cpp:106] Iteration 64200, lr = 0.0001
I0218 17:55:14.710631 26654 solver.cpp:237] Iteration 64400, loss = 0.114681
I0218 17:55:14.710682 26654 solver.cpp:253]     Train net output #0: loss = 0.114681 (* 1 = 0.114681 loss)
I0218 17:55:14.710938 26654 sgd_solver.cpp:106] Iteration 64400, lr = 0.0001
I0218 17:55:17.864598 26654 solver.cpp:237] Iteration 64600, loss = 0.0934689
I0218 17:55:17.864644 26654 solver.cpp:253]     Train net output #0: loss = 0.093469 (* 1 = 0.093469 loss)
I0218 17:55:17.864652 26654 sgd_solver.cpp:106] Iteration 64600, lr = 0.0001
I0218 17:55:21.013543 26654 solver.cpp:237] Iteration 64800, loss = 0.137683
I0218 17:55:21.013592 26654 solver.cpp:253]     Train net output #0: loss = 0.137684 (* 1 = 0.137684 loss)
I0218 17:55:21.013602 26654 sgd_solver.cpp:106] Iteration 64800, lr = 0.0001
I0218 17:55:24.152257 26654 solver.cpp:472] Snapshotting to HDF5 file examples/cifar10/cifar10_full_iter_65000.caffemodel.h5
I0218 17:55:24.703462 26654 sgd_solver.cpp:279] Snapshotting solver state to HDF5 file examples/cifar10/cifar10_full_iter_65000.solverstate.h5
I0218 17:55:24.710804 26654 solver.cpp:321] Iteration 65000, loss = 0.107784
I0218 17:55:24.710841 26654 solver.cpp:341] Iteration 65000, Testing net (#0)
I0218 17:55:25.231284 26654 solver.cpp:409]     Test net output #0: accuracy = 0.7929
I0218 17:55:25.231330 26654 solver.cpp:409]     Test net output #1: loss = 0.631528 (* 1 = 0.631528 loss)
I0218 17:55:25.231338 26654 solver.cpp:326] Optimization Done.
I0218 17:55:25.231343 26654 caffe.cpp:215] Optimization Done.
I0218 17:55:25.873924 26662 caffe.cpp:184] Using GPUs 0
I0218 17:55:26.111115 26662 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 1000
base_lr: 1e-05
display: 200
max_iter: 70000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.004
snapshot: 5000
snapshot_prefix: "examples/cifar10/cifar10_full"
solver_mode: GPU
device_id: 0
net: "examples/cifar10/cifar10_full_train_test.prototxt"
snapshot_format: HDF5
I0218 17:55:26.111259 26662 solver.cpp:91] Creating training net from net file: examples/cifar10/cifar10_full_train_test.prototxt
I0218 17:55:26.111974 26662 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0218 17:55:26.112006 26662 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0218 17:55:26.112167 26662 net.cpp:49] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TRAIN
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0218 17:55:26.112288 26662 layer_factory.hpp:77] Creating layer cifar
I0218 17:55:26.113070 26662 net.cpp:106] Creating Layer cifar
I0218 17:55:26.113093 26662 net.cpp:411] cifar -> data
I0218 17:55:26.113137 26662 net.cpp:411] cifar -> label
I0218 17:55:26.113174 26662 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0218 17:55:26.114720 26666 db_lmdb.cpp:38] Opened lmdb examples/cifar10/cifar10_train_lmdb
I0218 17:55:26.125150 26662 data_layer.cpp:41] output data size: 100,3,32,32
I0218 17:55:26.128062 26662 net.cpp:150] Setting up cifar
I0218 17:55:26.128105 26662 net.cpp:157] Top shape: 100 3 32 32 (307200)
I0218 17:55:26.128116 26662 net.cpp:157] Top shape: 100 (100)
I0218 17:55:26.128123 26662 net.cpp:165] Memory required for data: 1229200
I0218 17:55:26.128139 26662 layer_factory.hpp:77] Creating layer conv1
I0218 17:55:26.128170 26662 net.cpp:106] Creating Layer conv1
I0218 17:55:26.128180 26662 net.cpp:454] conv1 <- data
I0218 17:55:26.128197 26662 net.cpp:411] conv1 -> conv1
I0218 17:55:26.284301 26662 net.cpp:150] Setting up conv1
I0218 17:55:26.284351 26662 net.cpp:157] Top shape: 100 32 32 32 (3276800)
I0218 17:55:26.284360 26662 net.cpp:165] Memory required for data: 14336400
I0218 17:55:26.284384 26662 layer_factory.hpp:77] Creating layer pool1
I0218 17:55:26.284404 26662 net.cpp:106] Creating Layer pool1
I0218 17:55:26.284414 26662 net.cpp:454] pool1 <- conv1
I0218 17:55:26.284426 26662 net.cpp:411] pool1 -> pool1
I0218 17:55:26.285295 26662 net.cpp:150] Setting up pool1
I0218 17:55:26.285317 26662 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0218 17:55:26.285325 26662 net.cpp:165] Memory required for data: 17613200
I0218 17:55:26.285331 26662 layer_factory.hpp:77] Creating layer relu1
I0218 17:55:26.285343 26662 net.cpp:106] Creating Layer relu1
I0218 17:55:26.285354 26662 net.cpp:454] relu1 <- pool1
I0218 17:55:26.285364 26662 net.cpp:397] relu1 -> pool1 (in-place)
I0218 17:55:26.286123 26662 net.cpp:150] Setting up relu1
I0218 17:55:26.286144 26662 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0218 17:55:26.286152 26662 net.cpp:165] Memory required for data: 20890000
I0218 17:55:26.286159 26662 layer_factory.hpp:77] Creating layer norm1
I0218 17:55:26.286178 26662 net.cpp:106] Creating Layer norm1
I0218 17:55:26.286186 26662 net.cpp:454] norm1 <- pool1
I0218 17:55:26.286196 26662 net.cpp:411] norm1 -> norm1
I0218 17:55:26.287328 26662 net.cpp:150] Setting up norm1
I0218 17:55:26.287349 26662 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0218 17:55:26.287366 26662 net.cpp:165] Memory required for data: 24166800
I0218 17:55:26.287374 26662 layer_factory.hpp:77] Creating layer conv2
I0218 17:55:26.287392 26662 net.cpp:106] Creating Layer conv2
I0218 17:55:26.287401 26662 net.cpp:454] conv2 <- norm1
I0218 17:55:26.287412 26662 net.cpp:411] conv2 -> conv2
I0218 17:55:26.291466 26662 net.cpp:150] Setting up conv2
I0218 17:55:26.291502 26662 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0218 17:55:26.291510 26662 net.cpp:165] Memory required for data: 27443600
I0218 17:55:26.291527 26662 layer_factory.hpp:77] Creating layer relu2
I0218 17:55:26.291543 26662 net.cpp:106] Creating Layer relu2
I0218 17:55:26.291553 26662 net.cpp:454] relu2 <- conv2
I0218 17:55:26.291568 26662 net.cpp:397] relu2 -> conv2 (in-place)
I0218 17:55:26.292351 26662 net.cpp:150] Setting up relu2
I0218 17:55:26.292371 26662 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0218 17:55:26.292378 26662 net.cpp:165] Memory required for data: 30720400
I0218 17:55:26.292384 26662 layer_factory.hpp:77] Creating layer pool2
I0218 17:55:26.292397 26662 net.cpp:106] Creating Layer pool2
I0218 17:55:26.292405 26662 net.cpp:454] pool2 <- conv2
I0218 17:55:26.292415 26662 net.cpp:411] pool2 -> pool2
I0218 17:55:26.293195 26662 net.cpp:150] Setting up pool2
I0218 17:55:26.293217 26662 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0218 17:55:26.293226 26662 net.cpp:165] Memory required for data: 31539600
I0218 17:55:26.293233 26662 layer_factory.hpp:77] Creating layer norm2
I0218 17:55:26.293247 26662 net.cpp:106] Creating Layer norm2
I0218 17:55:26.293257 26662 net.cpp:454] norm2 <- pool2
I0218 17:55:26.293270 26662 net.cpp:411] norm2 -> norm2
I0218 17:55:26.294365 26662 net.cpp:150] Setting up norm2
I0218 17:55:26.294402 26662 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0218 17:55:26.294410 26662 net.cpp:165] Memory required for data: 32358800
I0218 17:55:26.294416 26662 layer_factory.hpp:77] Creating layer conv3
I0218 17:55:26.294431 26662 net.cpp:106] Creating Layer conv3
I0218 17:55:26.294440 26662 net.cpp:454] conv3 <- norm2
I0218 17:55:26.294452 26662 net.cpp:411] conv3 -> conv3
I0218 17:55:26.298902 26662 net.cpp:150] Setting up conv3
I0218 17:55:26.298944 26662 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0218 17:55:26.298951 26662 net.cpp:165] Memory required for data: 33997200
I0218 17:55:26.298969 26662 layer_factory.hpp:77] Creating layer relu3
I0218 17:55:26.298982 26662 net.cpp:106] Creating Layer relu3
I0218 17:55:26.298995 26662 net.cpp:454] relu3 <- conv3
I0218 17:55:26.299008 26662 net.cpp:397] relu3 -> conv3 (in-place)
I0218 17:55:26.299818 26662 net.cpp:150] Setting up relu3
I0218 17:55:26.299837 26662 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0218 17:55:26.299845 26662 net.cpp:165] Memory required for data: 35635600
I0218 17:55:26.299852 26662 layer_factory.hpp:77] Creating layer pool3
I0218 17:55:26.299865 26662 net.cpp:106] Creating Layer pool3
I0218 17:55:26.299875 26662 net.cpp:454] pool3 <- conv3
I0218 17:55:26.299886 26662 net.cpp:411] pool3 -> pool3
I0218 17:55:26.300673 26662 net.cpp:150] Setting up pool3
I0218 17:55:26.300693 26662 net.cpp:157] Top shape: 100 64 4 4 (102400)
I0218 17:55:26.300700 26662 net.cpp:165] Memory required for data: 36045200
I0218 17:55:26.300706 26662 layer_factory.hpp:77] Creating layer ip1
I0218 17:55:26.300719 26662 net.cpp:106] Creating Layer ip1
I0218 17:55:26.300726 26662 net.cpp:454] ip1 <- pool3
I0218 17:55:26.300740 26662 net.cpp:411] ip1 -> ip1
I0218 17:55:26.318904 26662 net.cpp:150] Setting up ip1
I0218 17:55:26.318954 26662 net.cpp:157] Top shape: 100 500 (50000)
I0218 17:55:26.318963 26662 net.cpp:165] Memory required for data: 36245200
I0218 17:55:26.318977 26662 layer_factory.hpp:77] Creating layer relu4
I0218 17:55:26.318994 26662 net.cpp:106] Creating Layer relu4
I0218 17:55:26.319005 26662 net.cpp:454] relu4 <- ip1
I0218 17:55:26.319017 26662 net.cpp:397] relu4 -> ip1 (in-place)
I0218 17:55:26.319913 26662 net.cpp:150] Setting up relu4
I0218 17:55:26.319937 26662 net.cpp:157] Top shape: 100 500 (50000)
I0218 17:55:26.319955 26662 net.cpp:165] Memory required for data: 36445200
I0218 17:55:26.319963 26662 layer_factory.hpp:77] Creating layer ip2
I0218 17:55:26.319979 26662 net.cpp:106] Creating Layer ip2
I0218 17:55:26.319988 26662 net.cpp:454] ip2 <- ip1
I0218 17:55:26.320000 26662 net.cpp:411] ip2 -> ip2
I0218 17:55:26.320802 26662 net.cpp:150] Setting up ip2
I0218 17:55:26.320825 26662 net.cpp:157] Top shape: 100 10 (1000)
I0218 17:55:26.320832 26662 net.cpp:165] Memory required for data: 36449200
I0218 17:55:26.320850 26662 layer_factory.hpp:77] Creating layer loss
I0218 17:55:26.320864 26662 net.cpp:106] Creating Layer loss
I0218 17:55:26.320873 26662 net.cpp:454] loss <- ip2
I0218 17:55:26.320880 26662 net.cpp:454] loss <- label
I0218 17:55:26.320893 26662 net.cpp:411] loss -> loss
I0218 17:55:26.320919 26662 layer_factory.hpp:77] Creating layer loss
I0218 17:55:26.321836 26662 net.cpp:150] Setting up loss
I0218 17:55:26.321856 26662 net.cpp:157] Top shape: (1)
I0218 17:55:26.321862 26662 net.cpp:160]     with loss weight 1
I0218 17:55:26.321882 26662 net.cpp:165] Memory required for data: 36449204
I0218 17:55:26.321889 26662 net.cpp:226] loss needs backward computation.
I0218 17:55:26.321897 26662 net.cpp:226] ip2 needs backward computation.
I0218 17:55:26.321903 26662 net.cpp:226] relu4 needs backward computation.
I0218 17:55:26.321914 26662 net.cpp:226] ip1 needs backward computation.
I0218 17:55:26.321921 26662 net.cpp:226] pool3 needs backward computation.
I0218 17:55:26.321928 26662 net.cpp:226] relu3 needs backward computation.
I0218 17:55:26.321934 26662 net.cpp:226] conv3 needs backward computation.
I0218 17:55:26.321941 26662 net.cpp:226] norm2 needs backward computation.
I0218 17:55:26.321964 26662 net.cpp:226] pool2 needs backward computation.
I0218 17:55:26.321971 26662 net.cpp:226] relu2 needs backward computation.
I0218 17:55:26.321979 26662 net.cpp:226] conv2 needs backward computation.
I0218 17:55:26.321985 26662 net.cpp:226] norm1 needs backward computation.
I0218 17:55:26.321992 26662 net.cpp:226] relu1 needs backward computation.
I0218 17:55:26.322000 26662 net.cpp:226] pool1 needs backward computation.
I0218 17:55:26.322010 26662 net.cpp:226] conv1 needs backward computation.
I0218 17:55:26.322017 26662 net.cpp:228] cifar does not need backward computation.
I0218 17:55:26.322023 26662 net.cpp:270] This network produces output loss
I0218 17:55:26.322042 26662 net.cpp:283] Network initialization done.
I0218 17:55:26.322753 26662 solver.cpp:181] Creating test net (#0) specified by net file: examples/cifar10/cifar10_full_train_test.prototxt
I0218 17:55:26.322805 26662 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0218 17:55:26.322988 26662 net.cpp:49] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0218 17:55:26.323134 26662 layer_factory.hpp:77] Creating layer cifar
I0218 17:55:26.323276 26662 net.cpp:106] Creating Layer cifar
I0218 17:55:26.323307 26662 net.cpp:411] cifar -> data
I0218 17:55:26.323325 26662 net.cpp:411] cifar -> label
I0218 17:55:26.323343 26662 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0218 17:55:26.325042 26668 db_lmdb.cpp:38] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0218 17:55:26.325227 26662 data_layer.cpp:41] output data size: 100,3,32,32
I0218 17:55:26.328155 26662 net.cpp:150] Setting up cifar
I0218 17:55:26.328200 26662 net.cpp:157] Top shape: 100 3 32 32 (307200)
I0218 17:55:26.328212 26662 net.cpp:157] Top shape: 100 (100)
I0218 17:55:26.328217 26662 net.cpp:165] Memory required for data: 1229200
I0218 17:55:26.328227 26662 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0218 17:55:26.328244 26662 net.cpp:106] Creating Layer label_cifar_1_split
I0218 17:55:26.328256 26662 net.cpp:454] label_cifar_1_split <- label
I0218 17:55:26.328268 26662 net.cpp:411] label_cifar_1_split -> label_cifar_1_split_0
I0218 17:55:26.328282 26662 net.cpp:411] label_cifar_1_split -> label_cifar_1_split_1
I0218 17:55:26.328342 26662 net.cpp:150] Setting up label_cifar_1_split
I0218 17:55:26.328356 26662 net.cpp:157] Top shape: 100 (100)
I0218 17:55:26.328364 26662 net.cpp:157] Top shape: 100 (100)
I0218 17:55:26.328371 26662 net.cpp:165] Memory required for data: 1230000
I0218 17:55:26.328377 26662 layer_factory.hpp:77] Creating layer conv1
I0218 17:55:26.328397 26662 net.cpp:106] Creating Layer conv1
I0218 17:55:26.328404 26662 net.cpp:454] conv1 <- data
I0218 17:55:26.328418 26662 net.cpp:411] conv1 -> conv1
I0218 17:55:26.331823 26662 net.cpp:150] Setting up conv1
I0218 17:55:26.331861 26662 net.cpp:157] Top shape: 100 32 32 32 (3276800)
I0218 17:55:26.331871 26662 net.cpp:165] Memory required for data: 14337200
I0218 17:55:26.331888 26662 layer_factory.hpp:77] Creating layer pool1
I0218 17:55:26.331902 26662 net.cpp:106] Creating Layer pool1
I0218 17:55:26.331933 26662 net.cpp:454] pool1 <- conv1
I0218 17:55:26.331946 26662 net.cpp:411] pool1 -> pool1
I0218 17:55:26.332804 26662 net.cpp:150] Setting up pool1
I0218 17:55:26.332831 26662 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0218 17:55:26.332839 26662 net.cpp:165] Memory required for data: 17614000
I0218 17:55:26.332846 26662 layer_factory.hpp:77] Creating layer relu1
I0218 17:55:26.332859 26662 net.cpp:106] Creating Layer relu1
I0218 17:55:26.332878 26662 net.cpp:454] relu1 <- pool1
I0218 17:55:26.332890 26662 net.cpp:397] relu1 -> pool1 (in-place)
I0218 17:55:26.333696 26662 net.cpp:150] Setting up relu1
I0218 17:55:26.333719 26662 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0218 17:55:26.333739 26662 net.cpp:165] Memory required for data: 20890800
I0218 17:55:26.333746 26662 layer_factory.hpp:77] Creating layer norm1
I0218 17:55:26.333760 26662 net.cpp:106] Creating Layer norm1
I0218 17:55:26.333782 26662 net.cpp:454] norm1 <- pool1
I0218 17:55:26.333802 26662 net.cpp:411] norm1 -> norm1
I0218 17:55:26.334935 26662 net.cpp:150] Setting up norm1
I0218 17:55:26.334957 26662 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0218 17:55:26.334965 26662 net.cpp:165] Memory required for data: 24167600
I0218 17:55:26.334974 26662 layer_factory.hpp:77] Creating layer conv2
I0218 17:55:26.334990 26662 net.cpp:106] Creating Layer conv2
I0218 17:55:26.335006 26662 net.cpp:454] conv2 <- norm1
I0218 17:55:26.335017 26662 net.cpp:411] conv2 -> conv2
I0218 17:55:26.338827 26662 net.cpp:150] Setting up conv2
I0218 17:55:26.338867 26662 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0218 17:55:26.338876 26662 net.cpp:165] Memory required for data: 27444400
I0218 17:55:26.338894 26662 layer_factory.hpp:77] Creating layer relu2
I0218 17:55:26.338913 26662 net.cpp:106] Creating Layer relu2
I0218 17:55:26.338939 26662 net.cpp:454] relu2 <- conv2
I0218 17:55:26.338950 26662 net.cpp:397] relu2 -> conv2 (in-place)
I0218 17:55:26.339797 26662 net.cpp:150] Setting up relu2
I0218 17:55:26.339821 26662 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0218 17:55:26.339829 26662 net.cpp:165] Memory required for data: 30721200
I0218 17:55:26.339836 26662 layer_factory.hpp:77] Creating layer pool2
I0218 17:55:26.339849 26662 net.cpp:106] Creating Layer pool2
I0218 17:55:26.339859 26662 net.cpp:454] pool2 <- conv2
I0218 17:55:26.339870 26662 net.cpp:411] pool2 -> pool2
I0218 17:55:26.340834 26662 net.cpp:150] Setting up pool2
I0218 17:55:26.340857 26662 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0218 17:55:26.340864 26662 net.cpp:165] Memory required for data: 31540400
I0218 17:55:26.340872 26662 layer_factory.hpp:77] Creating layer norm2
I0218 17:55:26.340884 26662 net.cpp:106] Creating Layer norm2
I0218 17:55:26.340901 26662 net.cpp:454] norm2 <- pool2
I0218 17:55:26.340914 26662 net.cpp:411] norm2 -> norm2
I0218 17:55:26.342056 26662 net.cpp:150] Setting up norm2
I0218 17:55:26.342079 26662 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0218 17:55:26.342087 26662 net.cpp:165] Memory required for data: 32359600
I0218 17:55:26.342094 26662 layer_factory.hpp:77] Creating layer conv3
I0218 17:55:26.342109 26662 net.cpp:106] Creating Layer conv3
I0218 17:55:26.342128 26662 net.cpp:454] conv3 <- norm2
I0218 17:55:26.342141 26662 net.cpp:411] conv3 -> conv3
I0218 17:55:26.346828 26662 net.cpp:150] Setting up conv3
I0218 17:55:26.346868 26662 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0218 17:55:26.346876 26662 net.cpp:165] Memory required for data: 33998000
I0218 17:55:26.346896 26662 layer_factory.hpp:77] Creating layer relu3
I0218 17:55:26.346911 26662 net.cpp:106] Creating Layer relu3
I0218 17:55:26.346921 26662 net.cpp:454] relu3 <- conv3
I0218 17:55:26.346933 26662 net.cpp:397] relu3 -> conv3 (in-place)
I0218 17:55:26.347725 26662 net.cpp:150] Setting up relu3
I0218 17:55:26.347746 26662 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0218 17:55:26.347757 26662 net.cpp:165] Memory required for data: 35636400
I0218 17:55:26.347764 26662 layer_factory.hpp:77] Creating layer pool3
I0218 17:55:26.347777 26662 net.cpp:106] Creating Layer pool3
I0218 17:55:26.347786 26662 net.cpp:454] pool3 <- conv3
I0218 17:55:26.347797 26662 net.cpp:411] pool3 -> pool3
I0218 17:55:26.348635 26662 net.cpp:150] Setting up pool3
I0218 17:55:26.348655 26662 net.cpp:157] Top shape: 100 64 4 4 (102400)
I0218 17:55:26.348664 26662 net.cpp:165] Memory required for data: 36046000
I0218 17:55:26.348672 26662 layer_factory.hpp:77] Creating layer ip1
I0218 17:55:26.348687 26662 net.cpp:106] Creating Layer ip1
I0218 17:55:26.348695 26662 net.cpp:454] ip1 <- pool3
I0218 17:55:26.348711 26662 net.cpp:411] ip1 -> ip1
I0218 17:55:26.366883 26662 net.cpp:150] Setting up ip1
I0218 17:55:26.366931 26662 net.cpp:157] Top shape: 100 500 (50000)
I0218 17:55:26.366940 26662 net.cpp:165] Memory required for data: 36246000
I0218 17:55:26.366953 26662 layer_factory.hpp:77] Creating layer relu4
I0218 17:55:26.366969 26662 net.cpp:106] Creating Layer relu4
I0218 17:55:26.366978 26662 net.cpp:454] relu4 <- ip1
I0218 17:55:26.366989 26662 net.cpp:397] relu4 -> ip1 (in-place)
I0218 17:55:26.367889 26662 net.cpp:150] Setting up relu4
I0218 17:55:26.367909 26662 net.cpp:157] Top shape: 100 500 (50000)
I0218 17:55:26.367915 26662 net.cpp:165] Memory required for data: 36446000
I0218 17:55:26.367923 26662 layer_factory.hpp:77] Creating layer ip2
I0218 17:55:26.367935 26662 net.cpp:106] Creating Layer ip2
I0218 17:55:26.367943 26662 net.cpp:454] ip2 <- ip1
I0218 17:55:26.367952 26662 net.cpp:411] ip2 -> ip2
I0218 17:55:26.368311 26662 net.cpp:150] Setting up ip2
I0218 17:55:26.368327 26662 net.cpp:157] Top shape: 100 10 (1000)
I0218 17:55:26.368335 26662 net.cpp:165] Memory required for data: 36450000
I0218 17:55:26.368352 26662 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0218 17:55:26.368366 26662 net.cpp:106] Creating Layer ip2_ip2_0_split
I0218 17:55:26.368373 26662 net.cpp:454] ip2_ip2_0_split <- ip2
I0218 17:55:26.368402 26662 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0218 17:55:26.368417 26662 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0218 17:55:26.368473 26662 net.cpp:150] Setting up ip2_ip2_0_split
I0218 17:55:26.368487 26662 net.cpp:157] Top shape: 100 10 (1000)
I0218 17:55:26.368495 26662 net.cpp:157] Top shape: 100 10 (1000)
I0218 17:55:26.368505 26662 net.cpp:165] Memory required for data: 36458000
I0218 17:55:26.368512 26662 layer_factory.hpp:77] Creating layer accuracy
I0218 17:55:26.368522 26662 net.cpp:106] Creating Layer accuracy
I0218 17:55:26.368531 26662 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I0218 17:55:26.368541 26662 net.cpp:454] accuracy <- label_cifar_1_split_0
I0218 17:55:26.368549 26662 net.cpp:411] accuracy -> accuracy
I0218 17:55:26.368566 26662 net.cpp:150] Setting up accuracy
I0218 17:55:26.368580 26662 net.cpp:157] Top shape: (1)
I0218 17:55:26.368590 26662 net.cpp:165] Memory required for data: 36458004
I0218 17:55:26.368597 26662 layer_factory.hpp:77] Creating layer loss
I0218 17:55:26.368607 26662 net.cpp:106] Creating Layer loss
I0218 17:55:26.368613 26662 net.cpp:454] loss <- ip2_ip2_0_split_1
I0218 17:55:26.368635 26662 net.cpp:454] loss <- label_cifar_1_split_1
I0218 17:55:26.368645 26662 net.cpp:411] loss -> loss
I0218 17:55:26.368664 26662 layer_factory.hpp:77] Creating layer loss
I0218 17:55:26.369712 26662 net.cpp:150] Setting up loss
I0218 17:55:26.369735 26662 net.cpp:157] Top shape: (1)
I0218 17:55:26.369741 26662 net.cpp:160]     with loss weight 1
I0218 17:55:26.369752 26662 net.cpp:165] Memory required for data: 36458008
I0218 17:55:26.369762 26662 net.cpp:226] loss needs backward computation.
I0218 17:55:26.369770 26662 net.cpp:228] accuracy does not need backward computation.
I0218 17:55:26.369777 26662 net.cpp:226] ip2_ip2_0_split needs backward computation.
I0218 17:55:26.369786 26662 net.cpp:226] ip2 needs backward computation.
I0218 17:55:26.369792 26662 net.cpp:226] relu4 needs backward computation.
I0218 17:55:26.369798 26662 net.cpp:226] ip1 needs backward computation.
I0218 17:55:26.369806 26662 net.cpp:226] pool3 needs backward computation.
I0218 17:55:26.369812 26662 net.cpp:226] relu3 needs backward computation.
I0218 17:55:26.369817 26662 net.cpp:226] conv3 needs backward computation.
I0218 17:55:26.369829 26662 net.cpp:226] norm2 needs backward computation.
I0218 17:55:26.369844 26662 net.cpp:226] pool2 needs backward computation.
I0218 17:55:26.369853 26662 net.cpp:226] relu2 needs backward computation.
I0218 17:55:26.369858 26662 net.cpp:226] conv2 needs backward computation.
I0218 17:55:26.369864 26662 net.cpp:226] norm1 needs backward computation.
I0218 17:55:26.369871 26662 net.cpp:226] relu1 needs backward computation.
I0218 17:55:26.369879 26662 net.cpp:226] pool1 needs backward computation.
I0218 17:55:26.369886 26662 net.cpp:226] conv1 needs backward computation.
I0218 17:55:26.369894 26662 net.cpp:228] label_cifar_1_split does not need backward computation.
I0218 17:55:26.369902 26662 net.cpp:228] cifar does not need backward computation.
I0218 17:55:26.369909 26662 net.cpp:270] This network produces output accuracy
I0218 17:55:26.369915 26662 net.cpp:270] This network produces output loss
I0218 17:55:26.369937 26662 net.cpp:283] Network initialization done.
I0218 17:55:26.370064 26662 solver.cpp:60] Solver scaffolding done.
I0218 17:55:26.370564 26662 caffe.cpp:202] Resuming from examples/cifar10/cifar10_full_iter_65000.solverstate.h5
I0218 17:55:26.371932 26662 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0218 17:55:26.375625 26662 caffe.cpp:212] Starting Optimization
I0218 17:55:26.375668 26662 solver.cpp:288] Solving CIFAR10_full
I0218 17:55:26.375676 26662 solver.cpp:289] Learning Rate Policy: fixed
I0218 17:55:26.376322 26662 solver.cpp:341] Iteration 65000, Testing net (#0)
I0218 17:55:26.911384 26662 solver.cpp:409]     Test net output #0: accuracy = 0.7929
I0218 17:55:26.911437 26662 solver.cpp:409]     Test net output #1: loss = 0.631528 (* 1 = 0.631528 loss)
I0218 17:55:26.919289 26662 solver.cpp:237] Iteration 65000, loss = 0.107784
I0218 17:55:26.919358 26662 solver.cpp:253]     Train net output #0: loss = 0.107784 (* 1 = 0.107784 loss)
I0218 17:55:26.919371 26662 sgd_solver.cpp:106] Iteration 65000, lr = 1e-05
I0218 17:55:30.101327 26662 solver.cpp:237] Iteration 65200, loss = 0.121
I0218 17:55:30.101387 26662 solver.cpp:253]     Train net output #0: loss = 0.121 (* 1 = 0.121 loss)
I0218 17:55:30.101398 26662 sgd_solver.cpp:106] Iteration 65200, lr = 1e-05
I0218 17:55:33.316808 26662 solver.cpp:237] Iteration 65400, loss = 0.0771162
I0218 17:55:33.316862 26662 solver.cpp:253]     Train net output #0: loss = 0.0771162 (* 1 = 0.0771162 loss)
I0218 17:55:33.316874 26662 sgd_solver.cpp:106] Iteration 65400, lr = 1e-05
I0218 17:55:36.508741 26662 solver.cpp:237] Iteration 65600, loss = 0.0898576
I0218 17:55:36.508797 26662 solver.cpp:253]     Train net output #0: loss = 0.0898576 (* 1 = 0.0898576 loss)
I0218 17:55:36.508810 26662 sgd_solver.cpp:106] Iteration 65600, lr = 1e-05
I0218 17:55:39.663890 26662 solver.cpp:237] Iteration 65800, loss = 0.145484
I0218 17:55:39.663944 26662 solver.cpp:253]     Train net output #0: loss = 0.145484 (* 1 = 0.145484 loss)
I0218 17:55:39.663955 26662 sgd_solver.cpp:106] Iteration 65800, lr = 1e-05
I0218 17:55:42.866786 26662 solver.cpp:341] Iteration 66000, Testing net (#0)
I0218 17:55:43.408013 26662 solver.cpp:409]     Test net output #0: accuracy = 0.7983
I0218 17:55:43.408056 26662 solver.cpp:409]     Test net output #1: loss = 0.623326 (* 1 = 0.623326 loss)
I0218 17:55:43.413005 26662 solver.cpp:237] Iteration 66000, loss = 0.108683
I0218 17:55:43.413038 26662 solver.cpp:253]     Train net output #0: loss = 0.108683 (* 1 = 0.108683 loss)
I0218 17:55:43.413048 26662 sgd_solver.cpp:106] Iteration 66000, lr = 1e-05
I0218 17:55:46.571207 26662 solver.cpp:237] Iteration 66200, loss = 0.116884
I0218 17:55:46.571259 26662 solver.cpp:253]     Train net output #0: loss = 0.116884 (* 1 = 0.116884 loss)
I0218 17:55:46.571270 26662 sgd_solver.cpp:106] Iteration 66200, lr = 1e-05
I0218 17:55:49.737777 26662 solver.cpp:237] Iteration 66400, loss = 0.0813966
I0218 17:55:49.737823 26662 solver.cpp:253]     Train net output #0: loss = 0.0813966 (* 1 = 0.0813966 loss)
I0218 17:55:49.737835 26662 sgd_solver.cpp:106] Iteration 66400, lr = 1e-05
I0218 17:55:52.925395 26662 solver.cpp:237] Iteration 66600, loss = 0.08939
I0218 17:55:52.925453 26662 solver.cpp:253]     Train net output #0: loss = 0.08939 (* 1 = 0.08939 loss)
I0218 17:55:52.925464 26662 sgd_solver.cpp:106] Iteration 66600, lr = 1e-05
I0218 17:55:56.135282 26662 solver.cpp:237] Iteration 66800, loss = 0.14332
I0218 17:55:56.135396 26662 solver.cpp:253]     Train net output #0: loss = 0.14332 (* 1 = 0.14332 loss)
I0218 17:55:56.135411 26662 sgd_solver.cpp:106] Iteration 66800, lr = 1e-05
I0218 17:55:59.319782 26662 solver.cpp:341] Iteration 67000, Testing net (#0)
I0218 17:55:59.862679 26662 solver.cpp:409]     Test net output #0: accuracy = 0.7979
I0218 17:55:59.862723 26662 solver.cpp:409]     Test net output #1: loss = 0.623073 (* 1 = 0.623073 loss)
I0218 17:55:59.867790 26662 solver.cpp:237] Iteration 67000, loss = 0.107676
I0218 17:55:59.867825 26662 solver.cpp:253]     Train net output #0: loss = 0.107676 (* 1 = 0.107676 loss)
I0218 17:55:59.867835 26662 sgd_solver.cpp:106] Iteration 67000, lr = 1e-05
I0218 17:56:03.034539 26662 solver.cpp:237] Iteration 67200, loss = 0.116731
I0218 17:56:03.034589 26662 solver.cpp:253]     Train net output #0: loss = 0.116731 (* 1 = 0.116731 loss)
I0218 17:56:03.034600 26662 sgd_solver.cpp:106] Iteration 67200, lr = 1e-05
I0218 17:56:06.203349 26662 solver.cpp:237] Iteration 67400, loss = 0.0836001
I0218 17:56:06.203404 26662 solver.cpp:253]     Train net output #0: loss = 0.0836001 (* 1 = 0.0836001 loss)
I0218 17:56:06.203415 26662 sgd_solver.cpp:106] Iteration 67400, lr = 1e-05
I0218 17:56:09.377310 26662 solver.cpp:237] Iteration 67600, loss = 0.0887192
I0218 17:56:09.377357 26662 solver.cpp:253]     Train net output #0: loss = 0.0887192 (* 1 = 0.0887192 loss)
I0218 17:56:09.377368 26662 sgd_solver.cpp:106] Iteration 67600, lr = 1e-05
I0218 17:56:12.545670 26662 solver.cpp:237] Iteration 67800, loss = 0.142568
I0218 17:56:12.545716 26662 solver.cpp:253]     Train net output #0: loss = 0.142568 (* 1 = 0.142568 loss)
I0218 17:56:12.545727 26662 sgd_solver.cpp:106] Iteration 67800, lr = 1e-05
I0218 17:56:15.690323 26662 solver.cpp:341] Iteration 68000, Testing net (#0)
I0218 17:56:16.229426 26662 solver.cpp:409]     Test net output #0: accuracy = 0.7979
I0218 17:56:16.229472 26662 solver.cpp:409]     Test net output #1: loss = 0.622891 (* 1 = 0.622891 loss)
I0218 17:56:16.234678 26662 solver.cpp:237] Iteration 68000, loss = 0.106755
I0218 17:56:16.234710 26662 solver.cpp:253]     Train net output #0: loss = 0.106755 (* 1 = 0.106755 loss)
I0218 17:56:16.234721 26662 sgd_solver.cpp:106] Iteration 68000, lr = 1e-05
I0218 17:56:19.395535 26662 solver.cpp:237] Iteration 68200, loss = 0.11691
I0218 17:56:19.395588 26662 solver.cpp:253]     Train net output #0: loss = 0.11691 (* 1 = 0.11691 loss)
I0218 17:56:19.395835 26662 sgd_solver.cpp:106] Iteration 68200, lr = 1e-05
I0218 17:56:22.557850 26662 solver.cpp:237] Iteration 68400, loss = 0.0849284
I0218 17:56:22.557895 26662 solver.cpp:253]     Train net output #0: loss = 0.0849284 (* 1 = 0.0849284 loss)
I0218 17:56:22.557904 26662 sgd_solver.cpp:106] Iteration 68400, lr = 1e-05
I0218 17:56:25.719105 26662 solver.cpp:237] Iteration 68600, loss = 0.0882165
I0218 17:56:25.719149 26662 solver.cpp:253]     Train net output #0: loss = 0.0882166 (* 1 = 0.0882166 loss)
I0218 17:56:25.719156 26662 sgd_solver.cpp:106] Iteration 68600, lr = 1e-05
I0218 17:56:28.877977 26662 solver.cpp:237] Iteration 68800, loss = 0.142167
I0218 17:56:28.878087 26662 solver.cpp:253]     Train net output #0: loss = 0.142167 (* 1 = 0.142167 loss)
I0218 17:56:28.878099 26662 sgd_solver.cpp:106] Iteration 68800, lr = 1e-05
I0218 17:56:32.023696 26662 solver.cpp:341] Iteration 69000, Testing net (#0)
I0218 17:56:32.559201 26662 solver.cpp:409]     Test net output #0: accuracy = 0.7981
I0218 17:56:32.559244 26662 solver.cpp:409]     Test net output #1: loss = 0.622734 (* 1 = 0.622734 loss)
I0218 17:56:32.564231 26662 solver.cpp:237] Iteration 69000, loss = 0.106061
I0218 17:56:32.564260 26662 solver.cpp:253]     Train net output #0: loss = 0.106061 (* 1 = 0.106061 loss)
I0218 17:56:32.564267 26662 sgd_solver.cpp:106] Iteration 69000, lr = 1e-05
I0218 17:56:35.710222 26662 solver.cpp:237] Iteration 69200, loss = 0.117124
I0218 17:56:35.710265 26662 solver.cpp:253]     Train net output #0: loss = 0.117124 (* 1 = 0.117124 loss)
I0218 17:56:35.710273 26662 sgd_solver.cpp:106] Iteration 69200, lr = 1e-05
I0218 17:56:38.880455 26662 solver.cpp:237] Iteration 69400, loss = 0.0858132
I0218 17:56:38.880493 26662 solver.cpp:253]     Train net output #0: loss = 0.0858132 (* 1 = 0.0858132 loss)
I0218 17:56:38.880501 26662 sgd_solver.cpp:106] Iteration 69400, lr = 1e-05
I0218 17:56:42.045125 26662 solver.cpp:237] Iteration 69600, loss = 0.0878673
I0218 17:56:42.045169 26662 solver.cpp:253]     Train net output #0: loss = 0.0878673 (* 1 = 0.0878673 loss)
I0218 17:56:42.045177 26662 sgd_solver.cpp:106] Iteration 69600, lr = 1e-05
I0218 17:56:45.215669 26662 solver.cpp:237] Iteration 69800, loss = 0.141952
I0218 17:56:45.215715 26662 solver.cpp:253]     Train net output #0: loss = 0.141952 (* 1 = 0.141952 loss)
I0218 17:56:45.215723 26662 sgd_solver.cpp:106] Iteration 69800, lr = 1e-05
I0218 17:56:48.379106 26662 solver.cpp:472] Snapshotting to HDF5 file examples/cifar10/cifar10_full_iter_70000.caffemodel.h5
I0218 17:56:48.937304 26662 sgd_solver.cpp:279] Snapshotting solver state to HDF5 file examples/cifar10/cifar10_full_iter_70000.solverstate.h5
I0218 17:56:48.945127 26662 solver.cpp:321] Iteration 70000, loss = 0.105435
I0218 17:56:48.945163 26662 solver.cpp:341] Iteration 70000, Testing net (#0)
I0218 17:56:49.473400 26662 solver.cpp:409]     Test net output #0: accuracy = 0.798
I0218 17:56:49.473443 26662 solver.cpp:409]     Test net output #1: loss = 0.622648 (* 1 = 0.622648 loss)
I0218 17:56:49.473448 26662 solver.cpp:326] Optimization Done.
I0218 17:56:49.473453 26662 caffe.cpp:215] Optimization Done.
