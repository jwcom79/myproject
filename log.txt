I0216 13:13:28.651908 23592 caffe.cpp:177] Use CPU.
I0216 13:13:28.911056 23592 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 1000
base_lr: 0.001
display: 200
max_iter: 60000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.004
snapshot: 10000
snapshot_prefix: "examples/cifar10/jungwoo_full"
solver_mode: CPU
net: "examples/cifar10/jungwoo_full_train_test.prototxt"
snapshot_format: HDF5
I0216 13:13:28.911234 23592 solver.cpp:91] Creating training net from net file: examples/cifar10/jungwoo_full_train_test.prototxt
I0216 13:13:28.911953 23592 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0216 13:13:28.911996 23592 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0216 13:13:28.912183 23592 net.cpp:49] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TRAIN
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "push1"
  type: "Pushin"
  bottom: "ip1"
  top: "ip1"
  pushin_param {
    pushin_ratio: 0.2
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0216 13:13:28.912322 23592 layer_factory.hpp:77] Creating layer cifar
I0216 13:13:28.914373 23592 net.cpp:106] Creating Layer cifar
I0216 13:13:28.914429 23592 net.cpp:411] cifar -> data
I0216 13:13:28.914470 23592 net.cpp:411] cifar -> label
I0216 13:13:28.914507 23592 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0216 13:13:28.915392 23596 db_lmdb.cpp:38] Opened lmdb examples/cifar10/cifar10_train_lmdb
I0216 13:13:28.916901 23592 data_layer.cpp:41] output data size: 100,3,32,32
I0216 13:13:28.918277 23592 net.cpp:150] Setting up cifar
I0216 13:13:28.918329 23592 net.cpp:157] Top shape: 100 3 32 32 (307200)
I0216 13:13:28.918344 23592 net.cpp:157] Top shape: 100 (100)
I0216 13:13:28.918351 23592 net.cpp:165] Memory required for data: 1229200
I0216 13:13:28.918367 23592 layer_factory.hpp:77] Creating layer conv1
I0216 13:13:28.918403 23592 net.cpp:106] Creating Layer conv1
I0216 13:13:28.918418 23592 net.cpp:454] conv1 <- data
I0216 13:13:28.918442 23592 net.cpp:411] conv1 -> conv1
I0216 13:13:29.169680 23592 net.cpp:150] Setting up conv1
I0216 13:13:29.169736 23592 net.cpp:157] Top shape: 100 32 32 32 (3276800)
I0216 13:13:29.169749 23592 net.cpp:165] Memory required for data: 14336400
I0216 13:13:29.169777 23592 layer_factory.hpp:77] Creating layer pool1
I0216 13:13:29.169800 23592 net.cpp:106] Creating Layer pool1
I0216 13:13:29.169812 23592 net.cpp:454] pool1 <- conv1
I0216 13:13:29.169826 23592 net.cpp:411] pool1 -> pool1
I0216 13:13:29.170670 23592 net.cpp:150] Setting up pool1
I0216 13:13:29.170696 23592 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 13:13:29.170706 23592 net.cpp:165] Memory required for data: 17613200
I0216 13:13:29.170716 23592 layer_factory.hpp:77] Creating layer relu1
I0216 13:13:29.170727 23592 net.cpp:106] Creating Layer relu1
I0216 13:13:29.170737 23592 net.cpp:454] relu1 <- pool1
I0216 13:13:29.170747 23592 net.cpp:397] relu1 -> pool1 (in-place)
I0216 13:13:29.171541 23592 net.cpp:150] Setting up relu1
I0216 13:13:29.171566 23592 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 13:13:29.171576 23592 net.cpp:165] Memory required for data: 20890000
I0216 13:13:29.171586 23592 layer_factory.hpp:77] Creating layer norm1
I0216 13:13:29.171619 23592 net.cpp:106] Creating Layer norm1
I0216 13:13:29.171632 23592 net.cpp:454] norm1 <- pool1
I0216 13:13:29.171643 23592 net.cpp:411] norm1 -> norm1
I0216 13:13:29.172830 23592 net.cpp:150] Setting up norm1
I0216 13:13:29.172871 23592 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 13:13:29.172883 23592 net.cpp:165] Memory required for data: 24166800
I0216 13:13:29.172891 23592 layer_factory.hpp:77] Creating layer conv2
I0216 13:13:29.172909 23592 net.cpp:106] Creating Layer conv2
I0216 13:13:29.172919 23592 net.cpp:454] conv2 <- norm1
I0216 13:13:29.172933 23592 net.cpp:411] conv2 -> conv2
I0216 13:13:29.176806 23592 net.cpp:150] Setting up conv2
I0216 13:13:29.176858 23592 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 13:13:29.176870 23592 net.cpp:165] Memory required for data: 27443600
I0216 13:13:29.176893 23592 layer_factory.hpp:77] Creating layer relu2
I0216 13:13:29.176915 23592 net.cpp:106] Creating Layer relu2
I0216 13:13:29.176928 23592 net.cpp:454] relu2 <- conv2
I0216 13:13:29.176940 23592 net.cpp:397] relu2 -> conv2 (in-place)
I0216 13:13:29.177860 23592 net.cpp:150] Setting up relu2
I0216 13:13:29.177886 23592 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 13:13:29.177894 23592 net.cpp:165] Memory required for data: 30720400
I0216 13:13:29.177902 23592 layer_factory.hpp:77] Creating layer pool2
I0216 13:13:29.177918 23592 net.cpp:106] Creating Layer pool2
I0216 13:13:29.177928 23592 net.cpp:454] pool2 <- conv2
I0216 13:13:29.177943 23592 net.cpp:411] pool2 -> pool2
I0216 13:13:29.178958 23592 net.cpp:150] Setting up pool2
I0216 13:13:29.178997 23592 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0216 13:13:29.179009 23592 net.cpp:165] Memory required for data: 31539600
I0216 13:13:29.179019 23592 layer_factory.hpp:77] Creating layer norm2
I0216 13:13:29.179040 23592 net.cpp:106] Creating Layer norm2
I0216 13:13:29.179064 23592 net.cpp:454] norm2 <- pool2
I0216 13:13:29.179095 23592 net.cpp:411] norm2 -> norm2
I0216 13:13:29.180357 23592 net.cpp:150] Setting up norm2
I0216 13:13:29.180382 23592 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0216 13:13:29.180392 23592 net.cpp:165] Memory required for data: 32358800
I0216 13:13:29.180399 23592 layer_factory.hpp:77] Creating layer conv3
I0216 13:13:29.180418 23592 net.cpp:106] Creating Layer conv3
I0216 13:13:29.180428 23592 net.cpp:454] conv3 <- norm2
I0216 13:13:29.180444 23592 net.cpp:411] conv3 -> conv3
I0216 13:13:29.185160 23592 net.cpp:150] Setting up conv3
I0216 13:13:29.185214 23592 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0216 13:13:29.185226 23592 net.cpp:165] Memory required for data: 33997200
I0216 13:13:29.185248 23592 layer_factory.hpp:77] Creating layer relu3
I0216 13:13:29.185266 23592 net.cpp:106] Creating Layer relu3
I0216 13:13:29.185276 23592 net.cpp:454] relu3 <- conv3
I0216 13:13:29.185286 23592 net.cpp:397] relu3 -> conv3 (in-place)
I0216 13:13:29.186206 23592 net.cpp:150] Setting up relu3
I0216 13:13:29.186228 23592 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0216 13:13:29.186236 23592 net.cpp:165] Memory required for data: 35635600
I0216 13:13:29.186244 23592 layer_factory.hpp:77] Creating layer pool3
I0216 13:13:29.186256 23592 net.cpp:106] Creating Layer pool3
I0216 13:13:29.186264 23592 net.cpp:454] pool3 <- conv3
I0216 13:13:29.186278 23592 net.cpp:411] pool3 -> pool3
I0216 13:13:29.187201 23592 net.cpp:150] Setting up pool3
I0216 13:13:29.187222 23592 net.cpp:157] Top shape: 100 64 4 4 (102400)
I0216 13:13:29.187230 23592 net.cpp:165] Memory required for data: 36045200
I0216 13:13:29.187238 23592 layer_factory.hpp:77] Creating layer ip1
I0216 13:13:29.187263 23592 net.cpp:106] Creating Layer ip1
I0216 13:13:29.187273 23592 net.cpp:454] ip1 <- pool3
I0216 13:13:29.187286 23592 net.cpp:411] ip1 -> ip1
I0216 13:13:29.204512 23592 net.cpp:150] Setting up ip1
I0216 13:13:29.204565 23592 net.cpp:157] Top shape: 100 500 (50000)
I0216 13:13:29.204576 23592 net.cpp:165] Memory required for data: 36245200
I0216 13:13:29.204592 23592 layer_factory.hpp:77] Creating layer relu4
I0216 13:13:29.204608 23592 net.cpp:106] Creating Layer relu4
I0216 13:13:29.204617 23592 net.cpp:454] relu4 <- ip1
I0216 13:13:29.204628 23592 net.cpp:397] relu4 -> ip1 (in-place)
I0216 13:13:29.205783 23592 net.cpp:150] Setting up relu4
I0216 13:13:29.205822 23592 net.cpp:157] Top shape: 100 500 (50000)
I0216 13:13:29.205834 23592 net.cpp:165] Memory required for data: 36445200
I0216 13:13:29.205844 23592 layer_factory.hpp:77] Creating layer push1
I0216 13:13:29.205859 23592 net.cpp:106] Creating Layer push1
I0216 13:13:29.205868 23592 net.cpp:454] push1 <- ip1
I0216 13:13:29.205885 23592 net.cpp:397] push1 -> ip1 (in-place)
I0216 13:13:29.205909 23592 net.cpp:150] Setting up push1
I0216 13:13:29.205921 23592 net.cpp:157] Top shape: 100 500 (50000)
I0216 13:13:29.205929 23592 net.cpp:165] Memory required for data: 36645200
I0216 13:13:29.205935 23592 layer_factory.hpp:77] Creating layer ip2
I0216 13:13:29.205950 23592 net.cpp:106] Creating Layer ip2
I0216 13:13:29.205957 23592 net.cpp:454] ip2 <- ip1
I0216 13:13:29.205970 23592 net.cpp:411] ip2 -> ip2
I0216 13:13:29.206174 23592 net.cpp:150] Setting up ip2
I0216 13:13:29.206192 23592 net.cpp:157] Top shape: 100 10 (1000)
I0216 13:13:29.206200 23592 net.cpp:165] Memory required for data: 36649200
I0216 13:13:29.206223 23592 layer_factory.hpp:77] Creating layer loss
I0216 13:13:29.206240 23592 net.cpp:106] Creating Layer loss
I0216 13:13:29.206250 23592 net.cpp:454] loss <- ip2
I0216 13:13:29.206259 23592 net.cpp:454] loss <- label
I0216 13:13:29.206271 23592 net.cpp:411] loss -> loss
I0216 13:13:29.206287 23592 layer_factory.hpp:77] Creating layer loss
I0216 13:13:29.207355 23592 net.cpp:150] Setting up loss
I0216 13:13:29.207386 23592 net.cpp:157] Top shape: (1)
I0216 13:13:29.207396 23592 net.cpp:160]     with loss weight 1
I0216 13:13:29.207419 23592 net.cpp:165] Memory required for data: 36649204
I0216 13:13:29.207442 23592 net.cpp:226] loss needs backward computation.
I0216 13:13:29.207465 23592 net.cpp:226] ip2 needs backward computation.
I0216 13:13:29.207474 23592 net.cpp:226] push1 needs backward computation.
I0216 13:13:29.207481 23592 net.cpp:226] relu4 needs backward computation.
I0216 13:13:29.207487 23592 net.cpp:226] ip1 needs backward computation.
I0216 13:13:29.207494 23592 net.cpp:226] pool3 needs backward computation.
I0216 13:13:29.207500 23592 net.cpp:226] relu3 needs backward computation.
I0216 13:13:29.207507 23592 net.cpp:226] conv3 needs backward computation.
I0216 13:13:29.207514 23592 net.cpp:226] norm2 needs backward computation.
I0216 13:13:29.207520 23592 net.cpp:226] pool2 needs backward computation.
I0216 13:13:29.207530 23592 net.cpp:226] relu2 needs backward computation.
I0216 13:13:29.207537 23592 net.cpp:226] conv2 needs backward computation.
I0216 13:13:29.207545 23592 net.cpp:226] norm1 needs backward computation.
I0216 13:13:29.207551 23592 net.cpp:226] relu1 needs backward computation.
I0216 13:13:29.207557 23592 net.cpp:226] pool1 needs backward computation.
I0216 13:13:29.207564 23592 net.cpp:226] conv1 needs backward computation.
I0216 13:13:29.207571 23592 net.cpp:228] cifar does not need backward computation.
I0216 13:13:29.207577 23592 net.cpp:270] This network produces output loss
I0216 13:13:29.207598 23592 net.cpp:283] Network initialization done.
I0216 13:13:29.208330 23592 solver.cpp:181] Creating test net (#0) specified by net file: examples/cifar10/jungwoo_full_train_test.prototxt
I0216 13:13:29.208392 23592 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0216 13:13:29.208596 23592 net.cpp:49] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "push1"
  type: "Pushin"
  bottom: "ip1"
  top: "ip1"
  pushin_param {
    pushin_ratio: 0.2
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0216 13:13:29.208757 23592 layer_factory.hpp:77] Creating layer cifar
I0216 13:13:29.208932 23592 net.cpp:106] Creating Layer cifar
I0216 13:13:29.208986 23592 net.cpp:411] cifar -> data
I0216 13:13:29.209010 23592 net.cpp:411] cifar -> label
I0216 13:13:29.209028 23592 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0216 13:13:29.209995 23598 db_lmdb.cpp:38] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0216 13:13:29.210057 23592 data_layer.cpp:41] output data size: 100,3,32,32
I0216 13:13:29.211566 23592 net.cpp:150] Setting up cifar
I0216 13:13:29.211616 23592 net.cpp:157] Top shape: 100 3 32 32 (307200)
I0216 13:13:29.211629 23592 net.cpp:157] Top shape: 100 (100)
I0216 13:13:29.211637 23592 net.cpp:165] Memory required for data: 1229200
I0216 13:13:29.211647 23592 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0216 13:13:29.211664 23592 net.cpp:106] Creating Layer label_cifar_1_split
I0216 13:13:29.211673 23592 net.cpp:454] label_cifar_1_split <- label
I0216 13:13:29.211685 23592 net.cpp:411] label_cifar_1_split -> label_cifar_1_split_0
I0216 13:13:29.211702 23592 net.cpp:411] label_cifar_1_split -> label_cifar_1_split_1
I0216 13:13:29.211722 23592 net.cpp:150] Setting up label_cifar_1_split
I0216 13:13:29.211735 23592 net.cpp:157] Top shape: 100 (100)
I0216 13:13:29.211743 23592 net.cpp:157] Top shape: 100 (100)
I0216 13:13:29.211750 23592 net.cpp:165] Memory required for data: 1230000
I0216 13:13:29.211757 23592 layer_factory.hpp:77] Creating layer conv1
I0216 13:13:29.211777 23592 net.cpp:106] Creating Layer conv1
I0216 13:13:29.211786 23592 net.cpp:454] conv1 <- data
I0216 13:13:29.213158 23592 net.cpp:411] conv1 -> conv1
I0216 13:13:29.216656 23592 net.cpp:150] Setting up conv1
I0216 13:13:29.216711 23592 net.cpp:157] Top shape: 100 32 32 32 (3276800)
I0216 13:13:29.216723 23592 net.cpp:165] Memory required for data: 14337200
I0216 13:13:29.216748 23592 layer_factory.hpp:77] Creating layer pool1
I0216 13:13:29.216769 23592 net.cpp:106] Creating Layer pool1
I0216 13:13:29.216779 23592 net.cpp:454] pool1 <- conv1
I0216 13:13:29.216794 23592 net.cpp:411] pool1 -> pool1
I0216 13:13:29.217721 23592 net.cpp:150] Setting up pool1
I0216 13:13:29.217751 23592 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 13:13:29.217761 23592 net.cpp:165] Memory required for data: 17614000
I0216 13:13:29.217771 23592 layer_factory.hpp:77] Creating layer relu1
I0216 13:13:29.217787 23592 net.cpp:106] Creating Layer relu1
I0216 13:13:29.217797 23592 net.cpp:454] relu1 <- pool1
I0216 13:13:29.217808 23592 net.cpp:397] relu1 -> pool1 (in-place)
I0216 13:13:29.218747 23592 net.cpp:150] Setting up relu1
I0216 13:13:29.218780 23592 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 13:13:29.218791 23592 net.cpp:165] Memory required for data: 20890800
I0216 13:13:29.218799 23592 layer_factory.hpp:77] Creating layer norm1
I0216 13:13:29.218818 23592 net.cpp:106] Creating Layer norm1
I0216 13:13:29.218829 23592 net.cpp:454] norm1 <- pool1
I0216 13:13:29.218842 23592 net.cpp:411] norm1 -> norm1
I0216 13:13:29.220099 23592 net.cpp:150] Setting up norm1
I0216 13:13:29.220156 23592 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 13:13:29.220167 23592 net.cpp:165] Memory required for data: 24167600
I0216 13:13:29.220177 23592 layer_factory.hpp:77] Creating layer conv2
I0216 13:13:29.220197 23592 net.cpp:106] Creating Layer conv2
I0216 13:13:29.220208 23592 net.cpp:454] conv2 <- norm1
I0216 13:13:29.220226 23592 net.cpp:411] conv2 -> conv2
I0216 13:13:29.224309 23592 net.cpp:150] Setting up conv2
I0216 13:13:29.224365 23592 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 13:13:29.224377 23592 net.cpp:165] Memory required for data: 27444400
I0216 13:13:29.224402 23592 layer_factory.hpp:77] Creating layer relu2
I0216 13:13:29.224423 23592 net.cpp:106] Creating Layer relu2
I0216 13:13:29.224433 23592 net.cpp:454] relu2 <- conv2
I0216 13:13:29.224448 23592 net.cpp:397] relu2 -> conv2 (in-place)
I0216 13:13:29.225378 23592 net.cpp:150] Setting up relu2
I0216 13:13:29.225407 23592 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0216 13:13:29.225417 23592 net.cpp:165] Memory required for data: 30721200
I0216 13:13:29.225425 23592 layer_factory.hpp:77] Creating layer pool2
I0216 13:13:29.225445 23592 net.cpp:106] Creating Layer pool2
I0216 13:13:29.225455 23592 net.cpp:454] pool2 <- conv2
I0216 13:13:29.225466 23592 net.cpp:411] pool2 -> pool2
I0216 13:13:29.226400 23592 net.cpp:150] Setting up pool2
I0216 13:13:29.226429 23592 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0216 13:13:29.226439 23592 net.cpp:165] Memory required for data: 31540400
I0216 13:13:29.226447 23592 layer_factory.hpp:77] Creating layer norm2
I0216 13:13:29.226464 23592 net.cpp:106] Creating Layer norm2
I0216 13:13:29.226475 23592 net.cpp:454] norm2 <- pool2
I0216 13:13:29.226485 23592 net.cpp:411] norm2 -> norm2
I0216 13:13:29.227771 23592 net.cpp:150] Setting up norm2
I0216 13:13:29.227810 23592 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0216 13:13:29.227820 23592 net.cpp:165] Memory required for data: 32359600
I0216 13:13:29.227829 23592 layer_factory.hpp:77] Creating layer conv3
I0216 13:13:29.227852 23592 net.cpp:106] Creating Layer conv3
I0216 13:13:29.227864 23592 net.cpp:454] conv3 <- norm2
I0216 13:13:29.227880 23592 net.cpp:411] conv3 -> conv3
I0216 13:13:29.233113 23592 net.cpp:150] Setting up conv3
I0216 13:13:29.233165 23592 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0216 13:13:29.233175 23592 net.cpp:165] Memory required for data: 33998000
I0216 13:13:29.233197 23592 layer_factory.hpp:77] Creating layer relu3
I0216 13:13:29.233217 23592 net.cpp:106] Creating Layer relu3
I0216 13:13:29.233227 23592 net.cpp:454] relu3 <- conv3
I0216 13:13:29.233242 23592 net.cpp:397] relu3 -> conv3 (in-place)
I0216 13:13:29.234549 23592 net.cpp:150] Setting up relu3
I0216 13:13:29.234602 23592 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0216 13:13:29.234634 23592 net.cpp:165] Memory required for data: 35636400
I0216 13:13:29.234666 23592 layer_factory.hpp:77] Creating layer pool3
I0216 13:13:29.234699 23592 net.cpp:106] Creating Layer pool3
I0216 13:13:29.234716 23592 net.cpp:454] pool3 <- conv3
I0216 13:13:29.234746 23592 net.cpp:411] pool3 -> pool3
I0216 13:13:29.236281 23592 net.cpp:150] Setting up pool3
I0216 13:13:29.236326 23592 net.cpp:157] Top shape: 100 64 4 4 (102400)
I0216 13:13:29.236337 23592 net.cpp:165] Memory required for data: 36046000
I0216 13:13:29.236348 23592 layer_factory.hpp:77] Creating layer ip1
I0216 13:13:29.236368 23592 net.cpp:106] Creating Layer ip1
I0216 13:13:29.236380 23592 net.cpp:454] ip1 <- pool3
I0216 13:13:29.236397 23592 net.cpp:411] ip1 -> ip1
I0216 13:13:29.253669 23592 net.cpp:150] Setting up ip1
I0216 13:13:29.253733 23592 net.cpp:157] Top shape: 100 500 (50000)
I0216 13:13:29.253744 23592 net.cpp:165] Memory required for data: 36246000
I0216 13:13:29.253759 23592 layer_factory.hpp:77] Creating layer relu4
I0216 13:13:29.253774 23592 net.cpp:106] Creating Layer relu4
I0216 13:13:29.253783 23592 net.cpp:454] relu4 <- ip1
I0216 13:13:29.253793 23592 net.cpp:397] relu4 -> ip1 (in-place)
I0216 13:13:29.254961 23592 net.cpp:150] Setting up relu4
I0216 13:13:29.255009 23592 net.cpp:157] Top shape: 100 500 (50000)
I0216 13:13:29.255020 23592 net.cpp:165] Memory required for data: 36446000
I0216 13:13:29.255029 23592 layer_factory.hpp:77] Creating layer push1
I0216 13:13:29.255041 23592 net.cpp:106] Creating Layer push1
I0216 13:13:29.255049 23592 net.cpp:454] push1 <- ip1
I0216 13:13:29.255059 23592 net.cpp:397] push1 -> ip1 (in-place)
I0216 13:13:29.255075 23592 net.cpp:150] Setting up push1
I0216 13:13:29.255085 23592 net.cpp:157] Top shape: 100 500 (50000)
I0216 13:13:29.255092 23592 net.cpp:165] Memory required for data: 36646000
I0216 13:13:29.255098 23592 layer_factory.hpp:77] Creating layer ip2
I0216 13:13:29.255117 23592 net.cpp:106] Creating Layer ip2
I0216 13:13:29.255127 23592 net.cpp:454] ip2 <- ip1
I0216 13:13:29.255139 23592 net.cpp:411] ip2 -> ip2
I0216 13:13:29.255350 23592 net.cpp:150] Setting up ip2
I0216 13:13:29.255369 23592 net.cpp:157] Top shape: 100 10 (1000)
I0216 13:13:29.255378 23592 net.cpp:165] Memory required for data: 36650000
I0216 13:13:29.255398 23592 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0216 13:13:29.255412 23592 net.cpp:106] Creating Layer ip2_ip2_0_split
I0216 13:13:29.255419 23592 net.cpp:454] ip2_ip2_0_split <- ip2
I0216 13:13:29.255429 23592 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0216 13:13:29.255442 23592 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0216 13:13:29.255457 23592 net.cpp:150] Setting up ip2_ip2_0_split
I0216 13:13:29.255467 23592 net.cpp:157] Top shape: 100 10 (1000)
I0216 13:13:29.255476 23592 net.cpp:157] Top shape: 100 10 (1000)
I0216 13:13:29.255484 23592 net.cpp:165] Memory required for data: 36658000
I0216 13:13:29.255491 23592 layer_factory.hpp:77] Creating layer accuracy
I0216 13:13:29.255502 23592 net.cpp:106] Creating Layer accuracy
I0216 13:13:29.255511 23592 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I0216 13:13:29.255518 23592 net.cpp:454] accuracy <- label_cifar_1_split_0
I0216 13:13:29.255532 23592 net.cpp:411] accuracy -> accuracy
I0216 13:13:29.255549 23592 net.cpp:150] Setting up accuracy
I0216 13:13:29.255561 23592 net.cpp:157] Top shape: (1)
I0216 13:13:29.255569 23592 net.cpp:165] Memory required for data: 36658004
I0216 13:13:29.255576 23592 layer_factory.hpp:77] Creating layer loss
I0216 13:13:29.255587 23592 net.cpp:106] Creating Layer loss
I0216 13:13:29.255595 23592 net.cpp:454] loss <- ip2_ip2_0_split_1
I0216 13:13:29.255604 23592 net.cpp:454] loss <- label_cifar_1_split_1
I0216 13:13:29.255614 23592 net.cpp:411] loss -> loss
I0216 13:13:29.255628 23592 layer_factory.hpp:77] Creating layer loss
I0216 13:13:29.256690 23592 net.cpp:150] Setting up loss
I0216 13:13:29.256717 23592 net.cpp:157] Top shape: (1)
I0216 13:13:29.256727 23592 net.cpp:160]     with loss weight 1
I0216 13:13:29.256742 23592 net.cpp:165] Memory required for data: 36658008
I0216 13:13:29.256750 23592 net.cpp:226] loss needs backward computation.
I0216 13:13:29.256758 23592 net.cpp:228] accuracy does not need backward computation.
I0216 13:13:29.256767 23592 net.cpp:226] ip2_ip2_0_split needs backward computation.
I0216 13:13:29.256775 23592 net.cpp:226] ip2 needs backward computation.
I0216 13:13:29.256784 23592 net.cpp:226] push1 needs backward computation.
I0216 13:13:29.256791 23592 net.cpp:226] relu4 needs backward computation.
I0216 13:13:29.256798 23592 net.cpp:226] ip1 needs backward computation.
I0216 13:13:29.256805 23592 net.cpp:226] pool3 needs backward computation.
I0216 13:13:29.256814 23592 net.cpp:226] relu3 needs backward computation.
I0216 13:13:29.256820 23592 net.cpp:226] conv3 needs backward computation.
I0216 13:13:29.256829 23592 net.cpp:226] norm2 needs backward computation.
I0216 13:13:29.256835 23592 net.cpp:226] pool2 needs backward computation.
I0216 13:13:29.256842 23592 net.cpp:226] relu2 needs backward computation.
I0216 13:13:29.256850 23592 net.cpp:226] conv2 needs backward computation.
I0216 13:13:29.256856 23592 net.cpp:226] norm1 needs backward computation.
I0216 13:13:29.256863 23592 net.cpp:226] relu1 needs backward computation.
I0216 13:13:29.256880 23592 net.cpp:226] pool1 needs backward computation.
I0216 13:13:29.256901 23592 net.cpp:226] conv1 needs backward computation.
I0216 13:13:29.256911 23592 net.cpp:228] label_cifar_1_split does not need backward computation.
I0216 13:13:29.256917 23592 net.cpp:228] cifar does not need backward computation.
I0216 13:13:29.256924 23592 net.cpp:270] This network produces output accuracy
I0216 13:13:29.256932 23592 net.cpp:270] This network produces output loss
I0216 13:13:29.256953 23592 net.cpp:283] Network initialization done.
I0216 13:13:29.257093 23592 solver.cpp:60] Solver scaffolding done.
I0216 13:13:29.257150 23592 caffe.cpp:212] Starting Optimization
I0216 13:13:29.257164 23592 solver.cpp:288] Solving CIFAR10_full
I0216 13:13:29.257169 23592 solver.cpp:289] Learning Rate Policy: fixed
I0216 13:13:29.258098 23592 solver.cpp:341] Iteration 0, Testing net (#0)
I0216 13:14:01.931062 23592 solver.cpp:409]     Test net output #0: accuracy = 0.1014
I0216 13:14:01.931140 23592 solver.cpp:409]     Test net output #1: loss = 2.30258 (* 1 = 2.30258 loss)
I0216 13:14:02.739500 23592 solver.cpp:237] Iteration 0, loss = 2.30258
I0216 13:14:02.739542 23592 solver.cpp:253]     Train net output #0: loss = 2.30258 (* 1 = 2.30258 loss)
I0216 13:14:02.739552 23592 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I0216 13:16:33.321692 23592 solver.cpp:237] Iteration 200, loss = 2.30269
I0216 13:16:33.321776 23592 solver.cpp:253]     Train net output #0: loss = 2.30269 (* 1 = 2.30269 loss)
I0216 13:16:33.321786 23592 sgd_solver.cpp:106] Iteration 200, lr = 0.001
I0216 13:19:01.798528 23592 solver.cpp:237] Iteration 400, loss = 2.30281
I0216 13:19:01.798612 23592 solver.cpp:253]     Train net output #0: loss = 2.30281 (* 1 = 2.30281 loss)
I0216 13:19:01.798622 23592 sgd_solver.cpp:106] Iteration 400, lr = 0.001
I0216 13:21:32.647438 23592 solver.cpp:237] Iteration 600, loss = 2.30218
I0216 13:21:32.647508 23592 solver.cpp:253]     Train net output #0: loss = 2.30218 (* 1 = 2.30218 loss)
I0216 13:21:32.647517 23592 sgd_solver.cpp:106] Iteration 600, lr = 0.001
I0216 13:24:03.419373 23592 solver.cpp:237] Iteration 800, loss = 2.30212
I0216 13:24:03.419450 23592 solver.cpp:253]     Train net output #0: loss = 2.30212 (* 1 = 2.30212 loss)
I0216 13:24:03.419458 23592 sgd_solver.cpp:106] Iteration 800, lr = 0.001
I0216 13:26:32.125049 23592 solver.cpp:341] Iteration 1000, Testing net (#0)
I0216 13:27:04.919730 23592 solver.cpp:409]     Test net output #0: accuracy = 0.1127
I0216 13:27:04.919812 23592 solver.cpp:409]     Test net output #1: loss = 2.29865 (* 1 = 2.29865 loss)
I0216 13:27:05.623411 23592 solver.cpp:237] Iteration 1000, loss = 2.28236
I0216 13:27:05.623461 23592 solver.cpp:253]     Train net output #0: loss = 2.28236 (* 1 = 2.28236 loss)
I0216 13:27:05.623469 23592 sgd_solver.cpp:106] Iteration 1000, lr = 0.001
I0216 13:29:36.429077 23592 solver.cpp:237] Iteration 1200, loss = 2.1504
I0216 13:29:36.429160 23592 solver.cpp:253]     Train net output #0: loss = 2.1504 (* 1 = 2.1504 loss)
I0216 13:29:36.429169 23592 sgd_solver.cpp:106] Iteration 1200, lr = 0.001
I0216 13:32:07.970475 23592 solver.cpp:237] Iteration 1400, loss = 1.80497
I0216 13:32:07.970641 23592 solver.cpp:253]     Train net output #0: loss = 1.80497 (* 1 = 1.80497 loss)
I0216 13:32:07.970671 23592 sgd_solver.cpp:106] Iteration 1400, lr = 0.001
I0216 13:34:39.096895 23592 solver.cpp:237] Iteration 1600, loss = 1.7643
I0216 13:34:39.096973 23592 solver.cpp:253]     Train net output #0: loss = 1.7643 (* 1 = 1.7643 loss)
I0216 13:34:39.096982 23592 sgd_solver.cpp:106] Iteration 1600, lr = 0.001
I0216 13:37:09.436959 23592 solver.cpp:237] Iteration 1800, loss = 1.56166
I0216 13:37:09.437063 23592 solver.cpp:253]     Train net output #0: loss = 1.56166 (* 1 = 1.56166 loss)
I0216 13:37:09.437073 23592 sgd_solver.cpp:106] Iteration 1800, lr = 0.001
I0216 13:39:41.119271 23592 solver.cpp:341] Iteration 2000, Testing net (#0)
I0216 13:40:15.540199 23592 solver.cpp:409]     Test net output #0: accuracy = 0.1974
I0216 13:40:15.540290 23592 solver.cpp:409]     Test net output #1: loss = 2.08571 (* 1 = 2.08571 loss)
I0216 13:40:16.341403 23592 solver.cpp:237] Iteration 2000, loss = 1.60444
I0216 13:40:16.341447 23592 solver.cpp:253]     Train net output #0: loss = 1.60444 (* 1 = 1.60444 loss)
I0216 13:40:16.341455 23592 sgd_solver.cpp:106] Iteration 2000, lr = 0.001
I0216 13:42:46.085590 23592 solver.cpp:237] Iteration 2200, loss = 1.54872
I0216 13:42:46.085676 23592 solver.cpp:253]     Train net output #0: loss = 1.54872 (* 1 = 1.54872 loss)
I0216 13:42:46.085686 23592 sgd_solver.cpp:106] Iteration 2200, lr = 0.001
I0216 13:45:09.130898 23592 solver.cpp:237] Iteration 2400, loss = 1.25093
I0216 13:45:09.130986 23592 solver.cpp:253]     Train net output #0: loss = 1.25093 (* 1 = 1.25093 loss)
I0216 13:45:09.130997 23592 sgd_solver.cpp:106] Iteration 2400, lr = 0.001
I0216 13:47:34.013236 23592 solver.cpp:237] Iteration 2600, loss = 1.43428
I0216 13:47:34.013324 23592 solver.cpp:253]     Train net output #0: loss = 1.43428 (* 1 = 1.43428 loss)
I0216 13:47:34.013334 23592 sgd_solver.cpp:106] Iteration 2600, lr = 0.001
I0216 13:49:58.373782 23592 solver.cpp:237] Iteration 2800, loss = 1.33125
I0216 13:49:58.373875 23592 solver.cpp:253]     Train net output #0: loss = 1.33125 (* 1 = 1.33125 loss)
I0216 13:49:58.373885 23592 sgd_solver.cpp:106] Iteration 2800, lr = 0.001
I0216 13:52:30.727478 23592 solver.cpp:341] Iteration 3000, Testing net (#0)
I0216 13:53:04.743417 23592 solver.cpp:409]     Test net output #0: accuracy = 0.2414
I0216 13:53:04.743512 23592 solver.cpp:409]     Test net output #1: loss = 1.98053 (* 1 = 1.98053 loss)
I0216 13:53:05.507907 23592 solver.cpp:237] Iteration 3000, loss = 1.2678
I0216 13:53:05.507951 23592 solver.cpp:253]     Train net output #0: loss = 1.2678 (* 1 = 1.2678 loss)
I0216 13:53:05.507959 23592 sgd_solver.cpp:106] Iteration 3000, lr = 0.001
I0216 13:55:40.077460 23592 solver.cpp:237] Iteration 3200, loss = 1.30801
I0216 13:55:40.077544 23592 solver.cpp:253]     Train net output #0: loss = 1.30801 (* 1 = 1.30801 loss)
I0216 13:55:40.077553 23592 sgd_solver.cpp:106] Iteration 3200, lr = 0.001
I0216 13:58:13.891840 23592 solver.cpp:237] Iteration 3400, loss = 1.07989
I0216 13:58:13.891916 23592 solver.cpp:253]     Train net output #0: loss = 1.07989 (* 1 = 1.07989 loss)
I0216 13:58:13.891926 23592 sgd_solver.cpp:106] Iteration 3400, lr = 0.001
I0216 14:00:44.933776 23592 solver.cpp:237] Iteration 3600, loss = 1.21508
I0216 14:00:44.933858 23592 solver.cpp:253]     Train net output #0: loss = 1.21508 (* 1 = 1.21508 loss)
I0216 14:00:44.933868 23592 sgd_solver.cpp:106] Iteration 3600, lr = 0.001
I0216 14:03:21.555171 23592 solver.cpp:237] Iteration 3800, loss = 1.16318
I0216 14:03:21.555253 23592 solver.cpp:253]     Train net output #0: loss = 1.16318 (* 1 = 1.16318 loss)
I0216 14:03:21.555263 23592 sgd_solver.cpp:106] Iteration 3800, lr = 0.001
I0216 14:05:56.269433 23592 solver.cpp:341] Iteration 4000, Testing net (#0)
I0216 14:06:30.077962 23592 solver.cpp:409]     Test net output #0: accuracy = 0.2825
I0216 14:06:30.078042 23592 solver.cpp:409]     Test net output #1: loss = 1.89002 (* 1 = 1.89002 loss)
I0216 14:06:30.804250 23592 solver.cpp:237] Iteration 4000, loss = 1.09387
I0216 14:06:30.804297 23592 solver.cpp:253]     Train net output #0: loss = 1.09387 (* 1 = 1.09387 loss)
I0216 14:06:30.804306 23592 sgd_solver.cpp:106] Iteration 4000, lr = 0.001
I0216 14:09:06.083400 23592 solver.cpp:237] Iteration 4200, loss = 1.21722
I0216 14:09:06.083511 23592 solver.cpp:253]     Train net output #0: loss = 1.21722 (* 1 = 1.21722 loss)
I0216 14:09:06.083529 23592 sgd_solver.cpp:106] Iteration 4200, lr = 0.001
I0216 14:11:42.036108 23592 solver.cpp:237] Iteration 4400, loss = 0.96608
I0216 14:11:42.036212 23592 solver.cpp:253]     Train net output #0: loss = 0.96608 (* 1 = 0.96608 loss)
I0216 14:11:42.036563 23592 sgd_solver.cpp:106] Iteration 4400, lr = 0.001
I0216 14:14:17.105880 23592 solver.cpp:237] Iteration 4600, loss = 1.06419
I0216 14:14:17.105995 23592 solver.cpp:253]     Train net output #0: loss = 1.06419 (* 1 = 1.06419 loss)
I0216 14:14:17.106009 23592 sgd_solver.cpp:106] Iteration 4600, lr = 0.001
I0216 14:16:51.319607 23592 solver.cpp:237] Iteration 4800, loss = 1.00579
I0216 14:16:51.319725 23592 solver.cpp:253]     Train net output #0: loss = 1.00579 (* 1 = 1.00579 loss)
I0216 14:16:51.319742 23592 sgd_solver.cpp:106] Iteration 4800, lr = 0.001
I0216 14:19:24.810166 23592 solver.cpp:341] Iteration 5000, Testing net (#0)
I0216 14:19:57.760365 23592 solver.cpp:409]     Test net output #0: accuracy = 0.3203
I0216 14:19:57.760442 23592 solver.cpp:409]     Test net output #1: loss = 1.7963 (* 1 = 1.7963 loss)
I0216 14:19:58.482635 23592 solver.cpp:237] Iteration 5000, loss = 0.933785
I0216 14:19:58.482684 23592 solver.cpp:253]     Train net output #0: loss = 0.933785 (* 1 = 0.933785 loss)
I0216 14:19:58.482693 23592 sgd_solver.cpp:106] Iteration 5000, lr = 0.001
I0216 14:22:32.941373 23592 solver.cpp:237] Iteration 5200, loss = 1.09494
I0216 14:22:32.941449 23592 solver.cpp:253]     Train net output #0: loss = 1.09494 (* 1 = 1.09494 loss)
I0216 14:22:32.941463 23592 sgd_solver.cpp:106] Iteration 5200, lr = 0.001
I0216 14:25:06.291858 23592 solver.cpp:237] Iteration 5400, loss = 0.853154
I0216 14:25:06.291939 23592 solver.cpp:253]     Train net output #0: loss = 0.853154 (* 1 = 0.853154 loss)
I0216 14:25:06.291949 23592 sgd_solver.cpp:106] Iteration 5400, lr = 0.001
I0216 14:27:41.351605 23592 solver.cpp:237] Iteration 5600, loss = 0.960057
I0216 14:27:41.351696 23592 solver.cpp:253]     Train net output #0: loss = 0.960057 (* 1 = 0.960057 loss)
I0216 14:27:41.351706 23592 sgd_solver.cpp:106] Iteration 5600, lr = 0.001
I0216 14:30:16.505507 23592 solver.cpp:237] Iteration 5800, loss = 0.882267
I0216 14:30:16.505604 23592 solver.cpp:253]     Train net output #0: loss = 0.882267 (* 1 = 0.882267 loss)
I0216 14:30:16.505614 23592 sgd_solver.cpp:106] Iteration 5800, lr = 0.001
I0216 14:32:52.447123 23592 solver.cpp:341] Iteration 6000, Testing net (#0)
I0216 14:33:27.043467 23592 solver.cpp:409]     Test net output #0: accuracy = 0.3535
I0216 14:33:27.043546 23592 solver.cpp:409]     Test net output #1: loss = 1.71308 (* 1 = 1.71308 loss)
I0216 14:33:27.973533 23592 solver.cpp:237] Iteration 6000, loss = 0.82845
I0216 14:33:27.973577 23592 solver.cpp:253]     Train net output #0: loss = 0.82845 (* 1 = 0.82845 loss)
I0216 14:33:27.973584 23592 sgd_solver.cpp:106] Iteration 6000, lr = 0.001
I0216 14:36:00.324219 23592 solver.cpp:237] Iteration 6200, loss = 0.985608
I0216 14:36:00.324300 23592 solver.cpp:253]     Train net output #0: loss = 0.985608 (* 1 = 0.985608 loss)
I0216 14:36:00.324309 23592 sgd_solver.cpp:106] Iteration 6200, lr = 0.001
I0216 14:38:29.038149 23592 solver.cpp:237] Iteration 6400, loss = 0.795405
I0216 14:38:29.038231 23592 solver.cpp:253]     Train net output #0: loss = 0.795405 (* 1 = 0.795405 loss)
I0216 14:38:29.038240 23592 sgd_solver.cpp:106] Iteration 6400, lr = 0.001
I0216 14:41:05.118955 23592 solver.cpp:237] Iteration 6600, loss = 0.864771
I0216 14:41:05.119035 23592 solver.cpp:253]     Train net output #0: loss = 0.864771 (* 1 = 0.864771 loss)
I0216 14:41:05.119043 23592 sgd_solver.cpp:106] Iteration 6600, lr = 0.001
I0216 14:43:38.090015 23592 solver.cpp:237] Iteration 6800, loss = 0.779524
I0216 14:43:38.090090 23592 solver.cpp:253]     Train net output #0: loss = 0.779524 (* 1 = 0.779524 loss)
I0216 14:43:38.090098 23592 sgd_solver.cpp:106] Iteration 6800, lr = 0.001
I0216 14:46:09.849653 23592 solver.cpp:341] Iteration 7000, Testing net (#0)
I0216 14:46:43.650403 23592 solver.cpp:409]     Test net output #0: accuracy = 0.3853
I0216 14:46:43.650475 23592 solver.cpp:409]     Test net output #1: loss = 1.63058 (* 1 = 1.63058 loss)
I0216 14:46:44.363351 23592 solver.cpp:237] Iteration 7000, loss = 0.752609
I0216 14:46:44.363395 23592 solver.cpp:253]     Train net output #0: loss = 0.752609 (* 1 = 0.752609 loss)
I0216 14:46:44.363404 23592 sgd_solver.cpp:106] Iteration 7000, lr = 0.001
I0216 14:49:16.418403 23592 solver.cpp:237] Iteration 7200, loss = 0.880897
I0216 14:49:16.418524 23592 solver.cpp:253]     Train net output #0: loss = 0.880897 (* 1 = 0.880897 loss)
I0216 14:49:16.418537 23592 sgd_solver.cpp:106] Iteration 7200, lr = 0.001
I0216 14:51:53.465420 23592 solver.cpp:237] Iteration 7400, loss = 0.747909
I0216 14:51:53.465548 23592 solver.cpp:253]     Train net output #0: loss = 0.747909 (* 1 = 0.747909 loss)
I0216 14:51:53.465556 23592 sgd_solver.cpp:106] Iteration 7400, lr = 0.001
I0216 14:54:24.219421 23592 solver.cpp:237] Iteration 7600, loss = 0.803397
I0216 14:54:24.219519 23592 solver.cpp:253]     Train net output #0: loss = 0.803397 (* 1 = 0.803397 loss)
I0216 14:54:24.219537 23592 sgd_solver.cpp:106] Iteration 7600, lr = 0.001
I0216 14:56:58.559819 23592 solver.cpp:237] Iteration 7800, loss = 0.705958
I0216 14:56:58.559909 23592 solver.cpp:253]     Train net output #0: loss = 0.705958 (* 1 = 0.705958 loss)
I0216 14:56:58.559921 23592 sgd_solver.cpp:106] Iteration 7800, lr = 0.001
I0216 14:59:27.911715 23592 solver.cpp:341] Iteration 8000, Testing net (#0)
I0216 15:00:00.957495 23592 solver.cpp:409]     Test net output #0: accuracy = 0.4233
I0216 15:00:00.957571 23592 solver.cpp:409]     Test net output #1: loss = 1.54393 (* 1 = 1.54393 loss)
I0216 15:00:01.670846 23592 solver.cpp:237] Iteration 8000, loss = 0.68321
I0216 15:00:01.670893 23592 solver.cpp:253]     Train net output #0: loss = 0.68321 (* 1 = 0.68321 loss)
I0216 15:00:01.670902 23592 sgd_solver.cpp:106] Iteration 8000, lr = 0.001
I0216 15:02:34.621856 23592 solver.cpp:237] Iteration 8200, loss = 0.774327
I0216 15:02:34.621930 23592 solver.cpp:253]     Train net output #0: loss = 0.774327 (* 1 = 0.774327 loss)
I0216 15:02:34.621940 23592 sgd_solver.cpp:106] Iteration 8200, lr = 0.001
I0216 15:05:08.760530 23592 solver.cpp:237] Iteration 8400, loss = 0.6975
I0216 15:05:08.760607 23592 solver.cpp:253]     Train net output #0: loss = 0.6975 (* 1 = 0.6975 loss)
I0216 15:05:08.760615 23592 sgd_solver.cpp:106] Iteration 8400, lr = 0.001
I0216 15:07:43.582628 23592 solver.cpp:237] Iteration 8600, loss = 0.758715
I0216 15:07:43.582713 23592 solver.cpp:253]     Train net output #0: loss = 0.758715 (* 1 = 0.758715 loss)
I0216 15:07:43.582723 23592 sgd_solver.cpp:106] Iteration 8600, lr = 0.001
I0216 15:10:18.559520 23592 solver.cpp:237] Iteration 8800, loss = 0.667608
I0216 15:10:18.559597 23592 solver.cpp:253]     Train net output #0: loss = 0.667608 (* 1 = 0.667608 loss)
I0216 15:10:18.559605 23592 sgd_solver.cpp:106] Iteration 8800, lr = 0.001
I0216 15:12:52.407485 23592 solver.cpp:341] Iteration 9000, Testing net (#0)
I0216 15:13:26.602684 23592 solver.cpp:409]     Test net output #0: accuracy = 0.4459
I0216 15:13:26.602767 23592 solver.cpp:409]     Test net output #1: loss = 1.46694 (* 1 = 1.46694 loss)
I0216 15:13:27.374819 23592 solver.cpp:237] Iteration 9000, loss = 0.618484
I0216 15:13:27.374861 23592 solver.cpp:253]     Train net output #0: loss = 0.618484 (* 1 = 0.618484 loss)
I0216 15:13:27.374869 23592 sgd_solver.cpp:106] Iteration 9000, lr = 0.001
I0216 15:16:02.236127 23592 solver.cpp:237] Iteration 9200, loss = 0.694601
I0216 15:16:02.236207 23592 solver.cpp:253]     Train net output #0: loss = 0.694601 (* 1 = 0.694601 loss)
I0216 15:16:02.236217 23592 sgd_solver.cpp:106] Iteration 9200, lr = 0.001
I0216 15:18:34.618558 23592 solver.cpp:237] Iteration 9400, loss = 0.661716
I0216 15:18:34.618639 23592 solver.cpp:253]     Train net output #0: loss = 0.661716 (* 1 = 0.661716 loss)
I0216 15:18:34.618659 23592 sgd_solver.cpp:106] Iteration 9400, lr = 0.001
I0216 15:21:06.973470 23592 solver.cpp:237] Iteration 9600, loss = 0.720423
I0216 15:21:06.973532 23592 solver.cpp:253]     Train net output #0: loss = 0.720423 (* 1 = 0.720423 loss)
I0216 15:21:06.973541 23592 sgd_solver.cpp:106] Iteration 9600, lr = 0.001
I0216 15:23:40.610869 23592 solver.cpp:237] Iteration 9800, loss = 0.619608
I0216 15:23:40.610952 23592 solver.cpp:253]     Train net output #0: loss = 0.619608 (* 1 = 0.619608 loss)
I0216 15:23:40.610961 23592 sgd_solver.cpp:106] Iteration 9800, lr = 0.001
I0216 15:26:11.882984 23592 solver.cpp:472] Snapshotting to HDF5 file examples/cifar10/jungwoo_full_iter_10000.caffemodel.h5
I0216 15:26:12.446071 23592 sgd_solver.cpp:279] Snapshotting solver state to HDF5 file examples/cifar10/jungwoo_full_iter_10000.solverstate.h5
I0216 15:26:12.448688 23592 solver.cpp:341] Iteration 10000, Testing net (#0)
I0216 15:26:46.500751 23592 solver.cpp:409]     Test net output #0: accuracy = 0.4746
I0216 15:26:46.500834 23592 solver.cpp:409]     Test net output #1: loss = 1.39316 (* 1 = 1.39316 loss)
I0216 15:26:47.394768 23592 solver.cpp:237] Iteration 10000, loss = 0.570962
I0216 15:26:47.394822 23592 solver.cpp:253]     Train net output #0: loss = 0.570962 (* 1 = 0.570962 loss)
I0216 15:26:47.394834 23592 sgd_solver.cpp:106] Iteration 10000, lr = 0.001
I0216 15:29:20.444455 23592 solver.cpp:237] Iteration 10200, loss = 0.632918
I0216 15:29:20.444537 23592 solver.cpp:253]     Train net output #0: loss = 0.632918 (* 1 = 0.632918 loss)
I0216 15:29:20.444547 23592 sgd_solver.cpp:106] Iteration 10200, lr = 0.001
I0216 15:31:51.774787 23592 solver.cpp:237] Iteration 10400, loss = 0.620671
I0216 15:31:51.774865 23592 solver.cpp:253]     Train net output #0: loss = 0.620671 (* 1 = 0.620671 loss)
I0216 15:31:51.774874 23592 sgd_solver.cpp:106] Iteration 10400, lr = 0.001
I0216 15:34:23.831254 23592 solver.cpp:237] Iteration 10600, loss = 0.662606
I0216 15:34:23.831333 23592 solver.cpp:253]     Train net output #0: loss = 0.662606 (* 1 = 0.662606 loss)
I0216 15:34:23.831342 23592 sgd_solver.cpp:106] Iteration 10600, lr = 0.001
I0216 15:36:56.314038 23592 solver.cpp:237] Iteration 10800, loss = 0.590074
I0216 15:36:56.314137 23592 solver.cpp:253]     Train net output #0: loss = 0.590074 (* 1 = 0.590074 loss)
I0216 15:36:56.314151 23592 sgd_solver.cpp:106] Iteration 10800, lr = 0.001
I0216 15:39:27.089505 23592 solver.cpp:341] Iteration 11000, Testing net (#0)
I0216 15:40:00.562458 23592 solver.cpp:409]     Test net output #0: accuracy = 0.5067
I0216 15:40:00.562517 23592 solver.cpp:409]     Test net output #1: loss = 1.31748 (* 1 = 1.31748 loss)
I0216 15:40:01.286959 23592 solver.cpp:237] Iteration 11000, loss = 0.548011
I0216 15:40:01.287001 23592 solver.cpp:253]     Train net output #0: loss = 0.548011 (* 1 = 0.548011 loss)
I0216 15:40:01.287009 23592 sgd_solver.cpp:106] Iteration 11000, lr = 0.001
I0216 15:42:33.715219 23592 solver.cpp:237] Iteration 11200, loss = 0.578437
I0216 15:42:33.715294 23592 solver.cpp:253]     Train net output #0: loss = 0.578437 (* 1 = 0.578437 loss)
I0216 15:42:33.715303 23592 sgd_solver.cpp:106] Iteration 11200, lr = 0.001
I0216 15:45:03.462349 23592 solver.cpp:237] Iteration 11400, loss = 0.594386
I0216 15:45:03.462429 23592 solver.cpp:253]     Train net output #0: loss = 0.594386 (* 1 = 0.594386 loss)
I0216 15:45:03.462438 23592 sgd_solver.cpp:106] Iteration 11400, lr = 0.001
I0216 15:47:32.429666 23592 solver.cpp:237] Iteration 11600, loss = 0.616205
I0216 15:47:32.429744 23592 solver.cpp:253]     Train net output #0: loss = 0.616205 (* 1 = 0.616205 loss)
I0216 15:47:32.429754 23592 sgd_solver.cpp:106] Iteration 11600, lr = 0.001
I0216 15:50:03.259003 23592 solver.cpp:237] Iteration 11800, loss = 0.550501
I0216 15:50:03.259090 23592 solver.cpp:253]     Train net output #0: loss = 0.550501 (* 1 = 0.550501 loss)
I0216 15:50:03.259457 23592 sgd_solver.cpp:106] Iteration 11800, lr = 0.001
I0216 15:52:37.461129 23592 solver.cpp:341] Iteration 12000, Testing net (#0)
I0216 15:53:11.117475 23592 solver.cpp:409]     Test net output #0: accuracy = 0.5331
I0216 15:53:11.117553 23592 solver.cpp:409]     Test net output #1: loss = 1.25693 (* 1 = 1.25693 loss)
I0216 15:53:11.855454 23592 solver.cpp:237] Iteration 12000, loss = 0.521864
I0216 15:53:11.855496 23592 solver.cpp:253]     Train net output #0: loss = 0.521864 (* 1 = 0.521864 loss)
I0216 15:53:11.855504 23592 sgd_solver.cpp:106] Iteration 12000, lr = 0.001
I0216 15:55:45.619712 23592 solver.cpp:237] Iteration 12200, loss = 0.525618
I0216 15:55:45.619806 23592 solver.cpp:253]     Train net output #0: loss = 0.525618 (* 1 = 0.525618 loss)
I0216 15:55:45.619815 23592 sgd_solver.cpp:106] Iteration 12200, lr = 0.001
I0216 15:58:20.493844 23592 solver.cpp:237] Iteration 12400, loss = 0.581681
I0216 15:58:20.493943 23592 solver.cpp:253]     Train net output #0: loss = 0.581681 (* 1 = 0.581681 loss)
I0216 15:58:20.493952 23592 sgd_solver.cpp:106] Iteration 12400, lr = 0.001
I0216 16:01:04.186581 23592 solver.cpp:237] Iteration 12600, loss = 0.562277
I0216 16:01:04.186683 23592 solver.cpp:253]     Train net output #0: loss = 0.562277 (* 1 = 0.562277 loss)
I0216 16:01:04.186692 23592 sgd_solver.cpp:106] Iteration 12600, lr = 0.001
I0216 16:03:47.223115 23592 solver.cpp:237] Iteration 12800, loss = 0.493923
I0216 16:03:47.223188 23592 solver.cpp:253]     Train net output #0: loss = 0.493923 (* 1 = 0.493923 loss)
I0216 16:03:47.223197 23592 sgd_solver.cpp:106] Iteration 12800, lr = 0.001
I0216 16:06:24.105362 23592 solver.cpp:341] Iteration 13000, Testing net (#0)
I0216 16:07:01.332386 23592 solver.cpp:409]     Test net output #0: accuracy = 0.5593
I0216 16:07:01.332478 23592 solver.cpp:409]     Test net output #1: loss = 1.19544 (* 1 = 1.19544 loss)
I0216 16:07:02.144726 23592 solver.cpp:237] Iteration 13000, loss = 0.481123
I0216 16:07:02.144770 23592 solver.cpp:253]     Train net output #0: loss = 0.481123 (* 1 = 0.481123 loss)
I0216 16:07:02.144778 23592 sgd_solver.cpp:106] Iteration 13000, lr = 0.001
I0216 16:09:36.469799 23592 solver.cpp:237] Iteration 13200, loss = 0.4776
I0216 16:09:36.469873 23592 solver.cpp:253]     Train net output #0: loss = 0.4776 (* 1 = 0.4776 loss)
I0216 16:09:36.469882 23592 sgd_solver.cpp:106] Iteration 13200, lr = 0.001
I0216 16:12:17.968068 23592 solver.cpp:237] Iteration 13400, loss = 0.567031
I0216 16:12:17.968143 23592 solver.cpp:253]     Train net output #0: loss = 0.567031 (* 1 = 0.567031 loss)
I0216 16:12:17.968153 23592 sgd_solver.cpp:106] Iteration 13400, lr = 0.001
I0216 16:14:55.665324 23592 solver.cpp:237] Iteration 13600, loss = 0.517607
I0216 16:14:55.665402 23592 solver.cpp:253]     Train net output #0: loss = 0.517607 (* 1 = 0.517607 loss)
I0216 16:14:55.665411 23592 sgd_solver.cpp:106] Iteration 13600, lr = 0.001
I0216 16:17:32.465864 23592 solver.cpp:237] Iteration 13800, loss = 0.45875
I0216 16:17:32.465950 23592 solver.cpp:253]     Train net output #0: loss = 0.45875 (* 1 = 0.45875 loss)
I0216 16:17:32.465958 23592 sgd_solver.cpp:106] Iteration 13800, lr = 0.001
I0216 16:20:12.645416 23592 solver.cpp:341] Iteration 14000, Testing net (#0)
I0216 16:20:47.805136 23592 solver.cpp:409]     Test net output #0: accuracy = 0.5912
I0216 16:20:47.805218 23592 solver.cpp:409]     Test net output #1: loss = 1.12074 (* 1 = 1.12074 loss)
I0216 16:20:48.533819 23592 solver.cpp:237] Iteration 14000, loss = 0.442075
I0216 16:20:48.533862 23592 solver.cpp:253]     Train net output #0: loss = 0.442075 (* 1 = 0.442075 loss)
I0216 16:20:48.533869 23592 sgd_solver.cpp:106] Iteration 14000, lr = 0.001
I0216 16:23:28.067636 23592 solver.cpp:237] Iteration 14200, loss = 0.439515
I0216 16:23:28.067708 23592 solver.cpp:253]     Train net output #0: loss = 0.439515 (* 1 = 0.439515 loss)
I0216 16:23:28.067718 23592 sgd_solver.cpp:106] Iteration 14200, lr = 0.001
I0216 16:26:08.889127 23592 solver.cpp:237] Iteration 14400, loss = 0.55642
I0216 16:26:08.889215 23592 solver.cpp:253]     Train net output #0: loss = 0.55642 (* 1 = 0.55642 loss)
I0216 16:26:08.889225 23592 sgd_solver.cpp:106] Iteration 14400, lr = 0.001
I0216 16:28:51.345772 23592 solver.cpp:237] Iteration 14600, loss = 0.480943
I0216 16:28:51.345867 23592 solver.cpp:253]     Train net output #0: loss = 0.480943 (* 1 = 0.480943 loss)
I0216 16:28:51.345878 23592 sgd_solver.cpp:106] Iteration 14600, lr = 0.001
I0216 16:31:23.710783 23592 solver.cpp:237] Iteration 14800, loss = 0.407934
I0216 16:31:23.710866 23592 solver.cpp:253]     Train net output #0: loss = 0.407934 (* 1 = 0.407934 loss)
I0216 16:31:23.710876 23592 sgd_solver.cpp:106] Iteration 14800, lr = 0.001
I0216 16:33:55.616150 23592 solver.cpp:341] Iteration 15000, Testing net (#0)
I0216 16:34:30.530185 23592 solver.cpp:409]     Test net output #0: accuracy = 0.6208
I0216 16:34:30.530283 23592 solver.cpp:409]     Test net output #1: loss = 1.05523 (* 1 = 1.05523 loss)
I0216 16:34:31.302652 23592 solver.cpp:237] Iteration 15000, loss = 0.425711
I0216 16:34:31.302696 23592 solver.cpp:253]     Train net output #0: loss = 0.425711 (* 1 = 0.425711 loss)
I0216 16:34:31.302703 23592 sgd_solver.cpp:106] Iteration 15000, lr = 0.001
I0216 16:37:02.756177 23592 solver.cpp:237] Iteration 15200, loss = 0.411513
I0216 16:37:02.758981 23592 solver.cpp:253]     Train net output #0: loss = 0.411513 (* 1 = 0.411513 loss)
I0216 16:37:02.759011 23592 sgd_solver.cpp:106] Iteration 15200, lr = 0.001
I0216 16:39:35.164485 23592 solver.cpp:237] Iteration 15400, loss = 0.521179
I0216 16:39:35.164589 23592 solver.cpp:253]     Train net output #0: loss = 0.521179 (* 1 = 0.521179 loss)
I0216 16:39:35.164607 23592 sgd_solver.cpp:106] Iteration 15400, lr = 0.001
I0216 16:42:06.169657 23592 solver.cpp:237] Iteration 15600, loss = 0.452261
I0216 16:42:06.169739 23592 solver.cpp:253]     Train net output #0: loss = 0.452261 (* 1 = 0.452261 loss)
I0216 16:42:06.169749 23592 sgd_solver.cpp:106] Iteration 15600, lr = 0.001
I0216 16:44:41.793993 23592 solver.cpp:237] Iteration 15800, loss = 0.407161
I0216 16:44:41.794121 23592 solver.cpp:253]     Train net output #0: loss = 0.407161 (* 1 = 0.407161 loss)
I0216 16:44:41.794137 23592 sgd_solver.cpp:106] Iteration 15800, lr = 0.001
I0216 16:47:15.908875 23592 solver.cpp:341] Iteration 16000, Testing net (#0)
I0216 16:47:49.613023 23592 solver.cpp:409]     Test net output #0: accuracy = 0.6492
I0216 16:47:49.613137 23592 solver.cpp:409]     Test net output #1: loss = 0.982582 (* 1 = 0.982582 loss)
I0216 16:47:50.435827 23592 solver.cpp:237] Iteration 16000, loss = 0.423432
I0216 16:47:50.435871 23592 solver.cpp:253]     Train net output #0: loss = 0.423432 (* 1 = 0.423432 loss)
I0216 16:47:50.435879 23592 sgd_solver.cpp:106] Iteration 16000, lr = 0.001
I0216 16:50:28.039535 23592 solver.cpp:237] Iteration 16200, loss = 0.367364
I0216 16:50:28.039645 23592 solver.cpp:253]     Train net output #0: loss = 0.367364 (* 1 = 0.367364 loss)
I0216 16:50:28.039657 23592 sgd_solver.cpp:106] Iteration 16200, lr = 0.001
I0216 16:53:03.271410 23592 solver.cpp:237] Iteration 16400, loss = 0.510147
I0216 16:53:03.271483 23592 solver.cpp:253]     Train net output #0: loss = 0.510147 (* 1 = 0.510147 loss)
I0216 16:53:03.271492 23592 sgd_solver.cpp:106] Iteration 16400, lr = 0.001
I0216 16:55:36.420203 23592 solver.cpp:237] Iteration 16600, loss = 0.437787
I0216 16:55:36.420300 23592 solver.cpp:253]     Train net output #0: loss = 0.437787 (* 1 = 0.437787 loss)
I0216 16:55:36.420310 23592 sgd_solver.cpp:106] Iteration 16600, lr = 0.001
I0216 16:58:13.578112 23592 solver.cpp:237] Iteration 16800, loss = 0.41817
I0216 16:58:13.578212 23592 solver.cpp:253]     Train net output #0: loss = 0.41817 (* 1 = 0.41817 loss)
I0216 16:58:13.578550 23592 sgd_solver.cpp:106] Iteration 16800, lr = 0.001
I0216 17:00:45.539207 23592 solver.cpp:341] Iteration 17000, Testing net (#0)
I0216 17:01:21.267817 23592 solver.cpp:409]     Test net output #0: accuracy = 0.6811
I0216 17:01:21.267891 23592 solver.cpp:409]     Test net output #1: loss = 0.908289 (* 1 = 0.908289 loss)
I0216 17:01:22.014811 23592 solver.cpp:237] Iteration 17000, loss = 0.40154
I0216 17:01:22.014856 23592 solver.cpp:253]     Train net output #0: loss = 0.40154 (* 1 = 0.40154 loss)
I0216 17:01:22.014864 23592 sgd_solver.cpp:106] Iteration 17000, lr = 0.001
I0216 17:03:55.852735 23592 solver.cpp:237] Iteration 17200, loss = 0.347912
I0216 17:03:55.852831 23592 solver.cpp:253]     Train net output #0: loss = 0.347912 (* 1 = 0.347912 loss)
I0216 17:03:55.852848 23592 sgd_solver.cpp:106] Iteration 17200, lr = 0.001
I0216 17:06:30.019336 23592 solver.cpp:237] Iteration 17400, loss = 0.482768
I0216 17:06:30.019435 23592 solver.cpp:253]     Train net output #0: loss = 0.482768 (* 1 = 0.482768 loss)
I0216 17:06:30.019448 23592 sgd_solver.cpp:106] Iteration 17400, lr = 0.001
I0216 17:09:01.100384 23592 solver.cpp:237] Iteration 17600, loss = 0.426468
I0216 17:09:01.100523 23592 solver.cpp:253]     Train net output #0: loss = 0.426468 (* 1 = 0.426468 loss)
I0216 17:09:01.100534 23592 sgd_solver.cpp:106] Iteration 17600, lr = 0.001
I0216 17:11:34.476583 23592 solver.cpp:237] Iteration 17800, loss = 0.389973
I0216 17:11:34.476662 23592 solver.cpp:253]     Train net output #0: loss = 0.389973 (* 1 = 0.389973 loss)
I0216 17:11:34.476671 23592 sgd_solver.cpp:106] Iteration 17800, lr = 0.001
I0216 17:14:09.646340 23592 solver.cpp:341] Iteration 18000, Testing net (#0)
I0216 17:14:42.859127 23592 solver.cpp:409]     Test net output #0: accuracy = 0.711
I0216 17:14:42.859201 23592 solver.cpp:409]     Test net output #1: loss = 0.844214 (* 1 = 0.844214 loss)
I0216 17:14:43.584517 23592 solver.cpp:237] Iteration 18000, loss = 0.370975
I0216 17:14:43.584559 23592 solver.cpp:253]     Train net output #0: loss = 0.370975 (* 1 = 0.370975 loss)
I0216 17:14:43.584568 23592 sgd_solver.cpp:106] Iteration 18000, lr = 0.001
I0216 17:17:16.384582 23592 solver.cpp:237] Iteration 18200, loss = 0.327637
I0216 17:17:16.384641 23592 solver.cpp:253]     Train net output #0: loss = 0.327637 (* 1 = 0.327637 loss)
I0216 17:17:16.384649 23592 sgd_solver.cpp:106] Iteration 18200, lr = 0.001
I0216 17:19:43.423916 23592 solver.cpp:237] Iteration 18400, loss = 0.465806
I0216 17:19:43.424015 23592 solver.cpp:253]     Train net output #0: loss = 0.465806 (* 1 = 0.465806 loss)
I0216 17:19:43.424027 23592 sgd_solver.cpp:106] Iteration 18400, lr = 0.001
I0216 17:22:14.848090 23592 solver.cpp:237] Iteration 18600, loss = 0.4018
I0216 17:22:14.848167 23592 solver.cpp:253]     Train net output #0: loss = 0.4018 (* 1 = 0.4018 loss)
I0216 17:22:14.848176 23592 sgd_solver.cpp:106] Iteration 18600, lr = 0.001
I0216 17:24:45.042310 23592 solver.cpp:237] Iteration 18800, loss = 0.379407
I0216 17:24:45.042407 23592 solver.cpp:253]     Train net output #0: loss = 0.379407 (* 1 = 0.379407 loss)
I0216 17:24:45.042420 23592 sgd_solver.cpp:106] Iteration 18800, lr = 0.001
I0216 17:27:14.583932 23592 solver.cpp:341] Iteration 19000, Testing net (#0)
I0216 17:27:47.774924 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7383
I0216 17:27:47.775002 23592 solver.cpp:409]     Test net output #1: loss = 0.77809 (* 1 = 0.77809 loss)
I0216 17:27:48.496706 23592 solver.cpp:237] Iteration 19000, loss = 0.339513
I0216 17:27:48.496747 23592 solver.cpp:253]     Train net output #0: loss = 0.339513 (* 1 = 0.339513 loss)
I0216 17:27:48.496755 23592 sgd_solver.cpp:106] Iteration 19000, lr = 0.001
I0216 17:30:18.652887 23592 solver.cpp:237] Iteration 19200, loss = 0.317929
I0216 17:30:18.652967 23592 solver.cpp:253]     Train net output #0: loss = 0.317929 (* 1 = 0.317929 loss)
I0216 17:30:18.652976 23592 sgd_solver.cpp:106] Iteration 19200, lr = 0.001
I0216 17:32:49.148623 23592 solver.cpp:237] Iteration 19400, loss = 0.454029
I0216 17:32:49.148696 23592 solver.cpp:253]     Train net output #0: loss = 0.454029 (* 1 = 0.454029 loss)
I0216 17:32:49.148705 23592 sgd_solver.cpp:106] Iteration 19400, lr = 0.001
I0216 17:35:21.772879 23592 solver.cpp:237] Iteration 19600, loss = 0.379176
I0216 17:35:21.772953 23592 solver.cpp:253]     Train net output #0: loss = 0.379176 (* 1 = 0.379176 loss)
I0216 17:35:21.772963 23592 sgd_solver.cpp:106] Iteration 19600, lr = 0.001
I0216 17:37:59.188685 23592 solver.cpp:237] Iteration 19800, loss = 0.352187
I0216 17:37:59.188829 23592 solver.cpp:253]     Train net output #0: loss = 0.352187 (* 1 = 0.352187 loss)
I0216 17:37:59.189388 23592 sgd_solver.cpp:106] Iteration 19800, lr = 0.001
I0216 17:40:35.138411 23592 solver.cpp:472] Snapshotting to HDF5 file examples/cifar10/jungwoo_full_iter_20000.caffemodel.h5
I0216 17:40:35.713028 23592 sgd_solver.cpp:279] Snapshotting solver state to HDF5 file examples/cifar10/jungwoo_full_iter_20000.solverstate.h5
I0216 17:40:35.715687 23592 solver.cpp:341] Iteration 20000, Testing net (#0)
I0216 17:41:11.450141 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7482
I0216 17:41:11.463991 23592 solver.cpp:409]     Test net output #1: loss = 0.769226 (* 1 = 0.769226 loss)
I0216 17:41:12.229265 23592 solver.cpp:237] Iteration 20000, loss = 0.331404
I0216 17:41:12.229315 23592 solver.cpp:253]     Train net output #0: loss = 0.331404 (* 1 = 0.331404 loss)
I0216 17:41:12.229323 23592 sgd_solver.cpp:106] Iteration 20000, lr = 0.001
I0216 17:43:51.305058 23592 solver.cpp:237] Iteration 20200, loss = 0.304572
I0216 17:43:51.305151 23592 solver.cpp:253]     Train net output #0: loss = 0.304572 (* 1 = 0.304572 loss)
I0216 17:43:51.305161 23592 sgd_solver.cpp:106] Iteration 20200, lr = 0.001
I0216 17:46:32.152093 23592 solver.cpp:237] Iteration 20400, loss = 0.429722
I0216 17:46:32.152174 23592 solver.cpp:253]     Train net output #0: loss = 0.429722 (* 1 = 0.429722 loss)
I0216 17:46:32.152184 23592 sgd_solver.cpp:106] Iteration 20400, lr = 0.001
I0216 17:49:13.602030 23592 solver.cpp:237] Iteration 20600, loss = 0.358257
I0216 17:49:13.602131 23592 solver.cpp:253]     Train net output #0: loss = 0.358257 (* 1 = 0.358257 loss)
I0216 17:49:13.602149 23592 sgd_solver.cpp:106] Iteration 20600, lr = 0.001
I0216 17:51:57.389636 23592 solver.cpp:237] Iteration 20800, loss = 0.346154
I0216 17:51:57.389734 23592 solver.cpp:253]     Train net output #0: loss = 0.346154 (* 1 = 0.346154 loss)
I0216 17:51:57.389750 23592 sgd_solver.cpp:106] Iteration 20800, lr = 0.001
I0216 17:54:36.733336 23592 solver.cpp:341] Iteration 21000, Testing net (#0)
I0216 17:55:12.779407 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7455
I0216 17:55:12.779526 23592 solver.cpp:409]     Test net output #1: loss = 0.778785 (* 1 = 0.778785 loss)
I0216 17:55:13.530858 23592 solver.cpp:237] Iteration 21000, loss = 0.307312
I0216 17:55:13.530903 23592 solver.cpp:253]     Train net output #0: loss = 0.307312 (* 1 = 0.307312 loss)
I0216 17:55:13.530911 23592 sgd_solver.cpp:106] Iteration 21000, lr = 0.001
I0216 17:57:48.620724 23592 solver.cpp:237] Iteration 21200, loss = 0.289546
I0216 17:57:48.620820 23592 solver.cpp:253]     Train net output #0: loss = 0.289546 (* 1 = 0.289546 loss)
I0216 17:57:48.620834 23592 sgd_solver.cpp:106] Iteration 21200, lr = 0.001
I0216 18:00:25.408367 23592 solver.cpp:237] Iteration 21400, loss = 0.38432
I0216 18:00:25.408470 23592 solver.cpp:253]     Train net output #0: loss = 0.38432 (* 1 = 0.38432 loss)
I0216 18:00:25.408479 23592 sgd_solver.cpp:106] Iteration 21400, lr = 0.001
I0216 18:03:01.545188 23592 solver.cpp:237] Iteration 21600, loss = 0.356824
I0216 18:03:01.545279 23592 solver.cpp:253]     Train net output #0: loss = 0.356824 (* 1 = 0.356824 loss)
I0216 18:03:01.545289 23592 sgd_solver.cpp:106] Iteration 21600, lr = 0.001
I0216 18:05:37.886975 23592 solver.cpp:237] Iteration 21800, loss = 0.336648
I0216 18:05:37.887081 23592 solver.cpp:253]     Train net output #0: loss = 0.336648 (* 1 = 0.336648 loss)
I0216 18:05:37.887095 23592 sgd_solver.cpp:106] Iteration 21800, lr = 0.001
I0216 18:08:15.689059 23592 solver.cpp:341] Iteration 22000, Testing net (#0)
I0216 18:08:50.711845 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7456
I0216 18:08:50.711931 23592 solver.cpp:409]     Test net output #1: loss = 0.78407 (* 1 = 0.78407 loss)
I0216 18:08:51.481359 23592 solver.cpp:237] Iteration 22000, loss = 0.294329
I0216 18:08:51.481405 23592 solver.cpp:253]     Train net output #0: loss = 0.294329 (* 1 = 0.294329 loss)
I0216 18:08:51.481415 23592 sgd_solver.cpp:106] Iteration 22000, lr = 0.001
I0216 18:11:31.101148 23592 solver.cpp:237] Iteration 22200, loss = 0.270785
I0216 18:11:31.101245 23592 solver.cpp:253]     Train net output #0: loss = 0.270785 (* 1 = 0.270785 loss)
I0216 18:11:31.101255 23592 sgd_solver.cpp:106] Iteration 22200, lr = 0.001
I0216 18:14:12.112404 23592 solver.cpp:237] Iteration 22400, loss = 0.356651
I0216 18:14:12.112506 23592 solver.cpp:253]     Train net output #0: loss = 0.356651 (* 1 = 0.356651 loss)
I0216 18:14:12.112517 23592 sgd_solver.cpp:106] Iteration 22400, lr = 0.001
I0216 18:16:48.825251 23592 solver.cpp:237] Iteration 22600, loss = 0.36658
I0216 18:16:48.830720 23592 solver.cpp:253]     Train net output #0: loss = 0.36658 (* 1 = 0.36658 loss)
I0216 18:16:48.830759 23592 sgd_solver.cpp:106] Iteration 22600, lr = 0.001
I0216 18:19:27.445344 23592 solver.cpp:237] Iteration 22800, loss = 0.311538
I0216 18:19:27.445437 23592 solver.cpp:253]     Train net output #0: loss = 0.311538 (* 1 = 0.311538 loss)
I0216 18:19:27.445447 23592 sgd_solver.cpp:106] Iteration 22800, lr = 0.001
I0216 18:22:00.991560 23592 solver.cpp:341] Iteration 23000, Testing net (#0)
I0216 18:22:36.969393 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7444
I0216 18:22:36.969485 23592 solver.cpp:409]     Test net output #1: loss = 0.79268 (* 1 = 0.79268 loss)
I0216 18:22:37.689849 23592 solver.cpp:237] Iteration 23000, loss = 0.324815
I0216 18:22:37.689893 23592 solver.cpp:253]     Train net output #0: loss = 0.324815 (* 1 = 0.324815 loss)
I0216 18:22:37.689900 23592 sgd_solver.cpp:106] Iteration 23000, lr = 0.001
I0216 18:25:14.827237 23592 solver.cpp:237] Iteration 23200, loss = 0.260637
I0216 18:25:14.827332 23592 solver.cpp:253]     Train net output #0: loss = 0.260637 (* 1 = 0.260637 loss)
I0216 18:25:14.827342 23592 sgd_solver.cpp:106] Iteration 23200, lr = 0.001
I0216 18:27:52.965289 23592 solver.cpp:237] Iteration 23400, loss = 0.331363
I0216 18:27:52.965375 23592 solver.cpp:253]     Train net output #0: loss = 0.331363 (* 1 = 0.331363 loss)
I0216 18:27:52.965385 23592 sgd_solver.cpp:106] Iteration 23400, lr = 0.001
I0216 18:30:29.977494 23592 solver.cpp:237] Iteration 23600, loss = 0.344331
I0216 18:30:29.977586 23592 solver.cpp:253]     Train net output #0: loss = 0.344331 (* 1 = 0.344331 loss)
I0216 18:30:29.977596 23592 sgd_solver.cpp:106] Iteration 23600, lr = 0.001
I0216 18:33:08.281594 23592 solver.cpp:237] Iteration 23800, loss = 0.26602
I0216 18:33:08.281672 23592 solver.cpp:253]     Train net output #0: loss = 0.26602 (* 1 = 0.26602 loss)
I0216 18:33:08.281682 23592 sgd_solver.cpp:106] Iteration 23800, lr = 0.001
I0216 18:35:45.816236 23592 solver.cpp:341] Iteration 24000, Testing net (#0)
I0216 18:36:20.256778 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7331
I0216 18:36:20.256855 23592 solver.cpp:409]     Test net output #1: loss = 0.847246 (* 1 = 0.847246 loss)
I0216 18:36:21.115180 23592 solver.cpp:237] Iteration 24000, loss = 0.378386
I0216 18:36:21.115226 23592 solver.cpp:253]     Train net output #0: loss = 0.378386 (* 1 = 0.378386 loss)
I0216 18:36:21.115234 23592 sgd_solver.cpp:106] Iteration 24000, lr = 0.001
I0216 18:39:01.005553 23592 solver.cpp:237] Iteration 24200, loss = 0.278768
I0216 18:39:01.005643 23592 solver.cpp:253]     Train net output #0: loss = 0.278768 (* 1 = 0.278768 loss)
I0216 18:39:01.005653 23592 sgd_solver.cpp:106] Iteration 24200, lr = 0.001
I0216 18:41:36.840836 23592 solver.cpp:237] Iteration 24400, loss = 0.319553
I0216 18:41:36.840924 23592 solver.cpp:253]     Train net output #0: loss = 0.319553 (* 1 = 0.319553 loss)
I0216 18:41:36.840934 23592 sgd_solver.cpp:106] Iteration 24400, lr = 0.001
I0216 18:44:14.557531 23592 solver.cpp:237] Iteration 24600, loss = 0.353418
I0216 18:44:14.557627 23592 solver.cpp:253]     Train net output #0: loss = 0.353418 (* 1 = 0.353418 loss)
I0216 18:44:14.557637 23592 sgd_solver.cpp:106] Iteration 24600, lr = 0.001
I0216 18:46:51.806398 23592 solver.cpp:237] Iteration 24800, loss = 0.252983
I0216 18:46:51.806488 23592 solver.cpp:253]     Train net output #0: loss = 0.252983 (* 1 = 0.252983 loss)
I0216 18:46:51.806499 23592 sgd_solver.cpp:106] Iteration 24800, lr = 0.001
I0216 18:49:27.599843 23592 solver.cpp:341] Iteration 25000, Testing net (#0)
I0216 18:50:03.054955 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7192
I0216 18:50:03.055043 23592 solver.cpp:409]     Test net output #1: loss = 0.895362 (* 1 = 0.895362 loss)
I0216 18:50:03.831835 23592 solver.cpp:237] Iteration 25000, loss = 0.386152
I0216 18:50:03.831879 23592 solver.cpp:253]     Train net output #0: loss = 0.386152 (* 1 = 0.386152 loss)
I0216 18:50:03.831887 23592 sgd_solver.cpp:106] Iteration 25000, lr = 0.001
I0216 18:52:39.787704 23592 solver.cpp:237] Iteration 25200, loss = 0.27785
I0216 18:52:39.787825 23592 solver.cpp:253]     Train net output #0: loss = 0.27785 (* 1 = 0.27785 loss)
I0216 18:52:39.787835 23592 sgd_solver.cpp:106] Iteration 25200, lr = 0.001
I0216 18:55:18.018173 23592 solver.cpp:237] Iteration 25400, loss = 0.2854
I0216 18:55:18.018251 23592 solver.cpp:253]     Train net output #0: loss = 0.2854 (* 1 = 0.2854 loss)
I0216 18:55:18.018260 23592 sgd_solver.cpp:106] Iteration 25400, lr = 0.001
I0216 18:57:54.121688 23592 solver.cpp:237] Iteration 25600, loss = 0.351273
I0216 18:57:54.121810 23592 solver.cpp:253]     Train net output #0: loss = 0.351273 (* 1 = 0.351273 loss)
I0216 18:57:54.121825 23592 sgd_solver.cpp:106] Iteration 25600, lr = 0.001
I0216 19:00:35.161350 23592 solver.cpp:237] Iteration 25800, loss = 0.245452
I0216 19:00:35.161437 23592 solver.cpp:253]     Train net output #0: loss = 0.245452 (* 1 = 0.245452 loss)
I0216 19:00:35.161448 23592 sgd_solver.cpp:106] Iteration 25800, lr = 0.001
I0216 19:03:15.108654 23592 solver.cpp:341] Iteration 26000, Testing net (#0)
I0216 19:03:49.729979 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7167
I0216 19:03:49.730051 23592 solver.cpp:409]     Test net output #1: loss = 0.902805 (* 1 = 0.902805 loss)
I0216 19:03:50.467042 23592 solver.cpp:237] Iteration 26000, loss = 0.364589
I0216 19:03:50.467087 23592 solver.cpp:253]     Train net output #0: loss = 0.364589 (* 1 = 0.364589 loss)
I0216 19:03:50.467094 23592 sgd_solver.cpp:106] Iteration 26000, lr = 0.001
I0216 19:06:31.880478 23592 solver.cpp:237] Iteration 26200, loss = 0.268763
I0216 19:06:31.880564 23592 solver.cpp:253]     Train net output #0: loss = 0.268763 (* 1 = 0.268763 loss)
I0216 19:06:31.880573 23592 sgd_solver.cpp:106] Iteration 26200, lr = 0.001
I0216 19:09:12.826352 23592 solver.cpp:237] Iteration 26400, loss = 0.243324
I0216 19:09:12.826856 23592 solver.cpp:253]     Train net output #0: loss = 0.243324 (* 1 = 0.243324 loss)
I0216 19:09:12.826869 23592 sgd_solver.cpp:106] Iteration 26400, lr = 0.001
I0216 19:11:53.340138 23592 solver.cpp:237] Iteration 26600, loss = 0.343483
I0216 19:11:53.340277 23592 solver.cpp:253]     Train net output #0: loss = 0.343483 (* 1 = 0.343483 loss)
I0216 19:11:53.340287 23592 sgd_solver.cpp:106] Iteration 26600, lr = 0.001
I0216 19:14:31.849828 23592 solver.cpp:237] Iteration 26800, loss = 0.232096
I0216 19:14:31.849915 23592 solver.cpp:253]     Train net output #0: loss = 0.232096 (* 1 = 0.232096 loss)
I0216 19:14:31.849925 23592 sgd_solver.cpp:106] Iteration 26800, lr = 0.001
I0216 19:17:10.150533 23592 solver.cpp:341] Iteration 27000, Testing net (#0)
I0216 19:17:45.129312 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7002
I0216 19:17:45.129421 23592 solver.cpp:409]     Test net output #1: loss = 0.977372 (* 1 = 0.977372 loss)
I0216 19:17:45.895673 23592 solver.cpp:237] Iteration 27000, loss = 0.395515
I0216 19:17:45.895721 23592 solver.cpp:253]     Train net output #0: loss = 0.395515 (* 1 = 0.395515 loss)
I0216 19:17:45.895730 23592 sgd_solver.cpp:106] Iteration 27000, lr = 0.001
I0216 19:20:34.451174 23592 solver.cpp:237] Iteration 27200, loss = 0.257088
I0216 19:20:34.451268 23592 solver.cpp:253]     Train net output #0: loss = 0.257088 (* 1 = 0.257088 loss)
I0216 19:20:34.451278 23592 sgd_solver.cpp:106] Iteration 27200, lr = 0.001
I0216 19:23:16.201066 23592 solver.cpp:237] Iteration 27400, loss = 0.239068
I0216 19:23:16.201153 23592 solver.cpp:253]     Train net output #0: loss = 0.239068 (* 1 = 0.239068 loss)
I0216 19:23:16.201164 23592 sgd_solver.cpp:106] Iteration 27400, lr = 0.001
I0216 19:25:55.807421 23592 solver.cpp:237] Iteration 27600, loss = 0.332052
I0216 19:25:55.807517 23592 solver.cpp:253]     Train net output #0: loss = 0.332052 (* 1 = 0.332052 loss)
I0216 19:25:55.807528 23592 sgd_solver.cpp:106] Iteration 27600, lr = 0.001
I0216 19:28:30.937834 23592 solver.cpp:237] Iteration 27800, loss = 0.218559
I0216 19:28:30.937901 23592 solver.cpp:253]     Train net output #0: loss = 0.218559 (* 1 = 0.218559 loss)
I0216 19:28:30.937912 23592 sgd_solver.cpp:106] Iteration 27800, lr = 0.001
I0216 19:31:04.856003 23592 solver.cpp:341] Iteration 28000, Testing net (#0)
I0216 19:31:41.289695 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7133
I0216 19:31:41.289772 23592 solver.cpp:409]     Test net output #1: loss = 0.927434 (* 1 = 0.927434 loss)
I0216 19:31:42.093098 23592 solver.cpp:237] Iteration 28000, loss = 0.330538
I0216 19:31:42.093143 23592 solver.cpp:253]     Train net output #0: loss = 0.330538 (* 1 = 0.330538 loss)
I0216 19:31:42.093157 23592 sgd_solver.cpp:106] Iteration 28000, lr = 0.001
I0216 19:34:13.623553 23592 solver.cpp:237] Iteration 28200, loss = 0.234336
I0216 19:34:13.623631 23592 solver.cpp:253]     Train net output #0: loss = 0.234336 (* 1 = 0.234336 loss)
I0216 19:34:13.623641 23592 sgd_solver.cpp:106] Iteration 28200, lr = 0.001
I0216 19:36:46.139812 23592 solver.cpp:237] Iteration 28400, loss = 0.23821
I0216 19:36:46.139897 23592 solver.cpp:253]     Train net output #0: loss = 0.23821 (* 1 = 0.23821 loss)
I0216 19:36:46.139906 23592 sgd_solver.cpp:106] Iteration 28400, lr = 0.001
I0216 19:39:18.943857 23592 solver.cpp:237] Iteration 28600, loss = 0.297445
I0216 19:39:18.944121 23592 solver.cpp:253]     Train net output #0: loss = 0.297445 (* 1 = 0.297445 loss)
I0216 19:39:18.944133 23592 sgd_solver.cpp:106] Iteration 28600, lr = 0.001
I0216 19:41:46.761178 23592 solver.cpp:237] Iteration 28800, loss = 0.212465
I0216 19:41:46.761275 23592 solver.cpp:253]     Train net output #0: loss = 0.212465 (* 1 = 0.212465 loss)
I0216 19:41:46.761288 23592 sgd_solver.cpp:106] Iteration 28800, lr = 0.001
I0216 19:44:19.317198 23592 solver.cpp:341] Iteration 29000, Testing net (#0)
I0216 19:44:54.973970 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7184
I0216 19:44:54.974086 23592 solver.cpp:409]     Test net output #1: loss = 0.910027 (* 1 = 0.910027 loss)
I0216 19:44:55.696635 23592 solver.cpp:237] Iteration 29000, loss = 0.318169
I0216 19:44:55.696679 23592 solver.cpp:253]     Train net output #0: loss = 0.318169 (* 1 = 0.318169 loss)
I0216 19:44:55.696687 23592 sgd_solver.cpp:106] Iteration 29000, lr = 0.001
I0216 19:47:36.242615 23592 solver.cpp:237] Iteration 29200, loss = 0.206794
I0216 19:47:36.242895 23592 solver.cpp:253]     Train net output #0: loss = 0.206794 (* 1 = 0.206794 loss)
I0216 19:47:36.242907 23592 sgd_solver.cpp:106] Iteration 29200, lr = 0.001
I0216 19:50:14.745304 23592 solver.cpp:237] Iteration 29400, loss = 0.275311
I0216 19:50:14.747486 23592 solver.cpp:253]     Train net output #0: loss = 0.275311 (* 1 = 0.275311 loss)
I0216 19:50:14.747510 23592 sgd_solver.cpp:106] Iteration 29400, lr = 0.001
I0216 19:52:58.063263 23592 solver.cpp:237] Iteration 29600, loss = 0.283224
I0216 19:52:58.063359 23592 solver.cpp:253]     Train net output #0: loss = 0.283224 (* 1 = 0.283224 loss)
I0216 19:52:58.063372 23592 sgd_solver.cpp:106] Iteration 29600, lr = 0.001
I0216 19:55:41.710050 23592 solver.cpp:237] Iteration 29800, loss = 0.223374
I0216 19:55:41.710691 23592 solver.cpp:253]     Train net output #0: loss = 0.223374 (* 1 = 0.223374 loss)
I0216 19:55:41.710707 23592 sgd_solver.cpp:106] Iteration 29800, lr = 0.001
I0216 19:58:20.117123 23592 solver.cpp:472] Snapshotting to HDF5 file examples/cifar10/jungwoo_full_iter_30000.caffemodel.h5
I0216 19:58:20.835433 23592 sgd_solver.cpp:279] Snapshotting solver state to HDF5 file examples/cifar10/jungwoo_full_iter_30000.solverstate.h5
I0216 19:58:20.838837 23592 solver.cpp:341] Iteration 30000, Testing net (#0)
I0216 19:58:56.906886 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7359
I0216 19:58:56.906981 23592 solver.cpp:409]     Test net output #1: loss = 0.845266 (* 1 = 0.845266 loss)
I0216 19:58:57.669564 23592 solver.cpp:237] Iteration 30000, loss = 0.25422
I0216 19:58:57.669610 23592 solver.cpp:253]     Train net output #0: loss = 0.25422 (* 1 = 0.25422 loss)
I0216 19:58:57.669618 23592 sgd_solver.cpp:106] Iteration 30000, lr = 0.001
I0216 20:01:36.605115 23592 solver.cpp:237] Iteration 30200, loss = 0.183671
I0216 20:01:36.605238 23592 solver.cpp:253]     Train net output #0: loss = 0.183671 (* 1 = 0.183671 loss)
I0216 20:01:36.605253 23592 sgd_solver.cpp:106] Iteration 30200, lr = 0.001
I0216 20:04:19.094570 23592 solver.cpp:237] Iteration 30400, loss = 0.293533
I0216 20:04:19.094694 23592 solver.cpp:253]     Train net output #0: loss = 0.293533 (* 1 = 0.293533 loss)
I0216 20:04:19.095155 23592 sgd_solver.cpp:106] Iteration 30400, lr = 0.001
I0216 20:06:56.234189 23592 solver.cpp:237] Iteration 30600, loss = 0.276151
I0216 20:06:56.234304 23592 solver.cpp:253]     Train net output #0: loss = 0.276151 (* 1 = 0.276151 loss)
I0216 20:06:56.234313 23592 sgd_solver.cpp:106] Iteration 30600, lr = 0.001
I0216 20:09:33.507702 23592 solver.cpp:237] Iteration 30800, loss = 0.223096
I0216 20:09:33.507820 23592 solver.cpp:253]     Train net output #0: loss = 0.223096 (* 1 = 0.223096 loss)
I0216 20:09:33.507838 23592 sgd_solver.cpp:106] Iteration 30800, lr = 0.001
I0216 20:11:58.006083 23592 solver.cpp:341] Iteration 31000, Testing net (#0)
I0216 20:12:30.026391 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7442
I0216 20:12:30.026474 23592 solver.cpp:409]     Test net output #1: loss = 0.813221 (* 1 = 0.813221 loss)
I0216 20:12:30.735525 23592 solver.cpp:237] Iteration 31000, loss = 0.227057
I0216 20:12:30.735568 23592 solver.cpp:253]     Train net output #0: loss = 0.227057 (* 1 = 0.227057 loss)
I0216 20:12:30.735575 23592 sgd_solver.cpp:106] Iteration 31000, lr = 0.001
I0216 20:14:53.757694 23592 solver.cpp:237] Iteration 31200, loss = 0.227564
I0216 20:14:53.757752 23592 solver.cpp:253]     Train net output #0: loss = 0.227564 (* 1 = 0.227564 loss)
I0216 20:14:53.757761 23592 sgd_solver.cpp:106] Iteration 31200, lr = 0.001
I0216 20:17:17.240067 23592 solver.cpp:237] Iteration 31400, loss = 0.242725
I0216 20:17:17.240169 23592 solver.cpp:253]     Train net output #0: loss = 0.242725 (* 1 = 0.242725 loss)
I0216 20:17:17.240178 23592 sgd_solver.cpp:106] Iteration 31400, lr = 0.001
I0216 20:19:39.983178 23592 solver.cpp:237] Iteration 31600, loss = 0.292941
I0216 20:19:39.983286 23592 solver.cpp:253]     Train net output #0: loss = 0.292941 (* 1 = 0.292941 loss)
I0216 20:19:39.983296 23592 sgd_solver.cpp:106] Iteration 31600, lr = 0.001
I0216 20:22:02.624495 23592 solver.cpp:237] Iteration 31800, loss = 0.224994
I0216 20:22:02.624580 23592 solver.cpp:253]     Train net output #0: loss = 0.224994 (* 1 = 0.224994 loss)
I0216 20:22:02.624589 23592 sgd_solver.cpp:106] Iteration 31800, lr = 0.001
I0216 20:24:24.636150 23592 solver.cpp:341] Iteration 32000, Testing net (#0)
I0216 20:24:56.609954 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7529
I0216 20:24:56.610030 23592 solver.cpp:409]     Test net output #1: loss = 0.797494 (* 1 = 0.797494 loss)
I0216 20:24:57.324021 23592 solver.cpp:237] Iteration 32000, loss = 0.239259
I0216 20:24:57.324064 23592 solver.cpp:253]     Train net output #0: loss = 0.239259 (* 1 = 0.239259 loss)
I0216 20:24:57.324072 23592 sgd_solver.cpp:106] Iteration 32000, lr = 0.001
I0216 20:27:20.284008 23592 solver.cpp:237] Iteration 32200, loss = 0.296501
I0216 20:27:20.284080 23592 solver.cpp:253]     Train net output #0: loss = 0.296501 (* 1 = 0.296501 loss)
I0216 20:27:20.284088 23592 sgd_solver.cpp:106] Iteration 32200, lr = 0.001
I0216 20:29:42.959450 23592 solver.cpp:237] Iteration 32400, loss = 0.230275
I0216 20:29:42.959523 23592 solver.cpp:253]     Train net output #0: loss = 0.230275 (* 1 = 0.230275 loss)
I0216 20:29:42.959532 23592 sgd_solver.cpp:106] Iteration 32400, lr = 0.001
I0216 20:32:05.473685 23592 solver.cpp:237] Iteration 32600, loss = 0.288282
I0216 20:32:05.473759 23592 solver.cpp:253]     Train net output #0: loss = 0.288283 (* 1 = 0.288283 loss)
I0216 20:32:05.473768 23592 sgd_solver.cpp:106] Iteration 32600, lr = 0.001
I0216 20:34:28.152616 23592 solver.cpp:237] Iteration 32800, loss = 0.21312
I0216 20:34:28.152675 23592 solver.cpp:253]     Train net output #0: loss = 0.21312 (* 1 = 0.21312 loss)
I0216 20:34:28.152684 23592 sgd_solver.cpp:106] Iteration 32800, lr = 0.001
I0216 20:36:50.060628 23592 solver.cpp:341] Iteration 33000, Testing net (#0)
I0216 20:37:22.001276 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7455
I0216 20:37:22.001335 23592 solver.cpp:409]     Test net output #1: loss = 0.847945 (* 1 = 0.847945 loss)
I0216 20:37:22.710330 23592 solver.cpp:237] Iteration 33000, loss = 0.287586
I0216 20:37:22.710373 23592 solver.cpp:253]     Train net output #0: loss = 0.287586 (* 1 = 0.287586 loss)
I0216 20:37:22.710381 23592 sgd_solver.cpp:106] Iteration 33000, lr = 0.001
I0216 20:39:45.414729 23592 solver.cpp:237] Iteration 33200, loss = 0.219353
I0216 20:39:45.414791 23592 solver.cpp:253]     Train net output #0: loss = 0.219353 (* 1 = 0.219353 loss)
I0216 20:39:45.414800 23592 sgd_solver.cpp:106] Iteration 33200, lr = 0.001
I0216 20:42:08.123623 23592 solver.cpp:237] Iteration 33400, loss = 0.232221
I0216 20:42:08.123703 23592 solver.cpp:253]     Train net output #0: loss = 0.232221 (* 1 = 0.232221 loss)
I0216 20:42:08.123713 23592 sgd_solver.cpp:106] Iteration 33400, lr = 0.001
I0216 20:44:30.768651 23592 solver.cpp:237] Iteration 33600, loss = 0.257113
I0216 20:44:30.768735 23592 solver.cpp:253]     Train net output #0: loss = 0.257113 (* 1 = 0.257113 loss)
I0216 20:44:30.768744 23592 sgd_solver.cpp:106] Iteration 33600, lr = 0.001
I0216 20:46:53.480417 23592 solver.cpp:237] Iteration 33800, loss = 0.208618
I0216 20:46:53.480495 23592 solver.cpp:253]     Train net output #0: loss = 0.208618 (* 1 = 0.208618 loss)
I0216 20:46:53.480504 23592 sgd_solver.cpp:106] Iteration 33800, lr = 0.001
I0216 20:49:15.023020 23592 solver.cpp:341] Iteration 34000, Testing net (#0)
I0216 20:49:46.818086 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7318
I0216 20:49:46.818167 23592 solver.cpp:409]     Test net output #1: loss = 0.916454 (* 1 = 0.916454 loss)
I0216 20:49:47.525657 23592 solver.cpp:237] Iteration 34000, loss = 0.356201
I0216 20:49:47.525699 23592 solver.cpp:253]     Train net output #0: loss = 0.356201 (* 1 = 0.356201 loss)
I0216 20:49:47.525707 23592 sgd_solver.cpp:106] Iteration 34000, lr = 0.001
I0216 20:52:09.418543 23592 solver.cpp:237] Iteration 34200, loss = 0.164116
I0216 20:52:09.418625 23592 solver.cpp:253]     Train net output #0: loss = 0.164116 (* 1 = 0.164116 loss)
I0216 20:52:09.418634 23592 sgd_solver.cpp:106] Iteration 34200, lr = 0.001
I0216 20:54:31.815497 23592 solver.cpp:237] Iteration 34400, loss = 0.226106
I0216 20:54:31.815570 23592 solver.cpp:253]     Train net output #0: loss = 0.226106 (* 1 = 0.226106 loss)
I0216 20:54:31.815580 23592 sgd_solver.cpp:106] Iteration 34400, lr = 0.001
I0216 20:56:54.283601 23592 solver.cpp:237] Iteration 34600, loss = 0.228302
I0216 20:56:54.283661 23592 solver.cpp:253]     Train net output #0: loss = 0.228302 (* 1 = 0.228302 loss)
I0216 20:56:54.283669 23592 sgd_solver.cpp:106] Iteration 34600, lr = 0.001
I0216 20:59:16.631726 23592 solver.cpp:237] Iteration 34800, loss = 0.202547
I0216 20:59:16.631799 23592 solver.cpp:253]     Train net output #0: loss = 0.202547 (* 1 = 0.202547 loss)
I0216 20:59:16.631808 23592 sgd_solver.cpp:106] Iteration 34800, lr = 0.001
I0216 21:01:38.334756 23592 solver.cpp:341] Iteration 35000, Testing net (#0)
I0216 21:02:10.287683 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7229
I0216 21:02:10.287739 23592 solver.cpp:409]     Test net output #1: loss = 0.941953 (* 1 = 0.941953 loss)
I0216 21:02:10.999896 23592 solver.cpp:237] Iteration 35000, loss = 0.358346
I0216 21:02:10.999940 23592 solver.cpp:253]     Train net output #0: loss = 0.358347 (* 1 = 0.358347 loss)
I0216 21:02:10.999948 23592 sgd_solver.cpp:106] Iteration 35000, lr = 0.001
I0216 21:04:33.688786 23592 solver.cpp:237] Iteration 35200, loss = 0.16535
I0216 21:04:33.688843 23592 solver.cpp:253]     Train net output #0: loss = 0.16535 (* 1 = 0.16535 loss)
I0216 21:04:33.688851 23592 sgd_solver.cpp:106] Iteration 35200, lr = 0.001
I0216 21:06:56.169720 23592 solver.cpp:237] Iteration 35400, loss = 0.237608
I0216 21:06:56.169807 23592 solver.cpp:253]     Train net output #0: loss = 0.237608 (* 1 = 0.237608 loss)
I0216 21:06:56.169816 23592 sgd_solver.cpp:106] Iteration 35400, lr = 0.001
I0216 21:09:18.701648 23592 solver.cpp:237] Iteration 35600, loss = 0.198964
I0216 21:09:18.701737 23592 solver.cpp:253]     Train net output #0: loss = 0.198965 (* 1 = 0.198965 loss)
I0216 21:09:18.701746 23592 sgd_solver.cpp:106] Iteration 35600, lr = 0.001
I0216 21:11:41.259699 23592 solver.cpp:237] Iteration 35800, loss = 0.192851
I0216 21:11:41.259786 23592 solver.cpp:253]     Train net output #0: loss = 0.192851 (* 1 = 0.192851 loss)
I0216 21:11:41.259795 23592 sgd_solver.cpp:106] Iteration 35800, lr = 0.001
I0216 21:14:03.130048 23592 solver.cpp:341] Iteration 36000, Testing net (#0)
I0216 21:14:35.010624 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7237
I0216 21:14:35.010710 23592 solver.cpp:409]     Test net output #1: loss = 0.952423 (* 1 = 0.952423 loss)
I0216 21:14:35.722295 23592 solver.cpp:237] Iteration 36000, loss = 0.294025
I0216 21:14:35.722337 23592 solver.cpp:253]     Train net output #0: loss = 0.294025 (* 1 = 0.294025 loss)
I0216 21:14:35.722345 23592 sgd_solver.cpp:106] Iteration 36000, lr = 0.001
I0216 21:16:58.461474 23592 solver.cpp:237] Iteration 36200, loss = 0.159673
I0216 21:16:58.461560 23592 solver.cpp:253]     Train net output #0: loss = 0.159673 (* 1 = 0.159673 loss)
I0216 21:16:58.461570 23592 sgd_solver.cpp:106] Iteration 36200, lr = 0.001
I0216 21:19:21.841841 23592 solver.cpp:237] Iteration 36400, loss = 0.252963
I0216 21:19:21.841909 23592 solver.cpp:253]     Train net output #0: loss = 0.252963 (* 1 = 0.252963 loss)
I0216 21:19:21.841917 23592 sgd_solver.cpp:106] Iteration 36400, lr = 0.001
I0216 21:21:44.614578 23592 solver.cpp:237] Iteration 36600, loss = 0.204273
I0216 21:21:44.614635 23592 solver.cpp:253]     Train net output #0: loss = 0.204273 (* 1 = 0.204273 loss)
I0216 21:21:44.614653 23592 sgd_solver.cpp:106] Iteration 36600, lr = 0.001
I0216 21:24:07.562173 23592 solver.cpp:237] Iteration 36800, loss = 0.172671
I0216 21:24:07.562230 23592 solver.cpp:253]     Train net output #0: loss = 0.172671 (* 1 = 0.172671 loss)
I0216 21:24:07.562239 23592 sgd_solver.cpp:106] Iteration 36800, lr = 0.001
I0216 21:26:29.339609 23592 solver.cpp:341] Iteration 37000, Testing net (#0)
I0216 21:27:01.174597 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7373
I0216 21:27:01.174684 23592 solver.cpp:409]     Test net output #1: loss = 0.90413 (* 1 = 0.90413 loss)
I0216 21:27:01.884091 23592 solver.cpp:237] Iteration 37000, loss = 0.258155
I0216 21:27:01.884135 23592 solver.cpp:253]     Train net output #0: loss = 0.258155 (* 1 = 0.258155 loss)
I0216 21:27:01.884141 23592 sgd_solver.cpp:106] Iteration 37000, lr = 0.001
I0216 21:29:24.214072 23592 solver.cpp:237] Iteration 37200, loss = 0.163624
I0216 21:29:24.214155 23592 solver.cpp:253]     Train net output #0: loss = 0.163624 (* 1 = 0.163624 loss)
I0216 21:29:24.214165 23592 sgd_solver.cpp:106] Iteration 37200, lr = 0.001
I0216 21:31:46.880385 23592 solver.cpp:237] Iteration 37400, loss = 0.274589
I0216 21:31:46.880470 23592 solver.cpp:253]     Train net output #0: loss = 0.274589 (* 1 = 0.274589 loss)
I0216 21:31:46.880481 23592 sgd_solver.cpp:106] Iteration 37400, lr = 0.001
I0216 21:34:09.487020 23592 solver.cpp:237] Iteration 37600, loss = 0.187286
I0216 21:34:09.487079 23592 solver.cpp:253]     Train net output #0: loss = 0.187286 (* 1 = 0.187286 loss)
I0216 21:34:09.487088 23592 sgd_solver.cpp:106] Iteration 37600, lr = 0.001
I0216 21:36:32.171928 23592 solver.cpp:237] Iteration 37800, loss = 0.159303
I0216 21:36:32.171988 23592 solver.cpp:253]     Train net output #0: loss = 0.159303 (* 1 = 0.159303 loss)
I0216 21:36:32.171996 23592 sgd_solver.cpp:106] Iteration 37800, lr = 0.001
I0216 21:38:53.945585 23592 solver.cpp:341] Iteration 38000, Testing net (#0)
I0216 21:39:25.815637 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7359
I0216 21:39:25.815726 23592 solver.cpp:409]     Test net output #1: loss = 0.936539 (* 1 = 0.936539 loss)
I0216 21:39:26.525620 23592 solver.cpp:237] Iteration 38000, loss = 0.255604
I0216 21:39:26.525661 23592 solver.cpp:253]     Train net output #0: loss = 0.255604 (* 1 = 0.255604 loss)
I0216 21:39:26.525682 23592 sgd_solver.cpp:106] Iteration 38000, lr = 0.001
I0216 21:41:49.051614 23592 solver.cpp:237] Iteration 38200, loss = 0.194119
I0216 21:41:49.051717 23592 solver.cpp:253]     Train net output #0: loss = 0.194119 (* 1 = 0.194119 loss)
I0216 21:41:49.051726 23592 sgd_solver.cpp:106] Iteration 38200, lr = 0.001
I0216 21:44:11.504786 23592 solver.cpp:237] Iteration 38400, loss = 0.30049
I0216 21:44:11.504858 23592 solver.cpp:253]     Train net output #0: loss = 0.30049 (* 1 = 0.30049 loss)
I0216 21:44:11.504868 23592 sgd_solver.cpp:106] Iteration 38400, lr = 0.001
I0216 21:46:34.416787 23592 solver.cpp:237] Iteration 38600, loss = 0.175903
I0216 21:46:34.416846 23592 solver.cpp:253]     Train net output #0: loss = 0.175903 (* 1 = 0.175903 loss)
I0216 21:46:34.416856 23592 sgd_solver.cpp:106] Iteration 38600, lr = 0.001
I0216 21:48:56.858248 23592 solver.cpp:237] Iteration 38800, loss = 0.176761
I0216 21:48:56.858337 23592 solver.cpp:253]     Train net output #0: loss = 0.176762 (* 1 = 0.176762 loss)
I0216 21:48:56.858347 23592 sgd_solver.cpp:106] Iteration 38800, lr = 0.001
I0216 21:51:18.763232 23592 solver.cpp:341] Iteration 39000, Testing net (#0)
I0216 21:51:50.714148 23592 solver.cpp:409]     Test net output #0: accuracy = 0.739
I0216 21:51:50.714221 23592 solver.cpp:409]     Test net output #1: loss = 0.911949 (* 1 = 0.911949 loss)
I0216 21:51:51.429594 23592 solver.cpp:237] Iteration 39000, loss = 0.211186
I0216 21:51:51.429638 23592 solver.cpp:253]     Train net output #0: loss = 0.211186 (* 1 = 0.211186 loss)
I0216 21:51:51.429646 23592 sgd_solver.cpp:106] Iteration 39000, lr = 0.001
I0216 21:54:14.088886 23592 solver.cpp:237] Iteration 39200, loss = 0.234649
I0216 21:54:14.088976 23592 solver.cpp:253]     Train net output #0: loss = 0.234649 (* 1 = 0.234649 loss)
I0216 21:54:14.088986 23592 sgd_solver.cpp:106] Iteration 39200, lr = 0.001
I0216 21:56:36.607105 23592 solver.cpp:237] Iteration 39400, loss = 0.319299
I0216 21:56:36.607197 23592 solver.cpp:253]     Train net output #0: loss = 0.319299 (* 1 = 0.319299 loss)
I0216 21:56:36.607208 23592 sgd_solver.cpp:106] Iteration 39400, lr = 0.001
I0216 21:58:59.154323 23592 solver.cpp:237] Iteration 39600, loss = 0.162049
I0216 21:58:59.154407 23592 solver.cpp:253]     Train net output #0: loss = 0.162049 (* 1 = 0.162049 loss)
I0216 21:58:59.154417 23592 sgd_solver.cpp:106] Iteration 39600, lr = 0.001
I0216 22:01:21.748996 23592 solver.cpp:237] Iteration 39800, loss = 0.19414
I0216 22:01:21.749073 23592 solver.cpp:253]     Train net output #0: loss = 0.19414 (* 1 = 0.19414 loss)
I0216 22:01:21.749081 23592 sgd_solver.cpp:106] Iteration 39800, lr = 0.001
I0216 22:03:45.284062 23592 solver.cpp:472] Snapshotting to HDF5 file examples/cifar10/jungwoo_full_iter_40000.caffemodel.h5
I0216 22:03:45.820087 23592 sgd_solver.cpp:279] Snapshotting solver state to HDF5 file examples/cifar10/jungwoo_full_iter_40000.solverstate.h5
I0216 22:03:45.822567 23592 solver.cpp:341] Iteration 40000, Testing net (#0)
I0216 22:04:17.904150 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7466
I0216 22:04:17.904234 23592 solver.cpp:409]     Test net output #1: loss = 0.87519 (* 1 = 0.87519 loss)
I0216 22:04:18.624101 23592 solver.cpp:237] Iteration 40000, loss = 0.194742
I0216 22:04:18.624145 23592 solver.cpp:253]     Train net output #0: loss = 0.194742 (* 1 = 0.194742 loss)
I0216 22:04:18.624152 23592 sgd_solver.cpp:106] Iteration 40000, lr = 0.001
I0216 22:06:41.680099 23592 solver.cpp:237] Iteration 40200, loss = 0.219039
I0216 22:06:41.680182 23592 solver.cpp:253]     Train net output #0: loss = 0.219039 (* 1 = 0.219039 loss)
I0216 22:06:41.680192 23592 sgd_solver.cpp:106] Iteration 40200, lr = 0.001
I0216 22:09:04.908732 23592 solver.cpp:237] Iteration 40400, loss = 0.291856
I0216 22:09:04.909096 23592 solver.cpp:253]     Train net output #0: loss = 0.291856 (* 1 = 0.291856 loss)
I0216 22:09:04.909114 23592 sgd_solver.cpp:106] Iteration 40400, lr = 0.001
I0216 22:11:27.425922 23592 solver.cpp:237] Iteration 40600, loss = 0.15382
I0216 22:11:27.426043 23592 solver.cpp:253]     Train net output #0: loss = 0.15382 (* 1 = 0.15382 loss)
I0216 22:11:27.426051 23592 sgd_solver.cpp:106] Iteration 40600, lr = 0.001
I0216 22:13:49.647395 23592 solver.cpp:237] Iteration 40800, loss = 0.128627
I0216 22:13:49.647483 23592 solver.cpp:253]     Train net output #0: loss = 0.128627 (* 1 = 0.128627 loss)
I0216 22:13:49.647493 23592 sgd_solver.cpp:106] Iteration 40800, lr = 0.001
I0216 22:16:11.142472 23592 solver.cpp:341] Iteration 41000, Testing net (#0)
I0216 22:16:43.112717 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7591
I0216 22:16:43.112773 23592 solver.cpp:409]     Test net output #1: loss = 0.813006 (* 1 = 0.813006 loss)
I0216 22:16:43.828495 23592 solver.cpp:237] Iteration 41000, loss = 0.129319
I0216 22:16:43.828536 23592 solver.cpp:253]     Train net output #0: loss = 0.129319 (* 1 = 0.129319 loss)
I0216 22:16:43.828544 23592 sgd_solver.cpp:106] Iteration 41000, lr = 0.001
I0216 22:19:06.390987 23592 solver.cpp:237] Iteration 41200, loss = 0.19029
I0216 22:19:06.391068 23592 solver.cpp:253]     Train net output #0: loss = 0.19029 (* 1 = 0.19029 loss)
I0216 22:19:06.391078 23592 sgd_solver.cpp:106] Iteration 41200, lr = 0.001
I0216 22:21:29.325592 23592 solver.cpp:237] Iteration 41400, loss = 0.239984
I0216 22:21:29.325671 23592 solver.cpp:253]     Train net output #0: loss = 0.239984 (* 1 = 0.239984 loss)
I0216 22:21:29.325680 23592 sgd_solver.cpp:106] Iteration 41400, lr = 0.001
I0216 22:23:52.308089 23592 solver.cpp:237] Iteration 41600, loss = 0.164624
I0216 22:23:52.308163 23592 solver.cpp:253]     Train net output #0: loss = 0.164624 (* 1 = 0.164624 loss)
I0216 22:23:52.308172 23592 sgd_solver.cpp:106] Iteration 41600, lr = 0.001
I0216 22:26:14.669759 23592 solver.cpp:237] Iteration 41800, loss = 0.119196
I0216 22:26:14.669817 23592 solver.cpp:253]     Train net output #0: loss = 0.119196 (* 1 = 0.119196 loss)
I0216 22:26:14.669826 23592 sgd_solver.cpp:106] Iteration 41800, lr = 0.001
I0216 22:28:36.339612 23592 solver.cpp:341] Iteration 42000, Testing net (#0)
I0216 22:29:08.316481 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7585
I0216 22:29:08.316567 23592 solver.cpp:409]     Test net output #1: loss = 0.827695 (* 1 = 0.827695 loss)
I0216 22:29:09.033282 23592 solver.cpp:237] Iteration 42000, loss = 0.128394
I0216 22:29:09.033321 23592 solver.cpp:253]     Train net output #0: loss = 0.128394 (* 1 = 0.128394 loss)
I0216 22:29:09.033329 23592 sgd_solver.cpp:106] Iteration 42000, lr = 0.001
I0216 22:31:31.634459 23592 solver.cpp:237] Iteration 42200, loss = 0.176726
I0216 22:31:31.634517 23592 solver.cpp:253]     Train net output #0: loss = 0.176726 (* 1 = 0.176726 loss)
I0216 22:31:31.634526 23592 sgd_solver.cpp:106] Iteration 42200, lr = 0.001
I0216 22:33:54.044015 23592 solver.cpp:237] Iteration 42400, loss = 0.195156
I0216 22:33:54.044093 23592 solver.cpp:253]     Train net output #0: loss = 0.195156 (* 1 = 0.195156 loss)
I0216 22:33:54.044103 23592 sgd_solver.cpp:106] Iteration 42400, lr = 0.001
I0216 22:36:16.598578 23592 solver.cpp:237] Iteration 42600, loss = 0.157311
I0216 22:36:16.598665 23592 solver.cpp:253]     Train net output #0: loss = 0.157311 (* 1 = 0.157311 loss)
I0216 22:36:16.598676 23592 sgd_solver.cpp:106] Iteration 42600, lr = 0.001
I0216 22:38:39.122932 23592 solver.cpp:237] Iteration 42800, loss = 0.126229
I0216 22:38:39.123010 23592 solver.cpp:253]     Train net output #0: loss = 0.126229 (* 1 = 0.126229 loss)
I0216 22:38:39.123020 23592 sgd_solver.cpp:106] Iteration 42800, lr = 0.001
I0216 22:41:00.805472 23592 solver.cpp:341] Iteration 43000, Testing net (#0)
I0216 22:41:32.829596 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7553
I0216 22:41:32.829655 23592 solver.cpp:409]     Test net output #1: loss = 0.858465 (* 1 = 0.858465 loss)
I0216 22:41:33.544178 23592 solver.cpp:237] Iteration 43000, loss = 0.127292
I0216 22:41:33.544220 23592 solver.cpp:253]     Train net output #0: loss = 0.127292 (* 1 = 0.127292 loss)
I0216 22:41:33.544227 23592 sgd_solver.cpp:106] Iteration 43000, lr = 0.001
I0216 22:43:55.999953 23592 solver.cpp:237] Iteration 43200, loss = 0.163774
I0216 22:43:56.000048 23592 solver.cpp:253]     Train net output #0: loss = 0.163774 (* 1 = 0.163774 loss)
I0216 22:43:56.000057 23592 sgd_solver.cpp:106] Iteration 43200, lr = 0.001
I0216 22:46:18.402601 23592 solver.cpp:237] Iteration 43400, loss = 0.168704
I0216 22:46:18.402678 23592 solver.cpp:253]     Train net output #0: loss = 0.168704 (* 1 = 0.168704 loss)
I0216 22:46:18.402686 23592 sgd_solver.cpp:106] Iteration 43400, lr = 0.001
I0216 22:48:40.991080 23592 solver.cpp:237] Iteration 43600, loss = 0.149479
I0216 22:48:40.991163 23592 solver.cpp:253]     Train net output #0: loss = 0.149479 (* 1 = 0.149479 loss)
I0216 22:48:40.991171 23592 sgd_solver.cpp:106] Iteration 43600, lr = 0.001
I0216 22:51:04.075319 23592 solver.cpp:237] Iteration 43800, loss = 0.12838
I0216 22:51:04.075397 23592 solver.cpp:253]     Train net output #0: loss = 0.12838 (* 1 = 0.12838 loss)
I0216 22:51:04.075407 23592 sgd_solver.cpp:106] Iteration 43800, lr = 0.001
I0216 22:53:25.863085 23592 solver.cpp:341] Iteration 44000, Testing net (#0)
I0216 22:53:57.802739 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7512
I0216 22:53:57.802795 23592 solver.cpp:409]     Test net output #1: loss = 0.879438 (* 1 = 0.879438 loss)
I0216 22:53:58.511688 23592 solver.cpp:237] Iteration 44000, loss = 0.147943
I0216 22:53:58.511729 23592 solver.cpp:253]     Train net output #0: loss = 0.147943 (* 1 = 0.147943 loss)
I0216 22:53:58.511736 23592 sgd_solver.cpp:106] Iteration 44000, lr = 0.001
I0216 22:56:21.018607 23592 solver.cpp:237] Iteration 44200, loss = 0.171406
I0216 22:56:21.018676 23592 solver.cpp:253]     Train net output #0: loss = 0.171406 (* 1 = 0.171406 loss)
I0216 22:56:21.018684 23592 sgd_solver.cpp:106] Iteration 44200, lr = 0.001
I0216 22:58:43.454690 23592 solver.cpp:237] Iteration 44400, loss = 0.156515
I0216 22:58:43.454751 23592 solver.cpp:253]     Train net output #0: loss = 0.156515 (* 1 = 0.156515 loss)
I0216 22:58:43.454759 23592 sgd_solver.cpp:106] Iteration 44400, lr = 0.001
I0216 23:01:05.794252 23592 solver.cpp:237] Iteration 44600, loss = 0.163403
I0216 23:01:05.794313 23592 solver.cpp:253]     Train net output #0: loss = 0.163403 (* 1 = 0.163403 loss)
I0216 23:01:05.794322 23592 sgd_solver.cpp:106] Iteration 44600, lr = 0.001
I0216 23:03:28.258558 23592 solver.cpp:237] Iteration 44800, loss = 0.139539
I0216 23:03:28.258617 23592 solver.cpp:253]     Train net output #0: loss = 0.139539 (* 1 = 0.139539 loss)
I0216 23:03:28.258626 23592 sgd_solver.cpp:106] Iteration 44800, lr = 0.001
I0216 23:05:50.038841 23592 solver.cpp:341] Iteration 45000, Testing net (#0)
I0216 23:06:21.970111 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7531
I0216 23:06:21.970191 23592 solver.cpp:409]     Test net output #1: loss = 0.846935 (* 1 = 0.846935 loss)
I0216 23:06:22.679544 23592 solver.cpp:237] Iteration 45000, loss = 0.162158
I0216 23:06:22.679586 23592 solver.cpp:253]     Train net output #0: loss = 0.162158 (* 1 = 0.162158 loss)
I0216 23:06:22.679594 23592 sgd_solver.cpp:106] Iteration 45000, lr = 0.001
I0216 23:08:45.005913 23592 solver.cpp:237] Iteration 45200, loss = 0.137336
I0216 23:08:45.005983 23592 solver.cpp:253]     Train net output #0: loss = 0.137336 (* 1 = 0.137336 loss)
I0216 23:08:45.005992 23592 sgd_solver.cpp:106] Iteration 45200, lr = 0.001
I0216 23:11:07.491948 23592 solver.cpp:237] Iteration 45400, loss = 0.153229
I0216 23:11:07.492035 23592 solver.cpp:253]     Train net output #0: loss = 0.153229 (* 1 = 0.153229 loss)
I0216 23:11:07.492044 23592 sgd_solver.cpp:106] Iteration 45400, lr = 0.001
I0216 23:13:29.815088 23592 solver.cpp:237] Iteration 45600, loss = 0.159438
I0216 23:13:29.815172 23592 solver.cpp:253]     Train net output #0: loss = 0.159438 (* 1 = 0.159438 loss)
I0216 23:13:29.815183 23592 sgd_solver.cpp:106] Iteration 45600, lr = 0.001
I0216 23:15:52.227352 23592 solver.cpp:237] Iteration 45800, loss = 0.146366
I0216 23:15:52.227527 23592 solver.cpp:253]     Train net output #0: loss = 0.146366 (* 1 = 0.146366 loss)
I0216 23:15:52.227562 23592 sgd_solver.cpp:106] Iteration 45800, lr = 0.001
I0216 23:18:14.139430 23592 solver.cpp:341] Iteration 46000, Testing net (#0)
I0216 23:18:46.147336 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7588
I0216 23:18:46.147425 23592 solver.cpp:409]     Test net output #1: loss = 0.801298 (* 1 = 0.801298 loss)
I0216 23:18:46.856101 23592 solver.cpp:237] Iteration 46000, loss = 0.119043
I0216 23:18:46.856143 23592 solver.cpp:253]     Train net output #0: loss = 0.119043 (* 1 = 0.119043 loss)
I0216 23:18:46.856150 23592 sgd_solver.cpp:106] Iteration 46000, lr = 0.001
I0216 23:21:09.344233 23592 solver.cpp:237] Iteration 46200, loss = 0.129789
I0216 23:21:09.344310 23592 solver.cpp:253]     Train net output #0: loss = 0.129789 (* 1 = 0.129789 loss)
I0216 23:21:09.344321 23592 sgd_solver.cpp:106] Iteration 46200, lr = 0.001
I0216 23:23:34.749433 23592 solver.cpp:237] Iteration 46400, loss = 0.166472
I0216 23:23:34.749511 23592 solver.cpp:253]     Train net output #0: loss = 0.166472 (* 1 = 0.166472 loss)
I0216 23:23:34.749521 23592 sgd_solver.cpp:106] Iteration 46400, lr = 0.001
I0216 23:25:57.494325 23592 solver.cpp:237] Iteration 46600, loss = 0.161564
I0216 23:25:57.494411 23592 solver.cpp:253]     Train net output #0: loss = 0.161564 (* 1 = 0.161564 loss)
I0216 23:25:57.494421 23592 sgd_solver.cpp:106] Iteration 46600, lr = 0.001
I0216 23:28:20.116780 23592 solver.cpp:237] Iteration 46800, loss = 0.126109
I0216 23:28:20.116868 23592 solver.cpp:253]     Train net output #0: loss = 0.126109 (* 1 = 0.126109 loss)
I0216 23:28:20.116878 23592 sgd_solver.cpp:106] Iteration 46800, lr = 0.001
I0216 23:30:42.001438 23592 solver.cpp:341] Iteration 47000, Testing net (#0)
I0216 23:31:13.995817 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7576
I0216 23:31:13.995898 23592 solver.cpp:409]     Test net output #1: loss = 0.817805 (* 1 = 0.817805 loss)
I0216 23:31:14.707999 23592 solver.cpp:237] Iteration 47000, loss = 0.125771
I0216 23:31:14.708042 23592 solver.cpp:253]     Train net output #0: loss = 0.12577 (* 1 = 0.12577 loss)
I0216 23:31:14.708050 23592 sgd_solver.cpp:106] Iteration 47000, lr = 0.001
I0216 23:33:37.409728 23592 solver.cpp:237] Iteration 47200, loss = 0.111732
I0216 23:33:37.409813 23592 solver.cpp:253]     Train net output #0: loss = 0.111732 (* 1 = 0.111732 loss)
I0216 23:33:37.409823 23592 sgd_solver.cpp:106] Iteration 47200, lr = 0.001
I0216 23:36:00.671908 23592 solver.cpp:237] Iteration 47400, loss = 0.172374
I0216 23:36:00.671989 23592 solver.cpp:253]     Train net output #0: loss = 0.172374 (* 1 = 0.172374 loss)
I0216 23:36:00.671999 23592 sgd_solver.cpp:106] Iteration 47400, lr = 0.001
I0216 23:38:23.836694 23592 solver.cpp:237] Iteration 47600, loss = 0.193192
I0216 23:38:23.836773 23592 solver.cpp:253]     Train net output #0: loss = 0.193191 (* 1 = 0.193191 loss)
I0216 23:38:23.836782 23592 sgd_solver.cpp:106] Iteration 47600, lr = 0.001
I0216 23:40:46.849845 23592 solver.cpp:237] Iteration 47800, loss = 0.106251
I0216 23:40:46.849923 23592 solver.cpp:253]     Train net output #0: loss = 0.106251 (* 1 = 0.106251 loss)
I0216 23:40:46.849933 23592 sgd_solver.cpp:106] Iteration 47800, lr = 0.001
I0216 23:43:08.762558 23592 solver.cpp:341] Iteration 48000, Testing net (#0)
I0216 23:43:40.730912 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7546
I0216 23:43:40.730998 23592 solver.cpp:409]     Test net output #1: loss = 0.844509 (* 1 = 0.844509 loss)
I0216 23:43:41.441331 23592 solver.cpp:237] Iteration 48000, loss = 0.145969
I0216 23:43:41.441372 23592 solver.cpp:253]     Train net output #0: loss = 0.145969 (* 1 = 0.145969 loss)
I0216 23:43:41.441380 23592 sgd_solver.cpp:106] Iteration 48000, lr = 0.001
I0216 23:46:03.962564 23592 solver.cpp:237] Iteration 48200, loss = 0.106799
I0216 23:46:03.962663 23592 solver.cpp:253]     Train net output #0: loss = 0.106799 (* 1 = 0.106799 loss)
I0216 23:46:03.962674 23592 sgd_solver.cpp:106] Iteration 48200, lr = 0.001
I0216 23:48:26.742635 23592 solver.cpp:237] Iteration 48400, loss = 0.174263
I0216 23:48:26.742763 23592 solver.cpp:253]     Train net output #0: loss = 0.174263 (* 1 = 0.174263 loss)
I0216 23:48:26.742772 23592 sgd_solver.cpp:106] Iteration 48400, lr = 0.001
I0216 23:50:49.330121 23592 solver.cpp:237] Iteration 48600, loss = 0.180421
I0216 23:50:49.330207 23592 solver.cpp:253]     Train net output #0: loss = 0.180421 (* 1 = 0.180421 loss)
I0216 23:50:49.330216 23592 sgd_solver.cpp:106] Iteration 48600, lr = 0.001
I0216 23:53:12.127780 23592 solver.cpp:237] Iteration 48800, loss = 0.0903363
I0216 23:53:12.127840 23592 solver.cpp:253]     Train net output #0: loss = 0.0903362 (* 1 = 0.0903362 loss)
I0216 23:53:12.127851 23592 sgd_solver.cpp:106] Iteration 48800, lr = 0.001
I0216 23:55:33.782310 23592 solver.cpp:341] Iteration 49000, Testing net (#0)
I0216 23:56:05.751479 23592 solver.cpp:409]     Test net output #0: accuracy = 0.754
I0216 23:56:05.751562 23592 solver.cpp:409]     Test net output #1: loss = 0.835405 (* 1 = 0.835405 loss)
I0216 23:56:06.464941 23592 solver.cpp:237] Iteration 49000, loss = 0.116146
I0216 23:56:06.464983 23592 solver.cpp:253]     Train net output #0: loss = 0.116146 (* 1 = 0.116146 loss)
I0216 23:56:06.464992 23592 sgd_solver.cpp:106] Iteration 49000, lr = 0.001
I0216 23:58:29.660590 23592 solver.cpp:237] Iteration 49200, loss = 0.0912106
I0216 23:58:29.660679 23592 solver.cpp:253]     Train net output #0: loss = 0.0912105 (* 1 = 0.0912105 loss)
I0216 23:58:29.660689 23592 sgd_solver.cpp:106] Iteration 49200, lr = 0.001
I0217 00:00:52.032387 23592 solver.cpp:237] Iteration 49400, loss = 0.146035
I0217 00:00:52.032469 23592 solver.cpp:253]     Train net output #0: loss = 0.146035 (* 1 = 0.146035 loss)
I0217 00:00:52.032479 23592 sgd_solver.cpp:106] Iteration 49400, lr = 0.001
I0217 00:03:14.728459 23592 solver.cpp:237] Iteration 49600, loss = 0.196878
I0217 00:03:14.728540 23592 solver.cpp:253]     Train net output #0: loss = 0.196878 (* 1 = 0.196878 loss)
I0217 00:03:14.728549 23592 sgd_solver.cpp:106] Iteration 49600, lr = 0.001
I0217 00:05:37.369874 23592 solver.cpp:237] Iteration 49800, loss = 0.0921267
I0217 00:05:37.369966 23592 solver.cpp:253]     Train net output #0: loss = 0.0921266 (* 1 = 0.0921266 loss)
I0217 00:05:37.369974 23592 sgd_solver.cpp:106] Iteration 49800, lr = 0.001
I0217 00:07:59.462522 23592 solver.cpp:472] Snapshotting to HDF5 file examples/cifar10/jungwoo_full_iter_50000.caffemodel.h5
I0217 00:08:00.002571 23592 sgd_solver.cpp:279] Snapshotting solver state to HDF5 file examples/cifar10/jungwoo_full_iter_50000.solverstate.h5
I0217 00:08:00.005041 23592 solver.cpp:341] Iteration 50000, Testing net (#0)
I0217 00:08:31.932301 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7622
I0217 00:08:31.932375 23592 solver.cpp:409]     Test net output #1: loss = 0.801733 (* 1 = 0.801733 loss)
I0217 00:08:32.642323 23592 solver.cpp:237] Iteration 50000, loss = 0.10226
I0217 00:08:32.642364 23592 solver.cpp:253]     Train net output #0: loss = 0.10226 (* 1 = 0.10226 loss)
I0217 00:08:32.642372 23592 sgd_solver.cpp:106] Iteration 50000, lr = 0.001
I0217 00:10:55.284852 23592 solver.cpp:237] Iteration 50200, loss = 0.0929527
I0217 00:10:55.284929 23592 solver.cpp:253]     Train net output #0: loss = 0.0929525 (* 1 = 0.0929525 loss)
I0217 00:10:55.284937 23592 sgd_solver.cpp:106] Iteration 50200, lr = 0.001
I0217 00:13:18.246518 23592 solver.cpp:237] Iteration 50400, loss = 0.150047
I0217 00:13:18.246604 23592 solver.cpp:253]     Train net output #0: loss = 0.150047 (* 1 = 0.150047 loss)
I0217 00:13:18.246614 23592 sgd_solver.cpp:106] Iteration 50400, lr = 0.001
I0217 00:15:41.194139 23592 solver.cpp:237] Iteration 50600, loss = 0.145743
I0217 00:15:41.194217 23592 solver.cpp:253]     Train net output #0: loss = 0.145743 (* 1 = 0.145743 loss)
I0217 00:15:41.194226 23592 sgd_solver.cpp:106] Iteration 50600, lr = 0.001
I0217 00:18:04.013362 23592 solver.cpp:237] Iteration 50800, loss = 0.157136
I0217 00:18:04.013456 23592 solver.cpp:253]     Train net output #0: loss = 0.157136 (* 1 = 0.157136 loss)
I0217 00:18:04.013465 23592 sgd_solver.cpp:106] Iteration 50800, lr = 0.001
I0217 00:20:26.276996 23592 solver.cpp:341] Iteration 51000, Testing net (#0)
I0217 00:20:58.244667 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7622
I0217 00:20:58.244753 23592 solver.cpp:409]     Test net output #1: loss = 0.801594 (* 1 = 0.801594 loss)
I0217 00:20:58.960072 23592 solver.cpp:237] Iteration 51000, loss = 0.106453
I0217 00:20:58.960115 23592 solver.cpp:253]     Train net output #0: loss = 0.106453 (* 1 = 0.106453 loss)
I0217 00:20:58.960124 23592 sgd_solver.cpp:106] Iteration 51000, lr = 0.001
I0217 00:23:21.746706 23592 solver.cpp:237] Iteration 51200, loss = 0.0918304
I0217 00:23:21.746795 23592 solver.cpp:253]     Train net output #0: loss = 0.0918303 (* 1 = 0.0918303 loss)
I0217 00:23:21.746804 23592 sgd_solver.cpp:106] Iteration 51200, lr = 0.001
I0217 00:25:44.574102 23592 solver.cpp:237] Iteration 51400, loss = 0.135763
I0217 00:25:44.574162 23592 solver.cpp:253]     Train net output #0: loss = 0.135763 (* 1 = 0.135763 loss)
I0217 00:25:44.574170 23592 sgd_solver.cpp:106] Iteration 51400, lr = 0.001
I0217 00:28:07.425791 23592 solver.cpp:237] Iteration 51600, loss = 0.122276
I0217 00:28:07.425873 23592 solver.cpp:253]     Train net output #0: loss = 0.122276 (* 1 = 0.122276 loss)
I0217 00:28:07.425882 23592 sgd_solver.cpp:106] Iteration 51600, lr = 0.001
I0217 00:30:30.277474 23592 solver.cpp:237] Iteration 51800, loss = 0.150589
I0217 00:30:30.277565 23592 solver.cpp:253]     Train net output #0: loss = 0.150588 (* 1 = 0.150588 loss)
I0217 00:30:30.277575 23592 sgd_solver.cpp:106] Iteration 51800, lr = 0.001
I0217 00:32:52.449270 23592 solver.cpp:341] Iteration 52000, Testing net (#0)
I0217 00:33:24.481274 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7632
I0217 00:33:24.481333 23592 solver.cpp:409]     Test net output #1: loss = 0.791236 (* 1 = 0.791236 loss)
I0217 00:33:25.196810 23592 solver.cpp:237] Iteration 52000, loss = 0.105135
I0217 00:33:25.196856 23592 solver.cpp:253]     Train net output #0: loss = 0.105135 (* 1 = 0.105135 loss)
I0217 00:33:25.196864 23592 sgd_solver.cpp:106] Iteration 52000, lr = 0.001
I0217 00:35:48.121745 23592 solver.cpp:237] Iteration 52200, loss = 0.0937251
I0217 00:35:48.121810 23592 solver.cpp:253]     Train net output #0: loss = 0.0937249 (* 1 = 0.0937249 loss)
I0217 00:35:48.121819 23592 sgd_solver.cpp:106] Iteration 52200, lr = 0.001
I0217 00:38:10.922178 23592 solver.cpp:237] Iteration 52400, loss = 0.150169
I0217 00:38:10.922257 23592 solver.cpp:253]     Train net output #0: loss = 0.150169 (* 1 = 0.150169 loss)
I0217 00:38:10.922267 23592 sgd_solver.cpp:106] Iteration 52400, lr = 0.001
I0217 00:40:33.659878 23592 solver.cpp:237] Iteration 52600, loss = 0.115988
I0217 00:40:33.659936 23592 solver.cpp:253]     Train net output #0: loss = 0.115987 (* 1 = 0.115987 loss)
I0217 00:40:33.659945 23592 sgd_solver.cpp:106] Iteration 52600, lr = 0.001
I0217 00:42:56.404397 23592 solver.cpp:237] Iteration 52800, loss = 0.141821
I0217 00:42:56.404475 23592 solver.cpp:253]     Train net output #0: loss = 0.141821 (* 1 = 0.141821 loss)
I0217 00:42:56.404485 23592 sgd_solver.cpp:106] Iteration 52800, lr = 0.001
I0217 00:45:18.528645 23592 solver.cpp:341] Iteration 53000, Testing net (#0)
I0217 00:45:50.577637 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7687
I0217 00:45:50.577694 23592 solver.cpp:409]     Test net output #1: loss = 0.777416 (* 1 = 0.777416 loss)
I0217 00:45:51.289969 23592 solver.cpp:237] Iteration 53000, loss = 0.0983167
I0217 00:45:51.290014 23592 solver.cpp:253]     Train net output #0: loss = 0.0983166 (* 1 = 0.0983166 loss)
I0217 00:45:51.290022 23592 sgd_solver.cpp:106] Iteration 53000, lr = 0.001
I0217 00:48:14.027863 23592 solver.cpp:237] Iteration 53200, loss = 0.0947974
I0217 00:48:14.027920 23592 solver.cpp:253]     Train net output #0: loss = 0.0947972 (* 1 = 0.0947972 loss)
I0217 00:48:14.027927 23592 sgd_solver.cpp:106] Iteration 53200, lr = 0.001
I0217 00:50:36.657855 23592 solver.cpp:237] Iteration 53400, loss = 0.138852
I0217 00:50:36.657954 23592 solver.cpp:253]     Train net output #0: loss = 0.138852 (* 1 = 0.138852 loss)
I0217 00:50:36.657968 23592 sgd_solver.cpp:106] Iteration 53400, lr = 0.001
I0217 00:52:59.731673 23592 solver.cpp:237] Iteration 53600, loss = 0.103013
I0217 00:52:59.731766 23592 solver.cpp:253]     Train net output #0: loss = 0.103013 (* 1 = 0.103013 loss)
I0217 00:52:59.731775 23592 sgd_solver.cpp:106] Iteration 53600, lr = 0.001
I0217 00:55:22.853611 23592 solver.cpp:237] Iteration 53800, loss = 0.129044
I0217 00:55:22.853700 23592 solver.cpp:253]     Train net output #0: loss = 0.129044 (* 1 = 0.129044 loss)
I0217 00:55:22.853709 23592 sgd_solver.cpp:106] Iteration 53800, lr = 0.001
I0217 00:57:45.193413 23592 solver.cpp:341] Iteration 54000, Testing net (#0)
I0217 00:58:17.248006 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7666
I0217 00:58:17.248088 23592 solver.cpp:409]     Test net output #1: loss = 0.788519 (* 1 = 0.788519 loss)
I0217 00:58:17.958561 23592 solver.cpp:237] Iteration 54000, loss = 0.0993892
I0217 00:58:17.958607 23592 solver.cpp:253]     Train net output #0: loss = 0.099389 (* 1 = 0.099389 loss)
I0217 00:58:17.958616 23592 sgd_solver.cpp:106] Iteration 54000, lr = 0.001
I0217 01:00:40.701484 23592 solver.cpp:237] Iteration 54200, loss = 0.113894
I0217 01:00:40.701544 23592 solver.cpp:253]     Train net output #0: loss = 0.113894 (* 1 = 0.113894 loss)
I0217 01:00:40.701551 23592 sgd_solver.cpp:106] Iteration 54200, lr = 0.001
I0217 01:03:03.488096 23592 solver.cpp:237] Iteration 54400, loss = 0.164298
I0217 01:03:03.488180 23592 solver.cpp:253]     Train net output #0: loss = 0.164298 (* 1 = 0.164298 loss)
I0217 01:03:03.488189 23592 sgd_solver.cpp:106] Iteration 54400, lr = 0.001
I0217 01:05:26.190610 23592 solver.cpp:237] Iteration 54600, loss = 0.093621
I0217 01:05:26.190701 23592 solver.cpp:253]     Train net output #0: loss = 0.0936209 (* 1 = 0.0936209 loss)
I0217 01:05:26.190711 23592 sgd_solver.cpp:106] Iteration 54600, lr = 0.001
I0217 01:07:48.881623 23592 solver.cpp:237] Iteration 54800, loss = 0.111271
I0217 01:07:48.881717 23592 solver.cpp:253]     Train net output #0: loss = 0.111271 (* 1 = 0.111271 loss)
I0217 01:07:48.881727 23592 sgd_solver.cpp:106] Iteration 54800, lr = 0.001
I0217 01:10:11.068056 23592 solver.cpp:341] Iteration 55000, Testing net (#0)
I0217 01:10:44.079021 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7627
I0217 01:10:44.079107 23592 solver.cpp:409]     Test net output #1: loss = 0.809354 (* 1 = 0.809354 loss)
I0217 01:10:44.794813 23592 solver.cpp:237] Iteration 55000, loss = 0.106579
I0217 01:10:44.794857 23592 solver.cpp:253]     Train net output #0: loss = 0.106578 (* 1 = 0.106578 loss)
I0217 01:10:44.794865 23592 sgd_solver.cpp:106] Iteration 55000, lr = 0.001
I0217 01:13:07.459962 23592 solver.cpp:237] Iteration 55200, loss = 0.166845
I0217 01:13:07.460052 23592 solver.cpp:253]     Train net output #0: loss = 0.166845 (* 1 = 0.166845 loss)
I0217 01:13:07.460062 23592 sgd_solver.cpp:106] Iteration 55200, lr = 0.001
I0217 01:15:29.810886 23592 solver.cpp:237] Iteration 55400, loss = 0.168327
I0217 01:15:29.810961 23592 solver.cpp:253]     Train net output #0: loss = 0.168327 (* 1 = 0.168327 loss)
I0217 01:15:29.810969 23592 sgd_solver.cpp:106] Iteration 55400, lr = 0.001
I0217 01:17:52.065853 23592 solver.cpp:237] Iteration 55600, loss = 0.0908107
I0217 01:17:52.065946 23592 solver.cpp:253]     Train net output #0: loss = 0.0908106 (* 1 = 0.0908106 loss)
I0217 01:17:52.065956 23592 sgd_solver.cpp:106] Iteration 55600, lr = 0.001
I0217 01:20:14.488694 23592 solver.cpp:237] Iteration 55800, loss = 0.11269
I0217 01:20:14.488783 23592 solver.cpp:253]     Train net output #0: loss = 0.11269 (* 1 = 0.11269 loss)
I0217 01:20:14.488792 23592 sgd_solver.cpp:106] Iteration 55800, lr = 0.001
I0217 01:22:36.164518 23592 solver.cpp:341] Iteration 56000, Testing net (#0)
I0217 01:23:08.034785 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7626
I0217 01:23:08.034903 23592 solver.cpp:409]     Test net output #1: loss = 0.817344 (* 1 = 0.817344 loss)
I0217 01:23:08.748025 23592 solver.cpp:237] Iteration 56000, loss = 0.102373
I0217 01:23:08.748082 23592 solver.cpp:253]     Train net output #0: loss = 0.102372 (* 1 = 0.102372 loss)
I0217 01:23:08.748091 23592 sgd_solver.cpp:106] Iteration 56000, lr = 0.001
I0217 01:25:31.164358 23592 solver.cpp:237] Iteration 56200, loss = 0.104145
I0217 01:25:31.164445 23592 solver.cpp:253]     Train net output #0: loss = 0.104145 (* 1 = 0.104145 loss)
I0217 01:25:31.164454 23592 sgd_solver.cpp:106] Iteration 56200, lr = 0.001
I0217 01:27:53.612306 23592 solver.cpp:237] Iteration 56400, loss = 0.133198
I0217 01:27:53.612388 23592 solver.cpp:253]     Train net output #0: loss = 0.133198 (* 1 = 0.133198 loss)
I0217 01:27:53.612398 23592 sgd_solver.cpp:106] Iteration 56400, lr = 0.001
I0217 01:30:16.014607 23592 solver.cpp:237] Iteration 56600, loss = 0.0953845
I0217 01:30:16.014698 23592 solver.cpp:253]     Train net output #0: loss = 0.0953844 (* 1 = 0.0953844 loss)
I0217 01:30:16.014709 23592 sgd_solver.cpp:106] Iteration 56600, lr = 0.001
I0217 01:32:38.555730 23592 solver.cpp:237] Iteration 56800, loss = 0.120184
I0217 01:32:38.555790 23592 solver.cpp:253]     Train net output #0: loss = 0.120184 (* 1 = 0.120184 loss)
I0217 01:32:38.555799 23592 sgd_solver.cpp:106] Iteration 56800, lr = 0.001
I0217 01:35:00.263476 23592 solver.cpp:341] Iteration 57000, Testing net (#0)
I0217 01:35:32.233332 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7538
I0217 01:35:32.233392 23592 solver.cpp:409]     Test net output #1: loss = 0.860449 (* 1 = 0.860449 loss)
I0217 01:35:32.948786 23592 solver.cpp:237] Iteration 57000, loss = 0.113943
I0217 01:35:32.948828 23592 solver.cpp:253]     Train net output #0: loss = 0.113943 (* 1 = 0.113943 loss)
I0217 01:35:32.948835 23592 sgd_solver.cpp:106] Iteration 57000, lr = 0.001
I0217 01:37:55.399891 23592 solver.cpp:237] Iteration 57200, loss = 0.10181
I0217 01:37:55.399986 23592 solver.cpp:253]     Train net output #0: loss = 0.101809 (* 1 = 0.101809 loss)
I0217 01:37:55.399996 23592 sgd_solver.cpp:106] Iteration 57200, lr = 0.001
I0217 01:40:17.844913 23592 solver.cpp:237] Iteration 57400, loss = 0.108806
I0217 01:40:17.844992 23592 solver.cpp:253]     Train net output #0: loss = 0.108806 (* 1 = 0.108806 loss)
I0217 01:40:17.845002 23592 sgd_solver.cpp:106] Iteration 57400, lr = 0.001
I0217 01:42:40.347203 23592 solver.cpp:237] Iteration 57600, loss = 0.125414
I0217 01:42:40.347261 23592 solver.cpp:253]     Train net output #0: loss = 0.125414 (* 1 = 0.125414 loss)
I0217 01:42:40.347270 23592 sgd_solver.cpp:106] Iteration 57600, lr = 0.001
I0217 01:45:02.792425 23592 solver.cpp:237] Iteration 57800, loss = 0.108205
I0217 01:45:02.792485 23592 solver.cpp:253]     Train net output #0: loss = 0.108205 (* 1 = 0.108205 loss)
I0217 01:45:02.792493 23592 sgd_solver.cpp:106] Iteration 57800, lr = 0.001
I0217 01:47:24.367543 23592 solver.cpp:341] Iteration 58000, Testing net (#0)
I0217 01:47:56.153650 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7497
I0217 01:47:56.153704 23592 solver.cpp:409]     Test net output #1: loss = 0.894304 (* 1 = 0.894304 loss)
I0217 01:47:56.861455 23592 solver.cpp:237] Iteration 58000, loss = 0.129675
I0217 01:47:56.861496 23592 solver.cpp:253]     Train net output #0: loss = 0.129675 (* 1 = 0.129675 loss)
I0217 01:47:56.861505 23592 sgd_solver.cpp:106] Iteration 58000, lr = 0.001
I0217 01:50:19.508075 23592 solver.cpp:237] Iteration 58200, loss = 0.107218
I0217 01:50:19.508138 23592 solver.cpp:253]     Train net output #0: loss = 0.107218 (* 1 = 0.107218 loss)
I0217 01:50:19.508147 23592 sgd_solver.cpp:106] Iteration 58200, lr = 0.001
I0217 01:52:42.292266 23592 solver.cpp:237] Iteration 58400, loss = 0.0971014
I0217 01:52:42.292357 23592 solver.cpp:253]     Train net output #0: loss = 0.0971013 (* 1 = 0.0971013 loss)
I0217 01:52:42.292367 23592 sgd_solver.cpp:106] Iteration 58400, lr = 0.001
I0217 01:55:04.859163 23592 solver.cpp:237] Iteration 58600, loss = 0.124963
I0217 01:55:04.859282 23592 solver.cpp:253]     Train net output #0: loss = 0.124962 (* 1 = 0.124962 loss)
I0217 01:55:04.859292 23592 sgd_solver.cpp:106] Iteration 58600, lr = 0.001
I0217 01:57:27.177592 23592 solver.cpp:237] Iteration 58800, loss = 0.125472
I0217 01:57:27.177691 23592 solver.cpp:253]     Train net output #0: loss = 0.125471 (* 1 = 0.125471 loss)
I0217 01:57:27.177701 23592 sgd_solver.cpp:106] Iteration 58800, lr = 0.001
I0217 01:59:48.742251 23592 solver.cpp:341] Iteration 59000, Testing net (#0)
I0217 02:00:20.697363 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7533
I0217 02:00:20.697439 23592 solver.cpp:409]     Test net output #1: loss = 0.877927 (* 1 = 0.877927 loss)
I0217 02:00:21.407814 23592 solver.cpp:237] Iteration 59000, loss = 0.116065
I0217 02:00:21.407860 23592 solver.cpp:253]     Train net output #0: loss = 0.116065 (* 1 = 0.116065 loss)
I0217 02:00:21.407867 23592 sgd_solver.cpp:106] Iteration 59000, lr = 0.001
I0217 02:02:43.861248 23592 solver.cpp:237] Iteration 59200, loss = 0.127781
I0217 02:02:43.861340 23592 solver.cpp:253]     Train net output #0: loss = 0.127781 (* 1 = 0.127781 loss)
I0217 02:02:43.861351 23592 sgd_solver.cpp:106] Iteration 59200, lr = 0.001
I0217 02:05:06.495012 23592 solver.cpp:237] Iteration 59400, loss = 0.103299
I0217 02:05:06.495096 23592 solver.cpp:253]     Train net output #0: loss = 0.103298 (* 1 = 0.103298 loss)
I0217 02:05:06.495105 23592 sgd_solver.cpp:106] Iteration 59400, lr = 0.001
I0217 02:07:29.109097 23592 solver.cpp:237] Iteration 59600, loss = 0.136661
I0217 02:07:29.109185 23592 solver.cpp:253]     Train net output #0: loss = 0.136661 (* 1 = 0.136661 loss)
I0217 02:07:29.109195 23592 sgd_solver.cpp:106] Iteration 59600, lr = 0.001
I0217 02:09:51.549751 23592 solver.cpp:237] Iteration 59800, loss = 0.110116
I0217 02:09:51.549834 23592 solver.cpp:253]     Train net output #0: loss = 0.110116 (* 1 = 0.110116 loss)
I0217 02:09:51.549844 23592 sgd_solver.cpp:106] Iteration 59800, lr = 0.001
I0217 02:12:14.563985 23592 solver.cpp:472] Snapshotting to HDF5 file examples/cifar10/jungwoo_full_iter_60000.caffemodel.h5
I0217 02:12:15.106051 23592 sgd_solver.cpp:279] Snapshotting solver state to HDF5 file examples/cifar10/jungwoo_full_iter_60000.solverstate.h5
I0217 02:12:15.429345 23592 solver.cpp:321] Iteration 60000, loss = 0.0975552
I0217 02:12:15.429383 23592 solver.cpp:341] Iteration 60000, Testing net (#0)
I0217 02:12:47.428375 23592 solver.cpp:409]     Test net output #0: accuracy = 0.7634
I0217 02:12:47.428460 23592 solver.cpp:409]     Test net output #1: loss = 0.822059 (* 1 = 0.822059 loss)
I0217 02:12:47.428468 23592 solver.cpp:326] Optimization Done.
I0217 02:12:47.428472 23592 caffe.cpp:215] Optimization Done.
I0217 02:12:48.082973 28930 caffe.cpp:177] Use CPU.
I0217 02:12:48.361686 28930 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 1000
base_lr: 0.0001
display: 200
max_iter: 65000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.004
snapshot: 5000
snapshot_prefix: "examples/cifar10/jungwoo_full"
solver_mode: CPU
net: "examples/cifar10/jungwoo_full_train_test.prototxt"
snapshot_format: HDF5
I0217 02:12:48.361848 28930 solver.cpp:91] Creating training net from net file: examples/cifar10/jungwoo_full_train_test.prototxt
I0217 02:12:48.362550 28930 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0217 02:12:48.362586 28930 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0217 02:12:48.362776 28930 net.cpp:49] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TRAIN
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "push1"
  type: "Pushin"
  bottom: "ip1"
  top: "ip1"
  pushin_param {
    pushin_ratio: 0.2
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0217 02:12:48.362916 28930 layer_factory.hpp:77] Creating layer cifar
I0217 02:12:48.364874 28930 net.cpp:106] Creating Layer cifar
I0217 02:12:48.364900 28930 net.cpp:411] cifar -> data
I0217 02:12:48.364935 28930 net.cpp:411] cifar -> label
I0217 02:12:48.364967 28930 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0217 02:12:48.366420 28935 db_lmdb.cpp:38] Opened lmdb examples/cifar10/cifar10_train_lmdb
I0217 02:12:48.366530 28930 data_layer.cpp:41] output data size: 100,3,32,32
I0217 02:12:48.369228 28930 net.cpp:150] Setting up cifar
I0217 02:12:48.369272 28930 net.cpp:157] Top shape: 100 3 32 32 (307200)
I0217 02:12:48.369287 28930 net.cpp:157] Top shape: 100 (100)
I0217 02:12:48.369295 28930 net.cpp:165] Memory required for data: 1229200
I0217 02:12:48.369312 28930 layer_factory.hpp:77] Creating layer conv1
I0217 02:12:48.369344 28930 net.cpp:106] Creating Layer conv1
I0217 02:12:48.369357 28930 net.cpp:454] conv1 <- data
I0217 02:12:48.369380 28930 net.cpp:411] conv1 -> conv1
I0217 02:12:48.556419 28930 net.cpp:150] Setting up conv1
I0217 02:12:48.556471 28930 net.cpp:157] Top shape: 100 32 32 32 (3276800)
I0217 02:12:48.556483 28930 net.cpp:165] Memory required for data: 14336400
I0217 02:12:48.556509 28930 layer_factory.hpp:77] Creating layer pool1
I0217 02:12:48.556529 28930 net.cpp:106] Creating Layer pool1
I0217 02:12:48.556538 28930 net.cpp:454] pool1 <- conv1
I0217 02:12:48.556550 28930 net.cpp:411] pool1 -> pool1
I0217 02:12:48.557377 28930 net.cpp:150] Setting up pool1
I0217 02:12:48.557397 28930 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0217 02:12:48.557406 28930 net.cpp:165] Memory required for data: 17613200
I0217 02:12:48.557415 28930 layer_factory.hpp:77] Creating layer relu1
I0217 02:12:48.557425 28930 net.cpp:106] Creating Layer relu1
I0217 02:12:48.557432 28930 net.cpp:454] relu1 <- pool1
I0217 02:12:48.557441 28930 net.cpp:397] relu1 -> pool1 (in-place)
I0217 02:12:48.558228 28930 net.cpp:150] Setting up relu1
I0217 02:12:48.558249 28930 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0217 02:12:48.558259 28930 net.cpp:165] Memory required for data: 20890000
I0217 02:12:48.558266 28930 layer_factory.hpp:77] Creating layer norm1
I0217 02:12:48.558297 28930 net.cpp:106] Creating Layer norm1
I0217 02:12:48.558310 28930 net.cpp:454] norm1 <- pool1
I0217 02:12:48.558321 28930 net.cpp:411] norm1 -> norm1
I0217 02:12:48.559492 28930 net.cpp:150] Setting up norm1
I0217 02:12:48.559516 28930 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0217 02:12:48.559527 28930 net.cpp:165] Memory required for data: 24166800
I0217 02:12:48.559536 28930 layer_factory.hpp:77] Creating layer conv2
I0217 02:12:48.559551 28930 net.cpp:106] Creating Layer conv2
I0217 02:12:48.559561 28930 net.cpp:454] conv2 <- norm1
I0217 02:12:48.559572 28930 net.cpp:411] conv2 -> conv2
I0217 02:12:48.563128 28930 net.cpp:150] Setting up conv2
I0217 02:12:48.563156 28930 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0217 02:12:48.563168 28930 net.cpp:165] Memory required for data: 27443600
I0217 02:12:48.563185 28930 layer_factory.hpp:77] Creating layer relu2
I0217 02:12:48.563199 28930 net.cpp:106] Creating Layer relu2
I0217 02:12:48.563207 28930 net.cpp:454] relu2 <- conv2
I0217 02:12:48.563218 28930 net.cpp:397] relu2 -> conv2 (in-place)
I0217 02:12:48.564043 28930 net.cpp:150] Setting up relu2
I0217 02:12:48.564062 28930 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0217 02:12:48.564071 28930 net.cpp:165] Memory required for data: 30720400
I0217 02:12:48.564079 28930 layer_factory.hpp:77] Creating layer pool2
I0217 02:12:48.564090 28930 net.cpp:106] Creating Layer pool2
I0217 02:12:48.564097 28930 net.cpp:454] pool2 <- conv2
I0217 02:12:48.564107 28930 net.cpp:411] pool2 -> pool2
I0217 02:12:48.564888 28930 net.cpp:150] Setting up pool2
I0217 02:12:48.564908 28930 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0217 02:12:48.564914 28930 net.cpp:165] Memory required for data: 31539600
I0217 02:12:48.564920 28930 layer_factory.hpp:77] Creating layer norm2
I0217 02:12:48.564934 28930 net.cpp:106] Creating Layer norm2
I0217 02:12:48.564940 28930 net.cpp:454] norm2 <- pool2
I0217 02:12:48.564970 28930 net.cpp:411] norm2 -> norm2
I0217 02:12:48.566085 28930 net.cpp:150] Setting up norm2
I0217 02:12:48.566107 28930 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0217 02:12:48.566118 28930 net.cpp:165] Memory required for data: 32358800
I0217 02:12:48.566125 28930 layer_factory.hpp:77] Creating layer conv3
I0217 02:12:48.566139 28930 net.cpp:106] Creating Layer conv3
I0217 02:12:48.566148 28930 net.cpp:454] conv3 <- norm2
I0217 02:12:48.566159 28930 net.cpp:411] conv3 -> conv3
I0217 02:12:48.570508 28930 net.cpp:150] Setting up conv3
I0217 02:12:48.570536 28930 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0217 02:12:48.570546 28930 net.cpp:165] Memory required for data: 33997200
I0217 02:12:48.570564 28930 layer_factory.hpp:77] Creating layer relu3
I0217 02:12:48.570577 28930 net.cpp:106] Creating Layer relu3
I0217 02:12:48.570586 28930 net.cpp:454] relu3 <- conv3
I0217 02:12:48.570595 28930 net.cpp:397] relu3 -> conv3 (in-place)
I0217 02:12:48.571521 28930 net.cpp:150] Setting up relu3
I0217 02:12:48.571544 28930 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0217 02:12:48.571555 28930 net.cpp:165] Memory required for data: 35635600
I0217 02:12:48.571563 28930 layer_factory.hpp:77] Creating layer pool3
I0217 02:12:48.571575 28930 net.cpp:106] Creating Layer pool3
I0217 02:12:48.571583 28930 net.cpp:454] pool3 <- conv3
I0217 02:12:48.571596 28930 net.cpp:411] pool3 -> pool3
I0217 02:12:48.572509 28930 net.cpp:150] Setting up pool3
I0217 02:12:48.572531 28930 net.cpp:157] Top shape: 100 64 4 4 (102400)
I0217 02:12:48.572542 28930 net.cpp:165] Memory required for data: 36045200
I0217 02:12:48.572551 28930 layer_factory.hpp:77] Creating layer ip1
I0217 02:12:48.572574 28930 net.cpp:106] Creating Layer ip1
I0217 02:12:48.572585 28930 net.cpp:454] ip1 <- pool3
I0217 02:12:48.572599 28930 net.cpp:411] ip1 -> ip1
I0217 02:12:48.589911 28930 net.cpp:150] Setting up ip1
I0217 02:12:48.589962 28930 net.cpp:157] Top shape: 100 500 (50000)
I0217 02:12:48.589973 28930 net.cpp:165] Memory required for data: 36245200
I0217 02:12:48.589988 28930 layer_factory.hpp:77] Creating layer relu4
I0217 02:12:48.590013 28930 net.cpp:106] Creating Layer relu4
I0217 02:12:48.590025 28930 net.cpp:454] relu4 <- ip1
I0217 02:12:48.590036 28930 net.cpp:397] relu4 -> ip1 (in-place)
I0217 02:12:48.591137 28930 net.cpp:150] Setting up relu4
I0217 02:12:48.591161 28930 net.cpp:157] Top shape: 100 500 (50000)
I0217 02:12:48.591171 28930 net.cpp:165] Memory required for data: 36445200
I0217 02:12:48.591179 28930 layer_factory.hpp:77] Creating layer push1
I0217 02:12:48.591195 28930 net.cpp:106] Creating Layer push1
I0217 02:12:48.591205 28930 net.cpp:454] push1 <- ip1
I0217 02:12:48.591218 28930 net.cpp:397] push1 -> ip1 (in-place)
I0217 02:12:48.591239 28930 net.cpp:150] Setting up push1
I0217 02:12:48.591251 28930 net.cpp:157] Top shape: 100 500 (50000)
I0217 02:12:48.591259 28930 net.cpp:165] Memory required for data: 36645200
I0217 02:12:48.591265 28930 layer_factory.hpp:77] Creating layer ip2
I0217 02:12:48.591279 28930 net.cpp:106] Creating Layer ip2
I0217 02:12:48.591289 28930 net.cpp:454] ip2 <- ip1
I0217 02:12:48.591305 28930 net.cpp:411] ip2 -> ip2
I0217 02:12:48.591509 28930 net.cpp:150] Setting up ip2
I0217 02:12:48.591527 28930 net.cpp:157] Top shape: 100 10 (1000)
I0217 02:12:48.591536 28930 net.cpp:165] Memory required for data: 36649200
I0217 02:12:48.591552 28930 layer_factory.hpp:77] Creating layer loss
I0217 02:12:48.591568 28930 net.cpp:106] Creating Layer loss
I0217 02:12:48.591578 28930 net.cpp:454] loss <- ip2
I0217 02:12:48.591586 28930 net.cpp:454] loss <- label
I0217 02:12:48.591596 28930 net.cpp:411] loss -> loss
I0217 02:12:48.591614 28930 layer_factory.hpp:77] Creating layer loss
I0217 02:12:48.592533 28930 net.cpp:150] Setting up loss
I0217 02:12:48.592555 28930 net.cpp:157] Top shape: (1)
I0217 02:12:48.592564 28930 net.cpp:160]     with loss weight 1
I0217 02:12:48.592587 28930 net.cpp:165] Memory required for data: 36649204
I0217 02:12:48.592597 28930 net.cpp:226] loss needs backward computation.
I0217 02:12:48.592624 28930 net.cpp:226] ip2 needs backward computation.
I0217 02:12:48.592633 28930 net.cpp:226] push1 needs backward computation.
I0217 02:12:48.592640 28930 net.cpp:226] relu4 needs backward computation.
I0217 02:12:48.592648 28930 net.cpp:226] ip1 needs backward computation.
I0217 02:12:48.592654 28930 net.cpp:226] pool3 needs backward computation.
I0217 02:12:48.592661 28930 net.cpp:226] relu3 needs backward computation.
I0217 02:12:48.592667 28930 net.cpp:226] conv3 needs backward computation.
I0217 02:12:48.592674 28930 net.cpp:226] norm2 needs backward computation.
I0217 02:12:48.592681 28930 net.cpp:226] pool2 needs backward computation.
I0217 02:12:48.592689 28930 net.cpp:226] relu2 needs backward computation.
I0217 02:12:48.592699 28930 net.cpp:226] conv2 needs backward computation.
I0217 02:12:48.592705 28930 net.cpp:226] norm1 needs backward computation.
I0217 02:12:48.592712 28930 net.cpp:226] relu1 needs backward computation.
I0217 02:12:48.592720 28930 net.cpp:226] pool1 needs backward computation.
I0217 02:12:48.592726 28930 net.cpp:226] conv1 needs backward computation.
I0217 02:12:48.592733 28930 net.cpp:228] cifar does not need backward computation.
I0217 02:12:48.592739 28930 net.cpp:270] This network produces output loss
I0217 02:12:48.592757 28930 net.cpp:283] Network initialization done.
I0217 02:12:48.593484 28930 solver.cpp:181] Creating test net (#0) specified by net file: examples/cifar10/jungwoo_full_train_test.prototxt
I0217 02:12:48.593549 28930 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0217 02:12:48.593747 28930 net.cpp:49] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "push1"
  type: "Pushin"
  bottom: "ip1"
  top: "ip1"
  pushin_param {
    pushin_ratio: 0.2
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0217 02:12:48.593899 28930 layer_factory.hpp:77] Creating layer cifar
I0217 02:12:48.594050 28930 net.cpp:106] Creating Layer cifar
I0217 02:12:48.594070 28930 net.cpp:411] cifar -> data
I0217 02:12:48.594089 28930 net.cpp:411] cifar -> label
I0217 02:12:48.594105 28930 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0217 02:12:48.595620 28937 db_lmdb.cpp:38] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0217 02:12:48.595715 28930 data_layer.cpp:41] output data size: 100,3,32,32
I0217 02:12:48.598356 28930 net.cpp:150] Setting up cifar
I0217 02:12:48.598410 28930 net.cpp:157] Top shape: 100 3 32 32 (307200)
I0217 02:12:48.598428 28930 net.cpp:157] Top shape: 100 (100)
I0217 02:12:48.598438 28930 net.cpp:165] Memory required for data: 1229200
I0217 02:12:48.598448 28930 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0217 02:12:48.598465 28930 net.cpp:106] Creating Layer label_cifar_1_split
I0217 02:12:48.598475 28930 net.cpp:454] label_cifar_1_split <- label
I0217 02:12:48.598489 28930 net.cpp:411] label_cifar_1_split -> label_cifar_1_split_0
I0217 02:12:48.598505 28930 net.cpp:411] label_cifar_1_split -> label_cifar_1_split_1
I0217 02:12:48.598521 28930 net.cpp:150] Setting up label_cifar_1_split
I0217 02:12:48.598533 28930 net.cpp:157] Top shape: 100 (100)
I0217 02:12:48.598541 28930 net.cpp:157] Top shape: 100 (100)
I0217 02:12:48.598548 28930 net.cpp:165] Memory required for data: 1230000
I0217 02:12:48.598554 28930 layer_factory.hpp:77] Creating layer conv1
I0217 02:12:48.598577 28930 net.cpp:106] Creating Layer conv1
I0217 02:12:48.598587 28930 net.cpp:454] conv1 <- data
I0217 02:12:48.598603 28930 net.cpp:411] conv1 -> conv1
I0217 02:12:48.601889 28930 net.cpp:150] Setting up conv1
I0217 02:12:48.601928 28930 net.cpp:157] Top shape: 100 32 32 32 (3276800)
I0217 02:12:48.601939 28930 net.cpp:165] Memory required for data: 14337200
I0217 02:12:48.601968 28930 layer_factory.hpp:77] Creating layer pool1
I0217 02:12:48.601989 28930 net.cpp:106] Creating Layer pool1
I0217 02:12:48.602000 28930 net.cpp:454] pool1 <- conv1
I0217 02:12:48.602015 28930 net.cpp:411] pool1 -> pool1
I0217 02:12:48.602968 28930 net.cpp:150] Setting up pool1
I0217 02:12:48.602990 28930 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0217 02:12:48.603005 28930 net.cpp:165] Memory required for data: 17614000
I0217 02:12:48.603014 28930 layer_factory.hpp:77] Creating layer relu1
I0217 02:12:48.603027 28930 net.cpp:106] Creating Layer relu1
I0217 02:12:48.603039 28930 net.cpp:454] relu1 <- pool1
I0217 02:12:48.603050 28930 net.cpp:397] relu1 -> pool1 (in-place)
I0217 02:12:48.603987 28930 net.cpp:150] Setting up relu1
I0217 02:12:48.604012 28930 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0217 02:12:48.604019 28930 net.cpp:165] Memory required for data: 20890800
I0217 02:12:48.604028 28930 layer_factory.hpp:77] Creating layer norm1
I0217 02:12:48.604043 28930 net.cpp:106] Creating Layer norm1
I0217 02:12:48.604053 28930 net.cpp:454] norm1 <- pool1
I0217 02:12:48.604064 28930 net.cpp:411] norm1 -> norm1
I0217 02:12:48.605305 28930 net.cpp:150] Setting up norm1
I0217 02:12:48.605347 28930 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0217 02:12:48.605357 28930 net.cpp:165] Memory required for data: 24167600
I0217 02:12:48.605365 28930 layer_factory.hpp:77] Creating layer conv2
I0217 02:12:48.605386 28930 net.cpp:106] Creating Layer conv2
I0217 02:12:48.605396 28930 net.cpp:454] conv2 <- norm1
I0217 02:12:48.605408 28930 net.cpp:411] conv2 -> conv2
I0217 02:12:48.609450 28930 net.cpp:150] Setting up conv2
I0217 02:12:48.609480 28930 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0217 02:12:48.609491 28930 net.cpp:165] Memory required for data: 27444400
I0217 02:12:48.609514 28930 layer_factory.hpp:77] Creating layer relu2
I0217 02:12:48.609530 28930 net.cpp:106] Creating Layer relu2
I0217 02:12:48.609539 28930 net.cpp:454] relu2 <- conv2
I0217 02:12:48.609554 28930 net.cpp:397] relu2 -> conv2 (in-place)
I0217 02:12:48.610482 28930 net.cpp:150] Setting up relu2
I0217 02:12:48.610503 28930 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0217 02:12:48.610512 28930 net.cpp:165] Memory required for data: 30721200
I0217 02:12:48.610520 28930 layer_factory.hpp:77] Creating layer pool2
I0217 02:12:48.610537 28930 net.cpp:106] Creating Layer pool2
I0217 02:12:48.610545 28930 net.cpp:454] pool2 <- conv2
I0217 02:12:48.610556 28930 net.cpp:411] pool2 -> pool2
I0217 02:12:48.611486 28930 net.cpp:150] Setting up pool2
I0217 02:12:48.611507 28930 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0217 02:12:48.611527 28930 net.cpp:165] Memory required for data: 31540400
I0217 02:12:48.611537 28930 layer_factory.hpp:77] Creating layer norm2
I0217 02:12:48.611552 28930 net.cpp:106] Creating Layer norm2
I0217 02:12:48.611562 28930 net.cpp:454] norm2 <- pool2
I0217 02:12:48.611572 28930 net.cpp:411] norm2 -> norm2
I0217 02:12:48.612836 28930 net.cpp:150] Setting up norm2
I0217 02:12:48.612859 28930 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0217 02:12:48.612869 28930 net.cpp:165] Memory required for data: 32359600
I0217 02:12:48.612876 28930 layer_factory.hpp:77] Creating layer conv3
I0217 02:12:48.612893 28930 net.cpp:106] Creating Layer conv3
I0217 02:12:48.612903 28930 net.cpp:454] conv3 <- norm2
I0217 02:12:48.612915 28930 net.cpp:411] conv3 -> conv3
I0217 02:12:48.617661 28930 net.cpp:150] Setting up conv3
I0217 02:12:48.617691 28930 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0217 02:12:48.617702 28930 net.cpp:165] Memory required for data: 33998000
I0217 02:12:48.617722 28930 layer_factory.hpp:77] Creating layer relu3
I0217 02:12:48.617736 28930 net.cpp:106] Creating Layer relu3
I0217 02:12:48.617745 28930 net.cpp:454] relu3 <- conv3
I0217 02:12:48.617760 28930 net.cpp:397] relu3 -> conv3 (in-place)
I0217 02:12:48.618688 28930 net.cpp:150] Setting up relu3
I0217 02:12:48.618711 28930 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0217 02:12:48.618721 28930 net.cpp:165] Memory required for data: 35636400
I0217 02:12:48.618729 28930 layer_factory.hpp:77] Creating layer pool3
I0217 02:12:48.618746 28930 net.cpp:106] Creating Layer pool3
I0217 02:12:48.618755 28930 net.cpp:454] pool3 <- conv3
I0217 02:12:48.618765 28930 net.cpp:411] pool3 -> pool3
I0217 02:12:48.619710 28930 net.cpp:150] Setting up pool3
I0217 02:12:48.619730 28930 net.cpp:157] Top shape: 100 64 4 4 (102400)
I0217 02:12:48.619741 28930 net.cpp:165] Memory required for data: 36046000
I0217 02:12:48.619750 28930 layer_factory.hpp:77] Creating layer ip1
I0217 02:12:48.619765 28930 net.cpp:106] Creating Layer ip1
I0217 02:12:48.619773 28930 net.cpp:454] ip1 <- pool3
I0217 02:12:48.619786 28930 net.cpp:411] ip1 -> ip1
I0217 02:12:48.637055 28930 net.cpp:150] Setting up ip1
I0217 02:12:48.637107 28930 net.cpp:157] Top shape: 100 500 (50000)
I0217 02:12:48.637120 28930 net.cpp:165] Memory required for data: 36246000
I0217 02:12:48.637135 28930 layer_factory.hpp:77] Creating layer relu4
I0217 02:12:48.637152 28930 net.cpp:106] Creating Layer relu4
I0217 02:12:48.637162 28930 net.cpp:454] relu4 <- ip1
I0217 02:12:48.637177 28930 net.cpp:397] relu4 -> ip1 (in-place)
I0217 02:12:48.638283 28930 net.cpp:150] Setting up relu4
I0217 02:12:48.638319 28930 net.cpp:157] Top shape: 100 500 (50000)
I0217 02:12:48.638326 28930 net.cpp:165] Memory required for data: 36446000
I0217 02:12:48.638334 28930 layer_factory.hpp:77] Creating layer push1
I0217 02:12:48.638345 28930 net.cpp:106] Creating Layer push1
I0217 02:12:48.638352 28930 net.cpp:454] push1 <- ip1
I0217 02:12:48.638360 28930 net.cpp:397] push1 -> ip1 (in-place)
I0217 02:12:48.638373 28930 net.cpp:150] Setting up push1
I0217 02:12:48.638381 28930 net.cpp:157] Top shape: 100 500 (50000)
I0217 02:12:48.638387 28930 net.cpp:165] Memory required for data: 36646000
I0217 02:12:48.638393 28930 layer_factory.hpp:77] Creating layer ip2
I0217 02:12:48.638408 28930 net.cpp:106] Creating Layer ip2
I0217 02:12:48.638416 28930 net.cpp:454] ip2 <- ip1
I0217 02:12:48.638424 28930 net.cpp:411] ip2 -> ip2
I0217 02:12:48.638623 28930 net.cpp:150] Setting up ip2
I0217 02:12:48.638643 28930 net.cpp:157] Top shape: 100 10 (1000)
I0217 02:12:48.638659 28930 net.cpp:165] Memory required for data: 36650000
I0217 02:12:48.638676 28930 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0217 02:12:48.638689 28930 net.cpp:106] Creating Layer ip2_ip2_0_split
I0217 02:12:48.638697 28930 net.cpp:454] ip2_ip2_0_split <- ip2
I0217 02:12:48.638706 28930 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0217 02:12:48.638718 28930 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0217 02:12:48.638742 28930 net.cpp:150] Setting up ip2_ip2_0_split
I0217 02:12:48.638754 28930 net.cpp:157] Top shape: 100 10 (1000)
I0217 02:12:48.638763 28930 net.cpp:157] Top shape: 100 10 (1000)
I0217 02:12:48.638770 28930 net.cpp:165] Memory required for data: 36658000
I0217 02:12:48.638777 28930 layer_factory.hpp:77] Creating layer accuracy
I0217 02:12:48.638790 28930 net.cpp:106] Creating Layer accuracy
I0217 02:12:48.638799 28930 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I0217 02:12:48.638809 28930 net.cpp:454] accuracy <- label_cifar_1_split_0
I0217 02:12:48.638819 28930 net.cpp:411] accuracy -> accuracy
I0217 02:12:48.638835 28930 net.cpp:150] Setting up accuracy
I0217 02:12:48.638847 28930 net.cpp:157] Top shape: (1)
I0217 02:12:48.638854 28930 net.cpp:165] Memory required for data: 36658004
I0217 02:12:48.638861 28930 layer_factory.hpp:77] Creating layer loss
I0217 02:12:48.638871 28930 net.cpp:106] Creating Layer loss
I0217 02:12:48.638878 28930 net.cpp:454] loss <- ip2_ip2_0_split_1
I0217 02:12:48.638885 28930 net.cpp:454] loss <- label_cifar_1_split_1
I0217 02:12:48.638895 28930 net.cpp:411] loss -> loss
I0217 02:12:48.638907 28930 layer_factory.hpp:77] Creating layer loss
I0217 02:12:48.639859 28930 net.cpp:150] Setting up loss
I0217 02:12:48.639880 28930 net.cpp:157] Top shape: (1)
I0217 02:12:48.639891 28930 net.cpp:160]     with loss weight 1
I0217 02:12:48.639904 28930 net.cpp:165] Memory required for data: 36658008
I0217 02:12:48.639912 28930 net.cpp:226] loss needs backward computation.
I0217 02:12:48.639920 28930 net.cpp:228] accuracy does not need backward computation.
I0217 02:12:48.639928 28930 net.cpp:226] ip2_ip2_0_split needs backward computation.
I0217 02:12:48.639935 28930 net.cpp:226] ip2 needs backward computation.
I0217 02:12:48.639942 28930 net.cpp:226] push1 needs backward computation.
I0217 02:12:48.639950 28930 net.cpp:226] relu4 needs backward computation.
I0217 02:12:48.639956 28930 net.cpp:226] ip1 needs backward computation.
I0217 02:12:48.639963 28930 net.cpp:226] pool3 needs backward computation.
I0217 02:12:48.639971 28930 net.cpp:226] relu3 needs backward computation.
I0217 02:12:48.639976 28930 net.cpp:226] conv3 needs backward computation.
I0217 02:12:48.639983 28930 net.cpp:226] norm2 needs backward computation.
I0217 02:12:48.639991 28930 net.cpp:226] pool2 needs backward computation.
I0217 02:12:48.639997 28930 net.cpp:226] relu2 needs backward computation.
I0217 02:12:48.640003 28930 net.cpp:226] conv2 needs backward computation.
I0217 02:12:48.640010 28930 net.cpp:226] norm1 needs backward computation.
I0217 02:12:48.640017 28930 net.cpp:226] relu1 needs backward computation.
I0217 02:12:48.640024 28930 net.cpp:226] pool1 needs backward computation.
I0217 02:12:48.640044 28930 net.cpp:226] conv1 needs backward computation.
I0217 02:12:48.640053 28930 net.cpp:228] label_cifar_1_split does not need backward computation.
I0217 02:12:48.640065 28930 net.cpp:228] cifar does not need backward computation.
I0217 02:12:48.640072 28930 net.cpp:270] This network produces output accuracy
I0217 02:12:48.640079 28930 net.cpp:270] This network produces output loss
I0217 02:12:48.640102 28930 net.cpp:283] Network initialization done.
I0217 02:12:48.640236 28930 solver.cpp:60] Solver scaffolding done.
I0217 02:12:48.640291 28930 caffe.cpp:202] Resuming from examples/cifar10/jungwoo_full_iter_60000.solverstate.h5
I0217 02:12:48.641789 28930 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0217 02:12:48.645385 28930 caffe.cpp:212] Starting Optimization
I0217 02:12:48.645424 28930 solver.cpp:288] Solving CIFAR10_full
I0217 02:12:48.645434 28930 solver.cpp:289] Learning Rate Policy: fixed
I0217 02:12:48.646337 28930 solver.cpp:341] Iteration 60000, Testing net (#0)
I0217 02:13:21.935986 28930 solver.cpp:409]     Test net output #0: accuracy = 0.2473
I0217 02:13:21.936054 28930 solver.cpp:409]     Test net output #1: loss = 2.00255 (* 1 = 2.00255 loss)
I0217 02:13:22.663779 28930 solver.cpp:237] Iteration 60000, loss = 1.90183
I0217 02:13:22.663821 28930 solver.cpp:253]     Train net output #0: loss = 1.90183 (* 1 = 1.90183 loss)
I0217 02:13:22.663842 28930 sgd_solver.cpp:106] Iteration 60000, lr = 0.0001
I0217 02:15:44.923758 28930 solver.cpp:237] Iteration 60200, loss = 1.55455
I0217 02:15:44.923846 28930 solver.cpp:253]     Train net output #0: loss = 1.55455 (* 1 = 1.55455 loss)
I0217 02:15:44.923854 28930 sgd_solver.cpp:106] Iteration 60200, lr = 0.0001
I0217 02:18:07.277335 28930 solver.cpp:237] Iteration 60400, loss = 1.21362
I0217 02:18:07.277426 28930 solver.cpp:253]     Train net output #0: loss = 1.21362 (* 1 = 1.21362 loss)
I0217 02:18:07.277434 28930 sgd_solver.cpp:106] Iteration 60400, lr = 0.0001
I0217 02:20:29.909029 28930 solver.cpp:237] Iteration 60600, loss = 0.859385
I0217 02:20:29.909091 28930 solver.cpp:253]     Train net output #0: loss = 0.859385 (* 1 = 0.859385 loss)
I0217 02:20:29.909101 28930 sgd_solver.cpp:106] Iteration 60600, lr = 0.0001
I0217 02:22:53.072124 28930 solver.cpp:237] Iteration 60800, loss = 0.483958
I0217 02:22:53.072182 28930 solver.cpp:253]     Train net output #0: loss = 0.483958 (* 1 = 0.483958 loss)
I0217 02:22:53.072191 28930 sgd_solver.cpp:106] Iteration 60800, lr = 0.0001
I0217 02:25:15.386028 28930 solver.cpp:341] Iteration 61000, Testing net (#0)
I0217 02:25:47.476733 28930 solver.cpp:409]     Test net output #0: accuracy = 0.2871
I0217 02:25:47.476794 28930 solver.cpp:409]     Test net output #1: loss = 1.88552 (* 1 = 1.88552 loss)
I0217 02:25:48.184145 28930 solver.cpp:237] Iteration 61000, loss = 0.122938
I0217 02:25:48.184191 28930 solver.cpp:253]     Train net output #0: loss = 0.122938 (* 1 = 0.122938 loss)
I0217 02:25:48.184200 28930 sgd_solver.cpp:106] Iteration 61000, lr = 0.0001
I0217 02:28:11.076977 28930 solver.cpp:237] Iteration 61200, loss = 0.129432
I0217 02:28:11.077040 28930 solver.cpp:253]     Train net output #0: loss = 0.129432 (* 1 = 0.129432 loss)
I0217 02:28:11.077049 28930 sgd_solver.cpp:106] Iteration 61200, lr = 0.0001
I0217 02:30:34.004325 28930 solver.cpp:237] Iteration 61400, loss = 0.10742
I0217 02:30:34.004387 28930 solver.cpp:253]     Train net output #0: loss = 0.10742 (* 1 = 0.10742 loss)
I0217 02:30:34.004396 28930 sgd_solver.cpp:106] Iteration 61400, lr = 0.0001
I0217 02:32:56.516502 28930 solver.cpp:237] Iteration 61600, loss = 0.0911301
I0217 02:32:56.516587 28930 solver.cpp:253]     Train net output #0: loss = 0.0911301 (* 1 = 0.0911301 loss)
I0217 02:32:56.516597 28930 sgd_solver.cpp:106] Iteration 61600, lr = 0.0001
I0217 02:35:18.934371 28930 solver.cpp:237] Iteration 61800, loss = 0.115012
I0217 02:35:18.934489 28930 solver.cpp:253]     Train net output #0: loss = 0.115012 (* 1 = 0.115012 loss)
I0217 02:35:18.934499 28930 sgd_solver.cpp:106] Iteration 61800, lr = 0.0001
I0217 02:37:40.778597 28930 solver.cpp:341] Iteration 62000, Testing net (#0)
I0217 02:38:12.884362 28930 solver.cpp:409]     Test net output #0: accuracy = 0.3125
I0217 02:38:12.884424 28930 solver.cpp:409]     Test net output #1: loss = 1.82195 (* 1 = 1.82195 loss)
I0217 02:38:13.591296 28930 solver.cpp:237] Iteration 62000, loss = 0.106934
I0217 02:38:13.591341 28930 solver.cpp:253]     Train net output #0: loss = 0.106934 (* 1 = 0.106934 loss)
I0217 02:38:13.591349 28930 sgd_solver.cpp:106] Iteration 62000, lr = 0.0001
I0217 02:40:35.389611 28930 solver.cpp:237] Iteration 62200, loss = 0.11755
I0217 02:40:35.389686 28930 solver.cpp:253]     Train net output #0: loss = 0.11755 (* 1 = 0.11755 loss)
I0217 02:40:35.389695 28930 sgd_solver.cpp:106] Iteration 62200, lr = 0.0001
I0217 02:42:57.305754 28930 solver.cpp:237] Iteration 62400, loss = 0.100445
I0217 02:42:57.305814 28930 solver.cpp:253]     Train net output #0: loss = 0.100445 (* 1 = 0.100445 loss)
I0217 02:42:57.305821 28930 sgd_solver.cpp:106] Iteration 62400, lr = 0.0001
I0217 02:45:19.233830 28930 solver.cpp:237] Iteration 62600, loss = 0.0857856
I0217 02:45:19.233889 28930 solver.cpp:253]     Train net output #0: loss = 0.0857856 (* 1 = 0.0857856 loss)
I0217 02:45:19.233898 28930 sgd_solver.cpp:106] Iteration 62600, lr = 0.0001
I0217 02:47:41.141403 28930 solver.cpp:237] Iteration 62800, loss = 0.110088
I0217 02:47:41.141497 28930 solver.cpp:253]     Train net output #0: loss = 0.110088 (* 1 = 0.110088 loss)
I0217 02:47:41.141507 28930 sgd_solver.cpp:106] Iteration 62800, lr = 0.0001
I0217 02:50:02.428519 28930 solver.cpp:341] Iteration 63000, Testing net (#0)
I0217 02:50:34.297811 28930 solver.cpp:409]     Test net output #0: accuracy = 0.3399
I0217 02:50:34.297868 28930 solver.cpp:409]     Test net output #1: loss = 1.75734 (* 1 = 1.75734 loss)
I0217 02:50:35.006747 28930 solver.cpp:237] Iteration 63000, loss = 0.101728
I0217 02:50:35.006790 28930 solver.cpp:253]     Train net output #0: loss = 0.101728 (* 1 = 0.101728 loss)
I0217 02:50:35.006798 28930 sgd_solver.cpp:106] Iteration 63000, lr = 0.0001
I0217 02:52:57.716423 28930 solver.cpp:237] Iteration 63200, loss = 0.113403
I0217 02:52:57.716511 28930 solver.cpp:253]     Train net output #0: loss = 0.113403 (* 1 = 0.113403 loss)
I0217 02:52:57.716521 28930 sgd_solver.cpp:106] Iteration 63200, lr = 0.0001
I0217 02:55:20.939368 28930 solver.cpp:237] Iteration 63400, loss = 0.096572
I0217 02:55:20.939429 28930 solver.cpp:253]     Train net output #0: loss = 0.096572 (* 1 = 0.096572 loss)
I0217 02:55:20.939436 28930 sgd_solver.cpp:106] Iteration 63400, lr = 0.0001
I0217 02:57:43.701128 28930 solver.cpp:237] Iteration 63600, loss = 0.0835139
I0217 02:57:43.701205 28930 solver.cpp:253]     Train net output #0: loss = 0.0835139 (* 1 = 0.0835139 loss)
I0217 02:57:43.701213 28930 sgd_solver.cpp:106] Iteration 63600, lr = 0.0001
I0217 03:00:06.667420 28930 solver.cpp:237] Iteration 63800, loss = 0.105565
I0217 03:00:06.667480 28930 solver.cpp:253]     Train net output #0: loss = 0.105565 (* 1 = 0.105565 loss)
I0217 03:00:06.667490 28930 sgd_solver.cpp:106] Iteration 63800, lr = 0.0001
I0217 03:02:29.132818 28930 solver.cpp:341] Iteration 64000, Testing net (#0)
I0217 03:03:01.137842 28930 solver.cpp:409]     Test net output #0: accuracy = 0.3681
I0217 03:03:01.137897 28930 solver.cpp:409]     Test net output #1: loss = 1.69154 (* 1 = 1.69154 loss)
I0217 03:03:01.845372 28930 solver.cpp:237] Iteration 64000, loss = 0.0984132
I0217 03:03:01.845415 28930 solver.cpp:253]     Train net output #0: loss = 0.0984132 (* 1 = 0.0984132 loss)
I0217 03:03:01.845423 28930 sgd_solver.cpp:106] Iteration 64000, lr = 0.0001
I0217 03:05:24.475497 28930 solver.cpp:237] Iteration 64200, loss = 0.110193
I0217 03:05:24.475558 28930 solver.cpp:253]     Train net output #0: loss = 0.110193 (* 1 = 0.110193 loss)
I0217 03:05:24.475566 28930 sgd_solver.cpp:106] Iteration 64200, lr = 0.0001
I0217 03:07:47.289686 28930 solver.cpp:237] Iteration 64400, loss = 0.0935699
I0217 03:07:47.289773 28930 solver.cpp:253]     Train net output #0: loss = 0.0935698 (* 1 = 0.0935698 loss)
I0217 03:07:47.289783 28930 sgd_solver.cpp:106] Iteration 64400, lr = 0.0001
I0217 03:10:10.285200 28930 solver.cpp:237] Iteration 64600, loss = 0.0813937
I0217 03:10:10.285264 28930 solver.cpp:253]     Train net output #0: loss = 0.0813937 (* 1 = 0.0813937 loss)
I0217 03:10:10.285272 28930 sgd_solver.cpp:106] Iteration 64600, lr = 0.0001
I0217 03:12:32.435977 28930 solver.cpp:237] Iteration 64800, loss = 0.102094
I0217 03:12:32.436053 28930 solver.cpp:253]     Train net output #0: loss = 0.102094 (* 1 = 0.102094 loss)
I0217 03:12:32.436064 28930 sgd_solver.cpp:106] Iteration 64800, lr = 0.0001
I0217 03:14:54.339108 28930 solver.cpp:472] Snapshotting to HDF5 file examples/cifar10/jungwoo_full_iter_65000.caffemodel.h5
I0217 03:14:54.884299 28930 sgd_solver.cpp:279] Snapshotting solver state to HDF5 file examples/cifar10/jungwoo_full_iter_65000.solverstate.h5
I0217 03:14:55.204123 28930 solver.cpp:321] Iteration 65000, loss = 0.0958305
I0217 03:14:55.204161 28930 solver.cpp:341] Iteration 65000, Testing net (#0)
I0217 03:15:27.159215 28930 solver.cpp:409]     Test net output #0: accuracy = 0.3942
I0217 03:15:27.159273 28930 solver.cpp:409]     Test net output #1: loss = 1.62781 (* 1 = 1.62781 loss)
I0217 03:15:27.159281 28930 solver.cpp:326] Optimization Done.
I0217 03:15:27.159284 28930 caffe.cpp:215] Optimization Done.
I0217 03:15:27.709468 29099 caffe.cpp:177] Use CPU.
I0217 03:15:27.902492 29099 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 1000
base_lr: 1e-05
display: 200
max_iter: 70000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.004
snapshot: 5000
snapshot_prefix: "examples/cifar10/jungwoo_full"
solver_mode: CPU
net: "examples/cifar10/jungwoo_full_train_test.prototxt"
snapshot_format: HDF5
I0217 03:15:27.902602 29099 solver.cpp:91] Creating training net from net file: examples/cifar10/jungwoo_full_train_test.prototxt
I0217 03:15:27.903033 29099 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0217 03:15:27.903054 29099 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0217 03:15:27.903158 29099 net.cpp:49] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TRAIN
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_train_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "push1"
  type: "Pushin"
  bottom: "ip1"
  top: "ip1"
  pushin_param {
    pushin_ratio: 0.2
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0217 03:15:27.903242 29099 layer_factory.hpp:77] Creating layer cifar
I0217 03:15:27.904594 29099 net.cpp:106] Creating Layer cifar
I0217 03:15:27.904609 29099 net.cpp:411] cifar -> data
I0217 03:15:27.904631 29099 net.cpp:411] cifar -> label
I0217 03:15:27.904657 29099 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0217 03:15:27.905786 29103 db_lmdb.cpp:38] Opened lmdb examples/cifar10/cifar10_train_lmdb
I0217 03:15:27.905866 29099 data_layer.cpp:41] output data size: 100,3,32,32
I0217 03:15:27.907819 29099 net.cpp:150] Setting up cifar
I0217 03:15:27.907846 29099 net.cpp:157] Top shape: 100 3 32 32 (307200)
I0217 03:15:27.907852 29099 net.cpp:157] Top shape: 100 (100)
I0217 03:15:27.907856 29099 net.cpp:165] Memory required for data: 1229200
I0217 03:15:27.907866 29099 layer_factory.hpp:77] Creating layer conv1
I0217 03:15:27.907887 29099 net.cpp:106] Creating Layer conv1
I0217 03:15:27.907893 29099 net.cpp:454] conv1 <- data
I0217 03:15:27.907907 29099 net.cpp:411] conv1 -> conv1
I0217 03:15:28.036084 29099 net.cpp:150] Setting up conv1
I0217 03:15:28.036120 29099 net.cpp:157] Top shape: 100 32 32 32 (3276800)
I0217 03:15:28.036125 29099 net.cpp:165] Memory required for data: 14336400
I0217 03:15:28.036142 29099 layer_factory.hpp:77] Creating layer pool1
I0217 03:15:28.036159 29099 net.cpp:106] Creating Layer pool1
I0217 03:15:28.036164 29099 net.cpp:454] pool1 <- conv1
I0217 03:15:28.036170 29099 net.cpp:411] pool1 -> pool1
I0217 03:15:28.036751 29099 net.cpp:150] Setting up pool1
I0217 03:15:28.036763 29099 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0217 03:15:28.036768 29099 net.cpp:165] Memory required for data: 17613200
I0217 03:15:28.036772 29099 layer_factory.hpp:77] Creating layer relu1
I0217 03:15:28.036782 29099 net.cpp:106] Creating Layer relu1
I0217 03:15:28.036785 29099 net.cpp:454] relu1 <- pool1
I0217 03:15:28.036792 29099 net.cpp:397] relu1 -> pool1 (in-place)
I0217 03:15:28.037340 29099 net.cpp:150] Setting up relu1
I0217 03:15:28.037354 29099 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0217 03:15:28.037359 29099 net.cpp:165] Memory required for data: 20890000
I0217 03:15:28.037364 29099 layer_factory.hpp:77] Creating layer norm1
I0217 03:15:28.037374 29099 net.cpp:106] Creating Layer norm1
I0217 03:15:28.037379 29099 net.cpp:454] norm1 <- pool1
I0217 03:15:28.037386 29099 net.cpp:411] norm1 -> norm1
I0217 03:15:28.038197 29099 net.cpp:150] Setting up norm1
I0217 03:15:28.038210 29099 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0217 03:15:28.038215 29099 net.cpp:165] Memory required for data: 24166800
I0217 03:15:28.038219 29099 layer_factory.hpp:77] Creating layer conv2
I0217 03:15:28.038231 29099 net.cpp:106] Creating Layer conv2
I0217 03:15:28.038236 29099 net.cpp:454] conv2 <- norm1
I0217 03:15:28.038244 29099 net.cpp:411] conv2 -> conv2
I0217 03:15:28.040765 29099 net.cpp:150] Setting up conv2
I0217 03:15:28.040781 29099 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0217 03:15:28.040786 29099 net.cpp:165] Memory required for data: 27443600
I0217 03:15:28.040794 29099 layer_factory.hpp:77] Creating layer relu2
I0217 03:15:28.040802 29099 net.cpp:106] Creating Layer relu2
I0217 03:15:28.040807 29099 net.cpp:454] relu2 <- conv2
I0217 03:15:28.040813 29099 net.cpp:397] relu2 -> conv2 (in-place)
I0217 03:15:28.041388 29099 net.cpp:150] Setting up relu2
I0217 03:15:28.041400 29099 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0217 03:15:28.041404 29099 net.cpp:165] Memory required for data: 30720400
I0217 03:15:28.041409 29099 layer_factory.hpp:77] Creating layer pool2
I0217 03:15:28.041417 29099 net.cpp:106] Creating Layer pool2
I0217 03:15:28.041422 29099 net.cpp:454] pool2 <- conv2
I0217 03:15:28.041429 29099 net.cpp:411] pool2 -> pool2
I0217 03:15:28.041988 29099 net.cpp:150] Setting up pool2
I0217 03:15:28.042001 29099 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0217 03:15:28.042006 29099 net.cpp:165] Memory required for data: 31539600
I0217 03:15:28.042009 29099 layer_factory.hpp:77] Creating layer norm2
I0217 03:15:28.042018 29099 net.cpp:106] Creating Layer norm2
I0217 03:15:28.042032 29099 net.cpp:454] norm2 <- pool2
I0217 03:15:28.042045 29099 net.cpp:411] norm2 -> norm2
I0217 03:15:28.042839 29099 net.cpp:150] Setting up norm2
I0217 03:15:28.042852 29099 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0217 03:15:28.042856 29099 net.cpp:165] Memory required for data: 32358800
I0217 03:15:28.042861 29099 layer_factory.hpp:77] Creating layer conv3
I0217 03:15:28.042870 29099 net.cpp:106] Creating Layer conv3
I0217 03:15:28.042876 29099 net.cpp:454] conv3 <- norm2
I0217 03:15:28.042882 29099 net.cpp:411] conv3 -> conv3
I0217 03:15:28.045963 29099 net.cpp:150] Setting up conv3
I0217 03:15:28.045980 29099 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0217 03:15:28.045985 29099 net.cpp:165] Memory required for data: 33997200
I0217 03:15:28.045997 29099 layer_factory.hpp:77] Creating layer relu3
I0217 03:15:28.046005 29099 net.cpp:106] Creating Layer relu3
I0217 03:15:28.046010 29099 net.cpp:454] relu3 <- conv3
I0217 03:15:28.046015 29099 net.cpp:397] relu3 -> conv3 (in-place)
I0217 03:15:28.046604 29099 net.cpp:150] Setting up relu3
I0217 03:15:28.046617 29099 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0217 03:15:28.046622 29099 net.cpp:165] Memory required for data: 35635600
I0217 03:15:28.046627 29099 layer_factory.hpp:77] Creating layer pool3
I0217 03:15:28.046633 29099 net.cpp:106] Creating Layer pool3
I0217 03:15:28.046638 29099 net.cpp:454] pool3 <- conv3
I0217 03:15:28.046655 29099 net.cpp:411] pool3 -> pool3
I0217 03:15:28.047237 29099 net.cpp:150] Setting up pool3
I0217 03:15:28.047250 29099 net.cpp:157] Top shape: 100 64 4 4 (102400)
I0217 03:15:28.047255 29099 net.cpp:165] Memory required for data: 36045200
I0217 03:15:28.047260 29099 layer_factory.hpp:77] Creating layer ip1
I0217 03:15:28.047268 29099 net.cpp:106] Creating Layer ip1
I0217 03:15:28.047273 29099 net.cpp:454] ip1 <- pool3
I0217 03:15:28.047281 29099 net.cpp:411] ip1 -> ip1
I0217 03:15:28.059816 29099 net.cpp:150] Setting up ip1
I0217 03:15:28.059845 29099 net.cpp:157] Top shape: 100 500 (50000)
I0217 03:15:28.059850 29099 net.cpp:165] Memory required for data: 36245200
I0217 03:15:28.059860 29099 layer_factory.hpp:77] Creating layer relu4
I0217 03:15:28.059871 29099 net.cpp:106] Creating Layer relu4
I0217 03:15:28.059876 29099 net.cpp:454] relu4 <- ip1
I0217 03:15:28.059882 29099 net.cpp:397] relu4 -> ip1 (in-place)
I0217 03:15:28.060607 29099 net.cpp:150] Setting up relu4
I0217 03:15:28.060622 29099 net.cpp:157] Top shape: 100 500 (50000)
I0217 03:15:28.060626 29099 net.cpp:165] Memory required for data: 36445200
I0217 03:15:28.060631 29099 layer_factory.hpp:77] Creating layer push1
I0217 03:15:28.060638 29099 net.cpp:106] Creating Layer push1
I0217 03:15:28.060643 29099 net.cpp:454] push1 <- ip1
I0217 03:15:28.060648 29099 net.cpp:397] push1 -> ip1 (in-place)
I0217 03:15:28.060660 29099 net.cpp:150] Setting up push1
I0217 03:15:28.060665 29099 net.cpp:157] Top shape: 100 500 (50000)
I0217 03:15:28.060669 29099 net.cpp:165] Memory required for data: 36645200
I0217 03:15:28.060673 29099 layer_factory.hpp:77] Creating layer ip2
I0217 03:15:28.060683 29099 net.cpp:106] Creating Layer ip2
I0217 03:15:28.060686 29099 net.cpp:454] ip2 <- ip1
I0217 03:15:28.060693 29099 net.cpp:411] ip2 -> ip2
I0217 03:15:28.060832 29099 net.cpp:150] Setting up ip2
I0217 03:15:28.060838 29099 net.cpp:157] Top shape: 100 10 (1000)
I0217 03:15:28.060842 29099 net.cpp:165] Memory required for data: 36649200
I0217 03:15:28.060853 29099 layer_factory.hpp:77] Creating layer loss
I0217 03:15:28.060863 29099 net.cpp:106] Creating Layer loss
I0217 03:15:28.060866 29099 net.cpp:454] loss <- ip2
I0217 03:15:28.060871 29099 net.cpp:454] loss <- label
I0217 03:15:28.060878 29099 net.cpp:411] loss -> loss
I0217 03:15:28.060888 29099 layer_factory.hpp:77] Creating layer loss
I0217 03:15:28.061480 29099 net.cpp:150] Setting up loss
I0217 03:15:28.061492 29099 net.cpp:157] Top shape: (1)
I0217 03:15:28.061496 29099 net.cpp:160]     with loss weight 1
I0217 03:15:28.061511 29099 net.cpp:165] Memory required for data: 36649204
I0217 03:15:28.061523 29099 net.cpp:226] loss needs backward computation.
I0217 03:15:28.061537 29099 net.cpp:226] ip2 needs backward computation.
I0217 03:15:28.061540 29099 net.cpp:226] push1 needs backward computation.
I0217 03:15:28.061544 29099 net.cpp:226] relu4 needs backward computation.
I0217 03:15:28.061548 29099 net.cpp:226] ip1 needs backward computation.
I0217 03:15:28.061553 29099 net.cpp:226] pool3 needs backward computation.
I0217 03:15:28.061556 29099 net.cpp:226] relu3 needs backward computation.
I0217 03:15:28.061560 29099 net.cpp:226] conv3 needs backward computation.
I0217 03:15:28.061564 29099 net.cpp:226] norm2 needs backward computation.
I0217 03:15:28.061568 29099 net.cpp:226] pool2 needs backward computation.
I0217 03:15:28.061573 29099 net.cpp:226] relu2 needs backward computation.
I0217 03:15:28.061576 29099 net.cpp:226] conv2 needs backward computation.
I0217 03:15:28.061580 29099 net.cpp:226] norm1 needs backward computation.
I0217 03:15:28.061584 29099 net.cpp:226] relu1 needs backward computation.
I0217 03:15:28.061589 29099 net.cpp:226] pool1 needs backward computation.
I0217 03:15:28.061592 29099 net.cpp:226] conv1 needs backward computation.
I0217 03:15:28.061596 29099 net.cpp:228] cifar does not need backward computation.
I0217 03:15:28.061600 29099 net.cpp:270] This network produces output loss
I0217 03:15:28.061612 29099 net.cpp:283] Network initialization done.
I0217 03:15:28.062021 29099 solver.cpp:181] Creating test net (#0) specified by net file: examples/cifar10/jungwoo_full_train_test.prototxt
I0217 03:15:28.062052 29099 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0217 03:15:28.062165 29099 net.cpp:49] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "examples/cifar10/mean.binaryproto"
  }
  data_param {
    source: "examples/cifar10/cifar10_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "push1"
  type: "Pushin"
  bottom: "ip1"
  top: "ip1"
  pushin_param {
    pushin_ratio: 0.2
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0217 03:15:28.062255 29099 layer_factory.hpp:77] Creating layer cifar
I0217 03:15:28.062353 29099 net.cpp:106] Creating Layer cifar
I0217 03:15:28.062363 29099 net.cpp:411] cifar -> data
I0217 03:15:28.062374 29099 net.cpp:411] cifar -> label
I0217 03:15:28.062383 29099 data_transformer.cpp:25] Loading mean file from: examples/cifar10/mean.binaryproto
I0217 03:15:28.063612 29105 db_lmdb.cpp:38] Opened lmdb examples/cifar10/cifar10_test_lmdb
I0217 03:15:28.063663 29099 data_layer.cpp:41] output data size: 100,3,32,32
I0217 03:15:28.065748 29099 net.cpp:150] Setting up cifar
I0217 03:15:28.065775 29099 net.cpp:157] Top shape: 100 3 32 32 (307200)
I0217 03:15:28.065781 29099 net.cpp:157] Top shape: 100 (100)
I0217 03:15:28.065786 29099 net.cpp:165] Memory required for data: 1229200
I0217 03:15:28.065793 29099 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0217 03:15:28.065804 29099 net.cpp:106] Creating Layer label_cifar_1_split
I0217 03:15:28.065810 29099 net.cpp:454] label_cifar_1_split <- label
I0217 03:15:28.065819 29099 net.cpp:411] label_cifar_1_split -> label_cifar_1_split_0
I0217 03:15:28.065829 29099 net.cpp:411] label_cifar_1_split -> label_cifar_1_split_1
I0217 03:15:28.065839 29099 net.cpp:150] Setting up label_cifar_1_split
I0217 03:15:28.065843 29099 net.cpp:157] Top shape: 100 (100)
I0217 03:15:28.065848 29099 net.cpp:157] Top shape: 100 (100)
I0217 03:15:28.065852 29099 net.cpp:165] Memory required for data: 1230000
I0217 03:15:28.065856 29099 layer_factory.hpp:77] Creating layer conv1
I0217 03:15:28.065867 29099 net.cpp:106] Creating Layer conv1
I0217 03:15:28.065872 29099 net.cpp:454] conv1 <- data
I0217 03:15:28.065881 29099 net.cpp:411] conv1 -> conv1
I0217 03:15:28.068056 29099 net.cpp:150] Setting up conv1
I0217 03:15:28.068074 29099 net.cpp:157] Top shape: 100 32 32 32 (3276800)
I0217 03:15:28.068079 29099 net.cpp:165] Memory required for data: 14337200
I0217 03:15:28.068090 29099 layer_factory.hpp:77] Creating layer pool1
I0217 03:15:28.068099 29099 net.cpp:106] Creating Layer pool1
I0217 03:15:28.068104 29099 net.cpp:454] pool1 <- conv1
I0217 03:15:28.068112 29099 net.cpp:411] pool1 -> pool1
I0217 03:15:28.068701 29099 net.cpp:150] Setting up pool1
I0217 03:15:28.068714 29099 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0217 03:15:28.068719 29099 net.cpp:165] Memory required for data: 17614000
I0217 03:15:28.068723 29099 layer_factory.hpp:77] Creating layer relu1
I0217 03:15:28.068733 29099 net.cpp:106] Creating Layer relu1
I0217 03:15:28.068737 29099 net.cpp:454] relu1 <- pool1
I0217 03:15:28.068743 29099 net.cpp:397] relu1 -> pool1 (in-place)
I0217 03:15:28.069320 29099 net.cpp:150] Setting up relu1
I0217 03:15:28.069334 29099 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0217 03:15:28.069337 29099 net.cpp:165] Memory required for data: 20890800
I0217 03:15:28.069342 29099 layer_factory.hpp:77] Creating layer norm1
I0217 03:15:28.069351 29099 net.cpp:106] Creating Layer norm1
I0217 03:15:28.069357 29099 net.cpp:454] norm1 <- pool1
I0217 03:15:28.069362 29099 net.cpp:411] norm1 -> norm1
I0217 03:15:28.070165 29099 net.cpp:150] Setting up norm1
I0217 03:15:28.070194 29099 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0217 03:15:28.070199 29099 net.cpp:165] Memory required for data: 24167600
I0217 03:15:28.070204 29099 layer_factory.hpp:77] Creating layer conv2
I0217 03:15:28.070214 29099 net.cpp:106] Creating Layer conv2
I0217 03:15:28.070221 29099 net.cpp:454] conv2 <- norm1
I0217 03:15:28.070230 29099 net.cpp:411] conv2 -> conv2
I0217 03:15:28.072840 29099 net.cpp:150] Setting up conv2
I0217 03:15:28.072862 29099 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0217 03:15:28.072867 29099 net.cpp:165] Memory required for data: 27444400
I0217 03:15:28.072880 29099 layer_factory.hpp:77] Creating layer relu2
I0217 03:15:28.072886 29099 net.cpp:106] Creating Layer relu2
I0217 03:15:28.072891 29099 net.cpp:454] relu2 <- conv2
I0217 03:15:28.072897 29099 net.cpp:397] relu2 -> conv2 (in-place)
I0217 03:15:28.073479 29099 net.cpp:150] Setting up relu2
I0217 03:15:28.073493 29099 net.cpp:157] Top shape: 100 32 16 16 (819200)
I0217 03:15:28.073498 29099 net.cpp:165] Memory required for data: 30721200
I0217 03:15:28.073503 29099 layer_factory.hpp:77] Creating layer pool2
I0217 03:15:28.073511 29099 net.cpp:106] Creating Layer pool2
I0217 03:15:28.073516 29099 net.cpp:454] pool2 <- conv2
I0217 03:15:28.073523 29099 net.cpp:411] pool2 -> pool2
I0217 03:15:28.074110 29099 net.cpp:150] Setting up pool2
I0217 03:15:28.074125 29099 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0217 03:15:28.074128 29099 net.cpp:165] Memory required for data: 31540400
I0217 03:15:28.074133 29099 layer_factory.hpp:77] Creating layer norm2
I0217 03:15:28.074143 29099 net.cpp:106] Creating Layer norm2
I0217 03:15:28.074147 29099 net.cpp:454] norm2 <- pool2
I0217 03:15:28.074153 29099 net.cpp:411] norm2 -> norm2
I0217 03:15:28.074965 29099 net.cpp:150] Setting up norm2
I0217 03:15:28.074980 29099 net.cpp:157] Top shape: 100 32 8 8 (204800)
I0217 03:15:28.074985 29099 net.cpp:165] Memory required for data: 32359600
I0217 03:15:28.074990 29099 layer_factory.hpp:77] Creating layer conv3
I0217 03:15:28.074998 29099 net.cpp:106] Creating Layer conv3
I0217 03:15:28.075003 29099 net.cpp:454] conv3 <- norm2
I0217 03:15:28.075011 29099 net.cpp:411] conv3 -> conv3
I0217 03:15:28.078156 29099 net.cpp:150] Setting up conv3
I0217 03:15:28.078172 29099 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0217 03:15:28.078176 29099 net.cpp:165] Memory required for data: 33998000
I0217 03:15:28.078186 29099 layer_factory.hpp:77] Creating layer relu3
I0217 03:15:28.078194 29099 net.cpp:106] Creating Layer relu3
I0217 03:15:28.078198 29099 net.cpp:454] relu3 <- conv3
I0217 03:15:28.078205 29099 net.cpp:397] relu3 -> conv3 (in-place)
I0217 03:15:28.078795 29099 net.cpp:150] Setting up relu3
I0217 03:15:28.078809 29099 net.cpp:157] Top shape: 100 64 8 8 (409600)
I0217 03:15:28.078814 29099 net.cpp:165] Memory required for data: 35636400
I0217 03:15:28.078819 29099 layer_factory.hpp:77] Creating layer pool3
I0217 03:15:28.078826 29099 net.cpp:106] Creating Layer pool3
I0217 03:15:28.078831 29099 net.cpp:454] pool3 <- conv3
I0217 03:15:28.078836 29099 net.cpp:411] pool3 -> pool3
I0217 03:15:28.079419 29099 net.cpp:150] Setting up pool3
I0217 03:15:28.079432 29099 net.cpp:157] Top shape: 100 64 4 4 (102400)
I0217 03:15:28.079437 29099 net.cpp:165] Memory required for data: 36046000
I0217 03:15:28.079442 29099 layer_factory.hpp:77] Creating layer ip1
I0217 03:15:28.079449 29099 net.cpp:106] Creating Layer ip1
I0217 03:15:28.079453 29099 net.cpp:454] ip1 <- pool3
I0217 03:15:28.079462 29099 net.cpp:411] ip1 -> ip1
I0217 03:15:28.091986 29099 net.cpp:150] Setting up ip1
I0217 03:15:28.092016 29099 net.cpp:157] Top shape: 100 500 (50000)
I0217 03:15:28.092020 29099 net.cpp:165] Memory required for data: 36246000
I0217 03:15:28.092031 29099 layer_factory.hpp:77] Creating layer relu4
I0217 03:15:28.092039 29099 net.cpp:106] Creating Layer relu4
I0217 03:15:28.092044 29099 net.cpp:454] relu4 <- ip1
I0217 03:15:28.092052 29099 net.cpp:397] relu4 -> ip1 (in-place)
I0217 03:15:28.092824 29099 net.cpp:150] Setting up relu4
I0217 03:15:28.092854 29099 net.cpp:157] Top shape: 100 500 (50000)
I0217 03:15:28.092859 29099 net.cpp:165] Memory required for data: 36446000
I0217 03:15:28.092864 29099 layer_factory.hpp:77] Creating layer push1
I0217 03:15:28.092875 29099 net.cpp:106] Creating Layer push1
I0217 03:15:28.092880 29099 net.cpp:454] push1 <- ip1
I0217 03:15:28.092886 29099 net.cpp:397] push1 -> ip1 (in-place)
I0217 03:15:28.092896 29099 net.cpp:150] Setting up push1
I0217 03:15:28.092901 29099 net.cpp:157] Top shape: 100 500 (50000)
I0217 03:15:28.092905 29099 net.cpp:165] Memory required for data: 36646000
I0217 03:15:28.092910 29099 layer_factory.hpp:77] Creating layer ip2
I0217 03:15:28.092921 29099 net.cpp:106] Creating Layer ip2
I0217 03:15:28.092926 29099 net.cpp:454] ip2 <- ip1
I0217 03:15:28.092932 29099 net.cpp:411] ip2 -> ip2
I0217 03:15:28.093078 29099 net.cpp:150] Setting up ip2
I0217 03:15:28.093086 29099 net.cpp:157] Top shape: 100 10 (1000)
I0217 03:15:28.093089 29099 net.cpp:165] Memory required for data: 36650000
I0217 03:15:28.093099 29099 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0217 03:15:28.093106 29099 net.cpp:106] Creating Layer ip2_ip2_0_split
I0217 03:15:28.093111 29099 net.cpp:454] ip2_ip2_0_split <- ip2
I0217 03:15:28.093117 29099 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0217 03:15:28.093123 29099 net.cpp:411] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0217 03:15:28.093132 29099 net.cpp:150] Setting up ip2_ip2_0_split
I0217 03:15:28.093137 29099 net.cpp:157] Top shape: 100 10 (1000)
I0217 03:15:28.093140 29099 net.cpp:157] Top shape: 100 10 (1000)
I0217 03:15:28.093145 29099 net.cpp:165] Memory required for data: 36658000
I0217 03:15:28.093149 29099 layer_factory.hpp:77] Creating layer accuracy
I0217 03:15:28.093157 29099 net.cpp:106] Creating Layer accuracy
I0217 03:15:28.093160 29099 net.cpp:454] accuracy <- ip2_ip2_0_split_0
I0217 03:15:28.093165 29099 net.cpp:454] accuracy <- label_cifar_1_split_0
I0217 03:15:28.093170 29099 net.cpp:411] accuracy -> accuracy
I0217 03:15:28.093179 29099 net.cpp:150] Setting up accuracy
I0217 03:15:28.093185 29099 net.cpp:157] Top shape: (1)
I0217 03:15:28.093189 29099 net.cpp:165] Memory required for data: 36658004
I0217 03:15:28.093192 29099 layer_factory.hpp:77] Creating layer loss
I0217 03:15:28.093200 29099 net.cpp:106] Creating Layer loss
I0217 03:15:28.093204 29099 net.cpp:454] loss <- ip2_ip2_0_split_1
I0217 03:15:28.093209 29099 net.cpp:454] loss <- label_cifar_1_split_1
I0217 03:15:28.093215 29099 net.cpp:411] loss -> loss
I0217 03:15:28.093224 29099 layer_factory.hpp:77] Creating layer loss
I0217 03:15:28.093829 29099 net.cpp:150] Setting up loss
I0217 03:15:28.093842 29099 net.cpp:157] Top shape: (1)
I0217 03:15:28.093847 29099 net.cpp:160]     with loss weight 1
I0217 03:15:28.093854 29099 net.cpp:165] Memory required for data: 36658008
I0217 03:15:28.093858 29099 net.cpp:226] loss needs backward computation.
I0217 03:15:28.093863 29099 net.cpp:228] accuracy does not need backward computation.
I0217 03:15:28.093868 29099 net.cpp:226] ip2_ip2_0_split needs backward computation.
I0217 03:15:28.093871 29099 net.cpp:226] ip2 needs backward computation.
I0217 03:15:28.093875 29099 net.cpp:226] push1 needs backward computation.
I0217 03:15:28.093879 29099 net.cpp:226] relu4 needs backward computation.
I0217 03:15:28.093883 29099 net.cpp:226] ip1 needs backward computation.
I0217 03:15:28.093888 29099 net.cpp:226] pool3 needs backward computation.
I0217 03:15:28.093891 29099 net.cpp:226] relu3 needs backward computation.
I0217 03:15:28.093895 29099 net.cpp:226] conv3 needs backward computation.
I0217 03:15:28.093899 29099 net.cpp:226] norm2 needs backward computation.
I0217 03:15:28.093904 29099 net.cpp:226] pool2 needs backward computation.
I0217 03:15:28.093907 29099 net.cpp:226] relu2 needs backward computation.
I0217 03:15:28.093911 29099 net.cpp:226] conv2 needs backward computation.
I0217 03:15:28.093915 29099 net.cpp:226] norm1 needs backward computation.
I0217 03:15:28.093919 29099 net.cpp:226] relu1 needs backward computation.
I0217 03:15:28.093930 29099 net.cpp:226] pool1 needs backward computation.
I0217 03:15:28.093940 29099 net.cpp:226] conv1 needs backward computation.
I0217 03:15:28.093945 29099 net.cpp:228] label_cifar_1_split does not need backward computation.
I0217 03:15:28.093948 29099 net.cpp:228] cifar does not need backward computation.
I0217 03:15:28.093952 29099 net.cpp:270] This network produces output accuracy
I0217 03:15:28.093956 29099 net.cpp:270] This network produces output loss
I0217 03:15:28.093971 29099 net.cpp:283] Network initialization done.
I0217 03:15:28.094048 29099 solver.cpp:60] Solver scaffolding done.
I0217 03:15:28.094076 29099 caffe.cpp:202] Resuming from examples/cifar10/jungwoo_full_iter_65000.solverstate.h5
I0217 03:15:28.095095 29099 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0217 03:15:28.097448 29099 caffe.cpp:212] Starting Optimization
I0217 03:15:28.097472 29099 solver.cpp:288] Solving CIFAR10_full
I0217 03:15:28.097477 29099 solver.cpp:289] Learning Rate Policy: fixed
I0217 03:15:28.098171 29099 solver.cpp:341] Iteration 65000, Testing net (#0)
I0217 03:16:00.018487 29099 solver.cpp:409]     Test net output #0: accuracy = 0.2578
I0217 03:16:00.018627 29099 solver.cpp:409]     Test net output #1: loss = 1.9576 (* 1 = 1.9576 loss)
I0217 03:16:00.747393 29099 solver.cpp:237] Iteration 65000, loss = 1.8972
I0217 03:16:00.747436 29099 solver.cpp:253]     Train net output #0: loss = 1.8972 (* 1 = 1.8972 loss)
I0217 03:16:00.747445 29099 sgd_solver.cpp:106] Iteration 65000, lr = 1e-05
I0217 03:18:23.177208 29099 solver.cpp:237] Iteration 65200, loss = 1.52118
I0217 03:18:23.177287 29099 solver.cpp:253]     Train net output #0: loss = 1.52118 (* 1 = 1.52118 loss)
I0217 03:18:23.177297 29099 sgd_solver.cpp:106] Iteration 65200, lr = 1e-05
I0217 03:20:45.424700 29099 solver.cpp:237] Iteration 65400, loss = 1.17777
I0217 03:20:45.424788 29099 solver.cpp:253]     Train net output #0: loss = 1.17777 (* 1 = 1.17777 loss)
I0217 03:20:45.424798 29099 sgd_solver.cpp:106] Iteration 65400, lr = 1e-05
I0217 03:23:07.737007 29099 solver.cpp:237] Iteration 65600, loss = 0.831366
I0217 03:23:07.737097 29099 solver.cpp:253]     Train net output #0: loss = 0.831366 (* 1 = 0.831366 loss)
I0217 03:23:07.737107 29099 sgd_solver.cpp:106] Iteration 65600, lr = 1e-05
I0217 03:25:30.188659 29099 solver.cpp:237] Iteration 65800, loss = 0.460924
I0217 03:25:30.188740 29099 solver.cpp:253]     Train net output #0: loss = 0.460924 (* 1 = 0.460924 loss)
I0217 03:25:30.188750 29099 sgd_solver.cpp:106] Iteration 65800, lr = 1e-05
I0217 03:27:51.777680 29099 solver.cpp:341] Iteration 66000, Testing net (#0)
I0217 03:28:23.728488 29099 solver.cpp:409]     Test net output #0: accuracy = 0.2865
I0217 03:28:23.728561 29099 solver.cpp:409]     Test net output #1: loss = 1.88542 (* 1 = 1.88542 loss)
I0217 03:28:24.439846 29099 solver.cpp:237] Iteration 66000, loss = 0.104184
I0217 03:28:24.439889 29099 solver.cpp:253]     Train net output #0: loss = 0.104184 (* 1 = 0.104184 loss)
I0217 03:28:24.439898 29099 sgd_solver.cpp:106] Iteration 66000, lr = 1e-05
I0217 03:30:47.062575 29099 solver.cpp:237] Iteration 66200, loss = 0.118064
I0217 03:30:47.062687 29099 solver.cpp:253]     Train net output #0: loss = 0.118064 (* 1 = 0.118064 loss)
I0217 03:30:47.062697 29099 sgd_solver.cpp:106] Iteration 66200, lr = 1e-05
I0217 03:33:09.492583 29099 solver.cpp:237] Iteration 66400, loss = 0.079106
I0217 03:33:09.492673 29099 solver.cpp:253]     Train net output #0: loss = 0.079106 (* 1 = 0.079106 loss)
I0217 03:33:09.492684 29099 sgd_solver.cpp:106] Iteration 66400, lr = 1e-05
I0217 03:35:32.097563 29099 solver.cpp:237] Iteration 66600, loss = 0.0768135
I0217 03:35:32.097650 29099 solver.cpp:253]     Train net output #0: loss = 0.0768135 (* 1 = 0.0768135 loss)
I0217 03:35:32.097659 29099 sgd_solver.cpp:106] Iteration 66600, lr = 1e-05
I0217 03:37:54.649870 29099 solver.cpp:237] Iteration 66800, loss = 0.105598
I0217 03:37:54.649981 29099 solver.cpp:253]     Train net output #0: loss = 0.105598 (* 1 = 0.105598 loss)
I0217 03:37:54.649997 29099 sgd_solver.cpp:106] Iteration 66800, lr = 1e-05
I0217 03:40:16.923180 29099 solver.cpp:341] Iteration 67000, Testing net (#0)
I0217 03:40:48.884702 29099 solver.cpp:409]     Test net output #0: accuracy = 0.3133
I0217 03:40:48.884778 29099 solver.cpp:409]     Test net output #1: loss = 1.82003 (* 1 = 1.82003 loss)
I0217 03:40:49.600081 29099 solver.cpp:237] Iteration 67000, loss = 0.104805
I0217 03:40:49.600129 29099 solver.cpp:253]     Train net output #0: loss = 0.104805 (* 1 = 0.104805 loss)
I0217 03:40:49.600138 29099 sgd_solver.cpp:106] Iteration 67000, lr = 1e-05
I0217 03:43:11.856809 29099 solver.cpp:237] Iteration 67200, loss = 0.111965
I0217 03:43:11.856896 29099 solver.cpp:253]     Train net output #0: loss = 0.111965 (* 1 = 0.111965 loss)
I0217 03:43:11.856907 29099 sgd_solver.cpp:106] Iteration 67200, lr = 1e-05
I0217 03:45:35.031443 29099 solver.cpp:237] Iteration 67400, loss = 0.0809487
I0217 03:45:35.031505 29099 solver.cpp:253]     Train net output #0: loss = 0.0809487 (* 1 = 0.0809487 loss)
I0217 03:45:35.031515 29099 sgd_solver.cpp:106] Iteration 67400, lr = 1e-05
I0217 03:47:57.908861 29099 solver.cpp:237] Iteration 67600, loss = 0.0734361
I0217 03:47:57.908949 29099 solver.cpp:253]     Train net output #0: loss = 0.0734361 (* 1 = 0.0734361 loss)
I0217 03:47:57.908959 29099 sgd_solver.cpp:106] Iteration 67600, lr = 1e-05
I0217 03:50:20.426304 29099 solver.cpp:237] Iteration 67800, loss = 0.105421
I0217 03:50:20.426393 29099 solver.cpp:253]     Train net output #0: loss = 0.105421 (* 1 = 0.105421 loss)
I0217 03:50:20.426403 29099 sgd_solver.cpp:106] Iteration 67800, lr = 1e-05
I0217 03:52:42.196655 29099 solver.cpp:341] Iteration 68000, Testing net (#0)
I0217 03:53:14.308272 29099 solver.cpp:409]     Test net output #0: accuracy = 0.3404
I0217 03:53:14.308363 29099 solver.cpp:409]     Test net output #1: loss = 1.75451 (* 1 = 1.75451 loss)
I0217 03:53:15.020761 29099 solver.cpp:237] Iteration 68000, loss = 0.10341
I0217 03:53:15.020807 29099 solver.cpp:253]     Train net output #0: loss = 0.10341 (* 1 = 0.10341 loss)
I0217 03:53:15.020815 29099 sgd_solver.cpp:106] Iteration 68000, lr = 1e-05
I0217 03:55:38.313491 29099 solver.cpp:237] Iteration 68200, loss = 0.109081
I0217 03:55:38.313576 29099 solver.cpp:253]     Train net output #0: loss = 0.109081 (* 1 = 0.109081 loss)
I0217 03:55:38.313586 29099 sgd_solver.cpp:106] Iteration 68200, lr = 1e-05
I0217 03:58:00.970485 29099 solver.cpp:237] Iteration 68400, loss = 0.0820269
I0217 03:58:00.970578 29099 solver.cpp:253]     Train net output #0: loss = 0.0820269 (* 1 = 0.0820269 loss)
I0217 03:58:00.970589 29099 sgd_solver.cpp:106] Iteration 68400, lr = 1e-05
I0217 04:00:23.364132 29099 solver.cpp:237] Iteration 68600, loss = 0.0717558
I0217 04:00:23.364223 29099 solver.cpp:253]     Train net output #0: loss = 0.0717558 (* 1 = 0.0717558 loss)
I0217 04:00:23.364233 29099 sgd_solver.cpp:106] Iteration 68600, lr = 1e-05
I0217 04:02:46.152191 29099 solver.cpp:237] Iteration 68800, loss = 0.105465
I0217 04:02:46.152267 29099 solver.cpp:253]     Train net output #0: loss = 0.105465 (* 1 = 0.105465 loss)
I0217 04:02:46.152277 29099 sgd_solver.cpp:106] Iteration 68800, lr = 1e-05
I0217 04:05:08.009358 29099 solver.cpp:341] Iteration 69000, Testing net (#0)
I0217 04:05:40.036954 29099 solver.cpp:409]     Test net output #0: accuracy = 0.3691
I0217 04:05:40.037012 29099 solver.cpp:409]     Test net output #1: loss = 1.68789 (* 1 = 1.68789 loss)
I0217 04:05:40.760237 29099 solver.cpp:237] Iteration 69000, loss = 0.10216
I0217 04:05:40.760284 29099 solver.cpp:253]     Train net output #0: loss = 0.10216 (* 1 = 0.10216 loss)
I0217 04:05:40.760294 29099 sgd_solver.cpp:106] Iteration 69000, lr = 1e-05
I0217 04:08:03.396525 29099 solver.cpp:237] Iteration 69200, loss = 0.107408
I0217 04:08:03.396589 29099 solver.cpp:253]     Train net output #0: loss = 0.107408 (* 1 = 0.107408 loss)
I0217 04:08:03.396600 29099 sgd_solver.cpp:106] Iteration 69200, lr = 1e-05
I0217 04:10:26.188714 29099 solver.cpp:237] Iteration 69400, loss = 0.0825891
I0217 04:10:26.188809 29099 solver.cpp:253]     Train net output #0: loss = 0.0825891 (* 1 = 0.0825891 loss)
I0217 04:10:26.188819 29099 sgd_solver.cpp:106] Iteration 69400, lr = 1e-05
I0217 04:12:48.873982 29099 solver.cpp:237] Iteration 69600, loss = 0.0707766
I0217 04:12:48.874069 29099 solver.cpp:253]     Train net output #0: loss = 0.0707766 (* 1 = 0.0707766 loss)
I0217 04:12:48.874080 29099 sgd_solver.cpp:106] Iteration 69600, lr = 1e-05
I0217 04:15:11.656680 29099 solver.cpp:237] Iteration 69800, loss = 0.105366
I0217 04:15:11.656761 29099 solver.cpp:253]     Train net output #0: loss = 0.105366 (* 1 = 0.105366 loss)
I0217 04:15:11.656771 29099 sgd_solver.cpp:106] Iteration 69800, lr = 1e-05
I0217 04:17:34.275671 29099 solver.cpp:472] Snapshotting to HDF5 file examples/cifar10/jungwoo_full_iter_70000.caffemodel.h5
I0217 04:17:34.823442 29099 sgd_solver.cpp:279] Snapshotting solver state to HDF5 file examples/cifar10/jungwoo_full_iter_70000.solverstate.h5
I0217 04:17:35.145393 29099 solver.cpp:321] Iteration 70000, loss = 0.101192
I0217 04:17:35.145433 29099 solver.cpp:341] Iteration 70000, Testing net (#0)
I0217 04:18:07.222133 29099 solver.cpp:409]     Test net output #0: accuracy = 0.3951
I0217 04:18:07.222213 29099 solver.cpp:409]     Test net output #1: loss = 1.62371 (* 1 = 1.62371 loss)
I0217 04:18:07.222221 29099 solver.cpp:326] Optimization Done.
I0217 04:18:07.222226 29099 caffe.cpp:215] Optimization Done.
