I0311 14:30:29.148916 31185 caffe.cpp:184] Using GPUs 0
I0311 14:30:29.386251 31185 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 10000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 2000
snapshot_prefix: "examples/mlp/mlp_absinner"
solver_mode: GPU
device_id: 0
net: "examples/mlp/mlp_train_test.prototxt"
I0311 14:30:29.386401 31185 solver.cpp:91] Creating training net from net file: examples/mlp/mlp_train_test.prototxt
I0311 14:30:29.386747 31185 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer MLP
I0311 14:30:29.386766 31185 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0311 14:30:29.386842 31185 net.cpp:49] Initializing net from parameters: 
name: "MLP"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "data"
  top: "ip1"
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "relu1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "relu1"
  top: "ip2"
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "relu2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "relu2"
  top: "ip3"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0311 14:30:29.386917 31185 layer_factory.hpp:77] Creating layer mnist
I0311 14:30:29.387586 31185 net.cpp:106] Creating Layer mnist
I0311 14:30:29.387606 31185 net.cpp:411] mnist -> data
I0311 14:30:29.387647 31185 net.cpp:411] mnist -> label
I0311 14:30:29.388568 31189 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_train_lmdb
I0311 14:30:29.402503 31185 data_layer.cpp:41] output data size: 100,1,28,28
I0311 14:30:29.404292 31185 net.cpp:150] Setting up mnist
I0311 14:30:29.404321 31185 net.cpp:157] Top shape: 100 1 28 28 (78400)
I0311 14:30:29.404332 31185 net.cpp:157] Top shape: 100 (100)
I0311 14:30:29.404338 31185 net.cpp:165] Memory required for data: 314000
I0311 14:30:29.404351 31185 layer_factory.hpp:77] Creating layer ip1
I0311 14:30:29.404373 31185 net.cpp:106] Creating Layer ip1
I0311 14:30:29.404383 31185 net.cpp:454] ip1 <- data
I0311 14:30:29.404399 31185 net.cpp:411] ip1 -> ip1
I0311 14:30:29.408392 31185 net.cpp:150] Setting up ip1
I0311 14:30:29.408422 31185 net.cpp:157] Top shape: 100 500 (50000)
I0311 14:30:29.408428 31185 net.cpp:165] Memory required for data: 514000
I0311 14:30:29.408464 31185 layer_factory.hpp:77] Creating layer relu1
I0311 14:30:29.408488 31185 net.cpp:106] Creating Layer relu1
I0311 14:30:29.408495 31185 net.cpp:454] relu1 <- ip1
I0311 14:30:29.408506 31185 net.cpp:411] relu1 -> relu1
I0311 14:30:29.581291 31185 net.cpp:150] Setting up relu1
I0311 14:30:29.581334 31185 net.cpp:157] Top shape: 100 500 (50000)
I0311 14:30:29.581344 31185 net.cpp:165] Memory required for data: 714000
I0311 14:30:29.581353 31185 layer_factory.hpp:77] Creating layer ip2
I0311 14:30:29.581368 31185 net.cpp:106] Creating Layer ip2
I0311 14:30:29.581377 31185 net.cpp:454] ip2 <- relu1
I0311 14:30:29.581388 31185 net.cpp:411] ip2 -> ip2
I0311 14:30:29.582193 31185 net.cpp:150] Setting up ip2
I0311 14:30:29.582216 31185 net.cpp:157] Top shape: 100 50 (5000)
I0311 14:30:29.582224 31185 net.cpp:165] Memory required for data: 734000
I0311 14:30:29.582240 31185 layer_factory.hpp:77] Creating layer relu2
I0311 14:30:29.582255 31185 net.cpp:106] Creating Layer relu2
I0311 14:30:29.582285 31185 net.cpp:454] relu2 <- ip2
I0311 14:30:29.582298 31185 net.cpp:411] relu2 -> relu2
I0311 14:30:29.583212 31185 net.cpp:150] Setting up relu2
I0311 14:30:29.583232 31185 net.cpp:157] Top shape: 100 50 (5000)
I0311 14:30:29.583240 31185 net.cpp:165] Memory required for data: 754000
I0311 14:30:29.583246 31185 layer_factory.hpp:77] Creating layer ip3
I0311 14:30:29.583256 31185 net.cpp:106] Creating Layer ip3
I0311 14:30:29.583262 31185 net.cpp:454] ip3 <- relu2
I0311 14:30:29.583271 31185 net.cpp:411] ip3 -> ip3
I0311 14:30:29.583411 31185 net.cpp:150] Setting up ip3
I0311 14:30:29.583427 31185 net.cpp:157] Top shape: 100 10 (1000)
I0311 14:30:29.583434 31185 net.cpp:165] Memory required for data: 758000
I0311 14:30:29.583447 31185 layer_factory.hpp:77] Creating layer loss
I0311 14:30:29.583458 31185 net.cpp:106] Creating Layer loss
I0311 14:30:29.583466 31185 net.cpp:454] loss <- ip3
I0311 14:30:29.583472 31185 net.cpp:454] loss <- label
I0311 14:30:29.583482 31185 net.cpp:411] loss -> loss
I0311 14:30:29.583508 31185 layer_factory.hpp:77] Creating layer loss
I0311 14:30:29.584383 31185 net.cpp:150] Setting up loss
I0311 14:30:29.584401 31185 net.cpp:157] Top shape: (1)
I0311 14:30:29.584408 31185 net.cpp:160]     with loss weight 1
I0311 14:30:29.584427 31185 net.cpp:165] Memory required for data: 758004
I0311 14:30:29.584434 31185 net.cpp:226] loss needs backward computation.
I0311 14:30:29.584440 31185 net.cpp:226] ip3 needs backward computation.
I0311 14:30:29.584445 31185 net.cpp:226] relu2 needs backward computation.
I0311 14:30:29.584450 31185 net.cpp:226] ip2 needs backward computation.
I0311 14:30:29.584455 31185 net.cpp:226] relu1 needs backward computation.
I0311 14:30:29.584460 31185 net.cpp:226] ip1 needs backward computation.
I0311 14:30:29.584466 31185 net.cpp:228] mnist does not need backward computation.
I0311 14:30:29.584471 31185 net.cpp:270] This network produces output loss
I0311 14:30:29.584482 31185 net.cpp:283] Network initialization done.
I0311 14:30:29.584825 31185 solver.cpp:181] Creating test net (#0) specified by net file: examples/mlp/mlp_train_test.prototxt
I0311 14:30:29.584862 31185 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0311 14:30:29.584960 31185 net.cpp:49] Initializing net from parameters: 
name: "MLP"
state {
  phase: TEST
}
layer {
  name: "MLP"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "data"
  top: "ip1"
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "relu1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "relu1"
  top: "ip2"
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "relu2"
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "relu2"
  top: "ip3"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0311 14:30:29.585057 31185 layer_factory.hpp:77] Creating layer MLP
I0311 14:30:29.585204 31185 net.cpp:106] Creating Layer MLP
I0311 14:30:29.585230 31185 net.cpp:411] MLP -> data
I0311 14:30:29.585247 31185 net.cpp:411] MLP -> label
I0311 14:30:29.586174 31191 db_lmdb.cpp:38] Opened lmdb examples/mnist/mnist_test_lmdb
I0311 14:30:29.586323 31185 data_layer.cpp:41] output data size: 100,1,28,28
I0311 14:30:29.587823 31185 net.cpp:150] Setting up MLP
I0311 14:30:29.587870 31185 net.cpp:157] Top shape: 100 1 28 28 (78400)
I0311 14:30:29.587882 31185 net.cpp:157] Top shape: 100 (100)
I0311 14:30:29.587888 31185 net.cpp:165] Memory required for data: 314000
I0311 14:30:29.587894 31185 layer_factory.hpp:77] Creating layer label_MLP_1_split
I0311 14:30:29.587911 31185 net.cpp:106] Creating Layer label_MLP_1_split
I0311 14:30:29.587919 31185 net.cpp:454] label_MLP_1_split <- label
I0311 14:30:29.587929 31185 net.cpp:411] label_MLP_1_split -> label_MLP_1_split_0
I0311 14:30:29.587941 31185 net.cpp:411] label_MLP_1_split -> label_MLP_1_split_1
I0311 14:30:29.587998 31185 net.cpp:150] Setting up label_MLP_1_split
I0311 14:30:29.588012 31185 net.cpp:157] Top shape: 100 (100)
I0311 14:30:29.588019 31185 net.cpp:157] Top shape: 100 (100)
I0311 14:30:29.588026 31185 net.cpp:165] Memory required for data: 314800
I0311 14:30:29.588030 31185 layer_factory.hpp:77] Creating layer ip1
I0311 14:30:29.588044 31185 net.cpp:106] Creating Layer ip1
I0311 14:30:29.588052 31185 net.cpp:454] ip1 <- data
I0311 14:30:29.588062 31185 net.cpp:411] ip1 -> ip1
I0311 14:30:29.591431 31185 net.cpp:150] Setting up ip1
I0311 14:30:29.591470 31185 net.cpp:157] Top shape: 100 500 (50000)
I0311 14:30:29.591482 31185 net.cpp:165] Memory required for data: 514800
I0311 14:30:29.591502 31185 layer_factory.hpp:77] Creating layer relu1
I0311 14:30:29.591519 31185 net.cpp:106] Creating Layer relu1
I0311 14:30:29.591528 31185 net.cpp:454] relu1 <- ip1
I0311 14:30:29.591537 31185 net.cpp:411] relu1 -> relu1
I0311 14:30:29.592751 31185 net.cpp:150] Setting up relu1
I0311 14:30:29.592773 31185 net.cpp:157] Top shape: 100 500 (50000)
I0311 14:30:29.592783 31185 net.cpp:165] Memory required for data: 714800
I0311 14:30:29.592790 31185 layer_factory.hpp:77] Creating layer ip2
I0311 14:30:29.592803 31185 net.cpp:106] Creating Layer ip2
I0311 14:30:29.592809 31185 net.cpp:454] ip2 <- relu1
I0311 14:30:29.592833 31185 net.cpp:411] ip2 -> ip2
I0311 14:30:29.593183 31185 net.cpp:150] Setting up ip2
I0311 14:30:29.593202 31185 net.cpp:157] Top shape: 100 50 (5000)
I0311 14:30:29.593209 31185 net.cpp:165] Memory required for data: 734800
I0311 14:30:29.593222 31185 layer_factory.hpp:77] Creating layer relu2
I0311 14:30:29.593233 31185 net.cpp:106] Creating Layer relu2
I0311 14:30:29.593240 31185 net.cpp:454] relu2 <- ip2
I0311 14:30:29.593251 31185 net.cpp:411] relu2 -> relu2
I0311 14:30:29.594161 31185 net.cpp:150] Setting up relu2
I0311 14:30:29.594180 31185 net.cpp:157] Top shape: 100 50 (5000)
I0311 14:30:29.594187 31185 net.cpp:165] Memory required for data: 754800
I0311 14:30:29.594193 31185 layer_factory.hpp:77] Creating layer ip3
I0311 14:30:29.594203 31185 net.cpp:106] Creating Layer ip3
I0311 14:30:29.594209 31185 net.cpp:454] ip3 <- relu2
I0311 14:30:29.594221 31185 net.cpp:411] ip3 -> ip3
I0311 14:30:29.594383 31185 net.cpp:150] Setting up ip3
I0311 14:30:29.594398 31185 net.cpp:157] Top shape: 100 10 (1000)
I0311 14:30:29.594404 31185 net.cpp:165] Memory required for data: 758800
I0311 14:30:29.594419 31185 layer_factory.hpp:77] Creating layer ip3_ip3_0_split
I0311 14:30:29.594427 31185 net.cpp:106] Creating Layer ip3_ip3_0_split
I0311 14:30:29.594434 31185 net.cpp:454] ip3_ip3_0_split <- ip3
I0311 14:30:29.594441 31185 net.cpp:411] ip3_ip3_0_split -> ip3_ip3_0_split_0
I0311 14:30:29.594450 31185 net.cpp:411] ip3_ip3_0_split -> ip3_ip3_0_split_1
I0311 14:30:29.594503 31185 net.cpp:150] Setting up ip3_ip3_0_split
I0311 14:30:29.594516 31185 net.cpp:157] Top shape: 100 10 (1000)
I0311 14:30:29.594523 31185 net.cpp:157] Top shape: 100 10 (1000)
I0311 14:30:29.594528 31185 net.cpp:165] Memory required for data: 766800
I0311 14:30:29.594533 31185 layer_factory.hpp:77] Creating layer accuracy
I0311 14:30:29.594544 31185 net.cpp:106] Creating Layer accuracy
I0311 14:30:29.594550 31185 net.cpp:454] accuracy <- ip3_ip3_0_split_0
I0311 14:30:29.594558 31185 net.cpp:454] accuracy <- label_MLP_1_split_0
I0311 14:30:29.594568 31185 net.cpp:411] accuracy -> accuracy
I0311 14:30:29.594581 31185 net.cpp:150] Setting up accuracy
I0311 14:30:29.594610 31185 net.cpp:157] Top shape: (1)
I0311 14:30:29.594617 31185 net.cpp:165] Memory required for data: 766804
I0311 14:30:29.594624 31185 layer_factory.hpp:77] Creating layer loss
I0311 14:30:29.594631 31185 net.cpp:106] Creating Layer loss
I0311 14:30:29.594640 31185 net.cpp:454] loss <- ip3_ip3_0_split_1
I0311 14:30:29.594646 31185 net.cpp:454] loss <- label_MLP_1_split_1
I0311 14:30:29.594653 31185 net.cpp:411] loss -> loss
I0311 14:30:29.594666 31185 layer_factory.hpp:77] Creating layer loss
I0311 14:30:29.595626 31185 net.cpp:150] Setting up loss
I0311 14:30:29.595645 31185 net.cpp:157] Top shape: (1)
I0311 14:30:29.595652 31185 net.cpp:160]     with loss weight 1
I0311 14:30:29.595664 31185 net.cpp:165] Memory required for data: 766808
I0311 14:30:29.595669 31185 net.cpp:226] loss needs backward computation.
I0311 14:30:29.595676 31185 net.cpp:228] accuracy does not need backward computation.
I0311 14:30:29.595681 31185 net.cpp:226] ip3_ip3_0_split needs backward computation.
I0311 14:30:29.595686 31185 net.cpp:226] ip3 needs backward computation.
I0311 14:30:29.595691 31185 net.cpp:226] relu2 needs backward computation.
I0311 14:30:29.595695 31185 net.cpp:226] ip2 needs backward computation.
I0311 14:30:29.595700 31185 net.cpp:226] relu1 needs backward computation.
I0311 14:30:29.595705 31185 net.cpp:226] ip1 needs backward computation.
I0311 14:30:29.595710 31185 net.cpp:228] label_MLP_1_split does not need backward computation.
I0311 14:30:29.595715 31185 net.cpp:228] MLP does not need backward computation.
I0311 14:30:29.595723 31185 net.cpp:270] This network produces output accuracy
I0311 14:30:29.595728 31185 net.cpp:270] This network produces output loss
I0311 14:30:29.595741 31185 net.cpp:283] Network initialization done.
I0311 14:30:29.595808 31185 solver.cpp:60] Solver scaffolding done.
I0311 14:30:29.596097 31185 caffe.cpp:212] Starting Optimization
I0311 14:30:29.596113 31185 solver.cpp:288] Solving MLP
I0311 14:30:29.596119 31185 solver.cpp:289] Learning Rate Policy: inv
I0311 14:30:29.596715 31185 solver.cpp:341] Iteration 0, Testing net (#0)
I0311 14:30:29.596734 31185 net.cpp:748] Ignoring source layer mnist
I0311 14:30:29.787889 31185 solver.cpp:409]     Test net output #0: accuracy = 0.0887
I0311 14:30:29.787921 31185 solver.cpp:409]     Test net output #1: loss = 2.32495 (* 1 = 2.32495 loss)
I0311 14:30:29.789907 31185 solver.cpp:237] Iteration 0, loss = 2.3102
I0311 14:30:29.789927 31185 solver.cpp:253]     Train net output #0: loss = 2.3102 (* 1 = 2.3102 loss)
I0311 14:30:29.789938 31185 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0311 14:30:30.010804 31185 solver.cpp:237] Iteration 100, loss = 0.48538
I0311 14:30:30.010836 31185 solver.cpp:253]     Train net output #0: loss = 0.48538 (* 1 = 0.48538 loss)
I0311 14:30:30.010843 31185 sgd_solver.cpp:106] Iteration 100, lr = 0.00992565
I0311 14:30:30.235250 31185 solver.cpp:237] Iteration 200, loss = 0.579684
I0311 14:30:30.235283 31185 solver.cpp:253]     Train net output #0: loss = 0.579684 (* 1 = 0.579684 loss)
I0311 14:30:30.235290 31185 sgd_solver.cpp:106] Iteration 200, lr = 0.00985258
I0311 14:30:30.334153 31185 blocking_queue.cpp:50] Data layer prefetch queue empty
I0311 14:30:30.471261 31185 solver.cpp:237] Iteration 300, loss = 0.374219
I0311 14:30:30.471290 31185 solver.cpp:253]     Train net output #0: loss = 0.374219 (* 1 = 0.374219 loss)
I0311 14:30:30.471297 31185 sgd_solver.cpp:106] Iteration 300, lr = 0.00978075
I0311 14:30:30.711737 31185 solver.cpp:237] Iteration 400, loss = 0.282895
I0311 14:30:30.711769 31185 solver.cpp:253]     Train net output #0: loss = 0.282895 (* 1 = 0.282895 loss)
I0311 14:30:30.711778 31185 sgd_solver.cpp:106] Iteration 400, lr = 0.00971013
I0311 14:30:30.951107 31185 solver.cpp:341] Iteration 500, Testing net (#0)
I0311 14:30:30.951129 31185 net.cpp:748] Ignoring source layer mnist
I0311 14:30:31.138471 31185 solver.cpp:409]     Test net output #0: accuracy = 0.9291
I0311 14:30:31.138502 31185 solver.cpp:409]     Test net output #1: loss = 0.251792 (* 1 = 0.251792 loss)
I0311 14:30:31.139897 31185 solver.cpp:237] Iteration 500, loss = 0.292821
I0311 14:30:31.139957 31185 solver.cpp:253]     Train net output #0: loss = 0.292821 (* 1 = 0.292821 loss)
I0311 14:30:31.139997 31185 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I0311 14:30:31.380532 31185 solver.cpp:237] Iteration 600, loss = 0.216206
I0311 14:30:31.380564 31185 solver.cpp:253]     Train net output #0: loss = 0.216206 (* 1 = 0.216206 loss)
I0311 14:30:31.380573 31185 sgd_solver.cpp:106] Iteration 600, lr = 0.0095724
I0311 14:30:31.621448 31185 solver.cpp:237] Iteration 700, loss = 0.225335
I0311 14:30:31.621595 31185 solver.cpp:253]     Train net output #0: loss = 0.225335 (* 1 = 0.225335 loss)
I0311 14:30:31.621649 31185 sgd_solver.cpp:106] Iteration 700, lr = 0.00950522
I0311 14:30:31.856742 31185 solver.cpp:237] Iteration 800, loss = 0.299948
I0311 14:30:31.856775 31185 solver.cpp:253]     Train net output #0: loss = 0.299948 (* 1 = 0.299948 loss)
I0311 14:30:31.856783 31185 sgd_solver.cpp:106] Iteration 800, lr = 0.00943913
I0311 14:30:32.096500 31185 solver.cpp:237] Iteration 900, loss = 0.229707
I0311 14:30:32.096532 31185 solver.cpp:253]     Train net output #0: loss = 0.229707 (* 1 = 0.229707 loss)
I0311 14:30:32.096542 31185 sgd_solver.cpp:106] Iteration 900, lr = 0.00937411
I0311 14:30:32.329854 31185 solver.cpp:341] Iteration 1000, Testing net (#0)
I0311 14:30:32.329876 31185 net.cpp:748] Ignoring source layer mnist
I0311 14:30:32.479135 31185 solver.cpp:409]     Test net output #0: accuracy = 0.9403
I0311 14:30:32.479167 31185 solver.cpp:409]     Test net output #1: loss = 0.200425 (* 1 = 0.200425 loss)
I0311 14:30:32.480557 31185 solver.cpp:237] Iteration 1000, loss = 0.155363
I0311 14:30:32.480576 31185 solver.cpp:253]     Train net output #0: loss = 0.155363 (* 1 = 0.155363 loss)
I0311 14:30:32.480587 31185 sgd_solver.cpp:106] Iteration 1000, lr = 0.00931012
I0311 14:30:32.711194 31185 solver.cpp:237] Iteration 1100, loss = 0.162615
I0311 14:30:32.711225 31185 solver.cpp:253]     Train net output #0: loss = 0.162616 (* 1 = 0.162616 loss)
I0311 14:30:32.711236 31185 sgd_solver.cpp:106] Iteration 1100, lr = 0.00924715
I0311 14:30:32.942001 31185 solver.cpp:237] Iteration 1200, loss = 0.137306
I0311 14:30:32.942033 31185 solver.cpp:253]     Train net output #0: loss = 0.137306 (* 1 = 0.137306 loss)
I0311 14:30:32.942098 31185 sgd_solver.cpp:106] Iteration 1200, lr = 0.00918515
I0311 14:30:33.173394 31185 solver.cpp:237] Iteration 1300, loss = 0.152871
I0311 14:30:33.173426 31185 solver.cpp:253]     Train net output #0: loss = 0.152871 (* 1 = 0.152871 loss)
I0311 14:30:33.173436 31185 sgd_solver.cpp:106] Iteration 1300, lr = 0.00912412
I0311 14:30:33.403543 31185 solver.cpp:237] Iteration 1400, loss = 0.213122
I0311 14:30:33.403576 31185 solver.cpp:253]     Train net output #0: loss = 0.213122 (* 1 = 0.213122 loss)
I0311 14:30:33.403586 31185 sgd_solver.cpp:106] Iteration 1400, lr = 0.00906403
I0311 14:30:33.632874 31185 solver.cpp:341] Iteration 1500, Testing net (#0)
I0311 14:30:33.632902 31185 net.cpp:748] Ignoring source layer mnist
I0311 14:30:33.789351 31185 solver.cpp:409]     Test net output #0: accuracy = 0.9578
I0311 14:30:33.789381 31185 solver.cpp:409]     Test net output #1: loss = 0.141775 (* 1 = 0.141775 loss)
I0311 14:30:33.791304 31185 solver.cpp:237] Iteration 1500, loss = 0.195231
I0311 14:30:33.791324 31185 solver.cpp:253]     Train net output #0: loss = 0.195231 (* 1 = 0.195231 loss)
I0311 14:30:33.791335 31185 sgd_solver.cpp:106] Iteration 1500, lr = 0.00900485
I0311 14:30:34.021150 31185 solver.cpp:237] Iteration 1600, loss = 0.105718
I0311 14:30:34.021184 31185 solver.cpp:253]     Train net output #0: loss = 0.105718 (* 1 = 0.105718 loss)
I0311 14:30:34.021194 31185 sgd_solver.cpp:106] Iteration 1600, lr = 0.00894657
I0311 14:30:34.248956 31185 solver.cpp:237] Iteration 1700, loss = 0.111972
I0311 14:30:34.248988 31185 solver.cpp:253]     Train net output #0: loss = 0.111973 (* 1 = 0.111973 loss)
I0311 14:30:34.248998 31185 sgd_solver.cpp:106] Iteration 1700, lr = 0.00888916
I0311 14:30:34.480377 31185 solver.cpp:237] Iteration 1800, loss = 0.101015
I0311 14:30:34.480411 31185 solver.cpp:253]     Train net output #0: loss = 0.101015 (* 1 = 0.101015 loss)
I0311 14:30:34.480419 31185 sgd_solver.cpp:106] Iteration 1800, lr = 0.0088326
I0311 14:30:34.709748 31185 solver.cpp:237] Iteration 1900, loss = 0.123984
I0311 14:30:34.709780 31185 solver.cpp:253]     Train net output #0: loss = 0.123984 (* 1 = 0.123984 loss)
I0311 14:30:34.709789 31185 sgd_solver.cpp:106] Iteration 1900, lr = 0.00877687
I0311 14:30:34.942534 31185 solver.cpp:459] Snapshotting to binary proto file examples/mlp/mlp_absinner_iter_2000.caffemodel
I0311 14:30:35.322530 31185 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mlp/mlp_absinner_iter_2000.solverstate
I0311 14:30:35.324728 31185 solver.cpp:341] Iteration 2000, Testing net (#0)
I0311 14:30:35.324748 31185 net.cpp:748] Ignoring source layer mnist
I0311 14:30:35.491989 31185 solver.cpp:409]     Test net output #0: accuracy = 0.965
I0311 14:30:35.492023 31185 solver.cpp:409]     Test net output #1: loss = 0.116589 (* 1 = 0.116589 loss)
I0311 14:30:35.494181 31185 solver.cpp:237] Iteration 2000, loss = 0.169076
I0311 14:30:35.494202 31185 solver.cpp:253]     Train net output #0: loss = 0.169076 (* 1 = 0.169076 loss)
I0311 14:30:35.494213 31185 sgd_solver.cpp:106] Iteration 2000, lr = 0.00872196
I0311 14:30:35.722460 31185 solver.cpp:237] Iteration 2100, loss = 0.177707
I0311 14:30:35.722492 31185 solver.cpp:253]     Train net output #0: loss = 0.177707 (* 1 = 0.177707 loss)
I0311 14:30:35.722501 31185 sgd_solver.cpp:106] Iteration 2100, lr = 0.00866784
I0311 14:30:35.945917 31185 solver.cpp:237] Iteration 2200, loss = 0.0816291
I0311 14:30:35.945950 31185 solver.cpp:253]     Train net output #0: loss = 0.0816292 (* 1 = 0.0816292 loss)
I0311 14:30:35.945960 31185 sgd_solver.cpp:106] Iteration 2200, lr = 0.0086145
I0311 14:30:36.168376 31185 solver.cpp:237] Iteration 2300, loss = 0.0885293
I0311 14:30:36.168408 31185 solver.cpp:253]     Train net output #0: loss = 0.0885294 (* 1 = 0.0885294 loss)
I0311 14:30:36.168417 31185 sgd_solver.cpp:106] Iteration 2300, lr = 0.00856192
I0311 14:30:36.393729 31185 solver.cpp:237] Iteration 2400, loss = 0.080126
I0311 14:30:36.393764 31185 solver.cpp:253]     Train net output #0: loss = 0.0801261 (* 1 = 0.0801261 loss)
I0311 14:30:36.393774 31185 sgd_solver.cpp:106] Iteration 2400, lr = 0.00851008
I0311 14:30:36.628725 31185 solver.cpp:341] Iteration 2500, Testing net (#0)
I0311 14:30:36.628748 31185 net.cpp:748] Ignoring source layer mnist
I0311 14:30:36.804927 31185 solver.cpp:409]     Test net output #0: accuracy = 0.9652
I0311 14:30:36.804960 31185 solver.cpp:409]     Test net output #1: loss = 0.113085 (* 1 = 0.113085 loss)
I0311 14:30:36.807376 31185 solver.cpp:237] Iteration 2500, loss = 0.100209
I0311 14:30:36.807399 31185 solver.cpp:253]     Train net output #0: loss = 0.100209 (* 1 = 0.100209 loss)
I0311 14:30:36.807410 31185 sgd_solver.cpp:106] Iteration 2500, lr = 0.00845897
I0311 14:30:37.044008 31185 solver.cpp:237] Iteration 2600, loss = 0.13942
I0311 14:30:37.044040 31185 solver.cpp:253]     Train net output #0: loss = 0.13942 (* 1 = 0.13942 loss)
I0311 14:30:37.044049 31185 sgd_solver.cpp:106] Iteration 2600, lr = 0.00840857
I0311 14:30:37.275676 31185 solver.cpp:237] Iteration 2700, loss = 0.16091
I0311 14:30:37.275708 31185 solver.cpp:253]     Train net output #0: loss = 0.16091 (* 1 = 0.16091 loss)
I0311 14:30:37.275717 31185 sgd_solver.cpp:106] Iteration 2700, lr = 0.00835886
I0311 14:30:37.507431 31185 solver.cpp:237] Iteration 2800, loss = 0.065795
I0311 14:30:37.507464 31185 solver.cpp:253]     Train net output #0: loss = 0.0657952 (* 1 = 0.0657952 loss)
I0311 14:30:37.507473 31185 sgd_solver.cpp:106] Iteration 2800, lr = 0.00830984
I0311 14:30:37.742236 31185 solver.cpp:237] Iteration 2900, loss = 0.0754923
I0311 14:30:37.742269 31185 solver.cpp:253]     Train net output #0: loss = 0.0754924 (* 1 = 0.0754924 loss)
I0311 14:30:37.742279 31185 sgd_solver.cpp:106] Iteration 2900, lr = 0.00826148
I0311 14:30:37.977236 31185 solver.cpp:341] Iteration 3000, Testing net (#0)
I0311 14:30:37.977257 31185 net.cpp:748] Ignoring source layer mnist
I0311 14:30:38.156359 31185 solver.cpp:409]     Test net output #0: accuracy = 0.9683
I0311 14:30:38.156393 31185 solver.cpp:409]     Test net output #1: loss = 0.105047 (* 1 = 0.105047 loss)
I0311 14:30:38.157945 31185 solver.cpp:237] Iteration 3000, loss = 0.0675596
I0311 14:30:38.157969 31185 solver.cpp:253]     Train net output #0: loss = 0.0675597 (* 1 = 0.0675597 loss)
I0311 14:30:38.157980 31185 sgd_solver.cpp:106] Iteration 3000, lr = 0.00821377
I0311 14:30:38.392580 31185 solver.cpp:237] Iteration 3100, loss = 0.0850985
I0311 14:30:38.392612 31185 solver.cpp:253]     Train net output #0: loss = 0.0850987 (* 1 = 0.0850987 loss)
I0311 14:30:38.392621 31185 sgd_solver.cpp:106] Iteration 3100, lr = 0.0081667
I0311 14:30:38.628340 31185 solver.cpp:237] Iteration 3200, loss = 0.11748
I0311 14:30:38.628372 31185 solver.cpp:253]     Train net output #0: loss = 0.11748 (* 1 = 0.11748 loss)
I0311 14:30:38.628381 31185 sgd_solver.cpp:106] Iteration 3200, lr = 0.00812025
I0311 14:30:38.858352 31185 solver.cpp:237] Iteration 3300, loss = 0.139684
I0311 14:30:38.858384 31185 solver.cpp:253]     Train net output #0: loss = 0.139684 (* 1 = 0.139684 loss)
I0311 14:30:38.858393 31185 sgd_solver.cpp:106] Iteration 3300, lr = 0.00807442
I0311 14:30:39.104339 31185 solver.cpp:237] Iteration 3400, loss = 0.0548978
I0311 14:30:39.104447 31185 solver.cpp:253]     Train net output #0: loss = 0.0548979 (* 1 = 0.0548979 loss)
I0311 14:30:39.104478 31185 sgd_solver.cpp:106] Iteration 3400, lr = 0.00802918
I0311 14:30:39.333922 31185 solver.cpp:341] Iteration 3500, Testing net (#0)
I0311 14:30:39.333948 31185 net.cpp:748] Ignoring source layer mnist
I0311 14:30:39.508551 31185 solver.cpp:409]     Test net output #0: accuracy = 0.9724
I0311 14:30:39.508582 31185 solver.cpp:409]     Test net output #1: loss = 0.0912454 (* 1 = 0.0912454 loss)
I0311 14:30:39.509709 31185 solver.cpp:237] Iteration 3500, loss = 0.0665727
I0311 14:30:39.509732 31185 solver.cpp:253]     Train net output #0: loss = 0.0665728 (* 1 = 0.0665728 loss)
I0311 14:30:39.509744 31185 sgd_solver.cpp:106] Iteration 3500, lr = 0.00798454
I0311 14:30:39.738322 31185 solver.cpp:237] Iteration 3600, loss = 0.0592228
I0311 14:30:39.738356 31185 solver.cpp:253]     Train net output #0: loss = 0.0592229 (* 1 = 0.0592229 loss)
I0311 14:30:39.738366 31185 sgd_solver.cpp:106] Iteration 3600, lr = 0.00794046
I0311 14:30:39.958561 31185 solver.cpp:237] Iteration 3700, loss = 0.0756274
I0311 14:30:39.958595 31185 solver.cpp:253]     Train net output #0: loss = 0.0756275 (* 1 = 0.0756275 loss)
I0311 14:30:39.958605 31185 sgd_solver.cpp:106] Iteration 3700, lr = 0.00789695
I0311 14:30:40.185626 31185 solver.cpp:237] Iteration 3800, loss = 0.105816
I0311 14:30:40.185659 31185 solver.cpp:253]     Train net output #0: loss = 0.105816 (* 1 = 0.105816 loss)
I0311 14:30:40.185668 31185 sgd_solver.cpp:106] Iteration 3800, lr = 0.007854
I0311 14:30:40.416996 31185 solver.cpp:237] Iteration 3900, loss = 0.121242
I0311 14:30:40.417027 31185 solver.cpp:253]     Train net output #0: loss = 0.121242 (* 1 = 0.121242 loss)
I0311 14:30:40.417037 31185 sgd_solver.cpp:106] Iteration 3900, lr = 0.00781158
I0311 14:30:40.644618 31185 solver.cpp:459] Snapshotting to binary proto file examples/mlp/mlp_absinner_iter_4000.caffemodel
I0311 14:30:41.023298 31185 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mlp/mlp_absinner_iter_4000.solverstate
I0311 14:30:41.025506 31185 solver.cpp:341] Iteration 4000, Testing net (#0)
I0311 14:30:41.025527 31185 net.cpp:748] Ignoring source layer mnist
I0311 14:30:41.199666 31185 solver.cpp:409]     Test net output #0: accuracy = 0.9738
I0311 14:30:41.199697 31185 solver.cpp:409]     Test net output #1: loss = 0.0885779 (* 1 = 0.0885779 loss)
I0311 14:30:41.201776 31185 solver.cpp:237] Iteration 4000, loss = 0.0474244
I0311 14:30:41.201797 31185 solver.cpp:253]     Train net output #0: loss = 0.0474245 (* 1 = 0.0474245 loss)
I0311 14:30:41.201835 31185 sgd_solver.cpp:106] Iteration 4000, lr = 0.0077697
I0311 14:30:41.454339 31185 solver.cpp:237] Iteration 4100, loss = 0.0589173
I0311 14:30:41.454432 31185 solver.cpp:253]     Train net output #0: loss = 0.0589174 (* 1 = 0.0589174 loss)
I0311 14:30:41.454466 31185 sgd_solver.cpp:106] Iteration 4100, lr = 0.00772833
I0311 14:30:41.705454 31185 solver.cpp:237] Iteration 4200, loss = 0.0544157
I0311 14:30:41.705545 31185 solver.cpp:253]     Train net output #0: loss = 0.0544159 (* 1 = 0.0544159 loss)
I0311 14:30:41.705580 31185 sgd_solver.cpp:106] Iteration 4200, lr = 0.00768748
I0311 14:30:41.955723 31185 solver.cpp:237] Iteration 4300, loss = 0.0694131
I0311 14:30:41.955759 31185 solver.cpp:253]     Train net output #0: loss = 0.0694132 (* 1 = 0.0694132 loss)
I0311 14:30:41.955767 31185 sgd_solver.cpp:106] Iteration 4300, lr = 0.00764712
I0311 14:30:42.165850 31185 solver.cpp:237] Iteration 4400, loss = 0.0963809
I0311 14:30:42.165884 31185 solver.cpp:253]     Train net output #0: loss = 0.096381 (* 1 = 0.096381 loss)
I0311 14:30:42.165894 31185 sgd_solver.cpp:106] Iteration 4400, lr = 0.00760726
I0311 14:30:42.379417 31185 solver.cpp:341] Iteration 4500, Testing net (#0)
I0311 14:30:42.379490 31185 net.cpp:748] Ignoring source layer mnist
I0311 14:30:42.577879 31185 solver.cpp:409]     Test net output #0: accuracy = 0.9738
I0311 14:30:42.577970 31185 solver.cpp:409]     Test net output #1: loss = 0.0817192 (* 1 = 0.0817192 loss)
I0311 14:30:42.579716 31185 solver.cpp:237] Iteration 4500, loss = 0.104069
I0311 14:30:42.579763 31185 solver.cpp:253]     Train net output #0: loss = 0.104069 (* 1 = 0.104069 loss)
I0311 14:30:42.579792 31185 sgd_solver.cpp:106] Iteration 4500, lr = 0.00756788
I0311 14:30:42.836812 31185 solver.cpp:237] Iteration 4600, loss = 0.0420477
I0311 14:30:42.836907 31185 solver.cpp:253]     Train net output #0: loss = 0.0420478 (* 1 = 0.0420478 loss)
I0311 14:30:42.836943 31185 sgd_solver.cpp:106] Iteration 4600, lr = 0.00752897
I0311 14:30:43.090975 31185 solver.cpp:237] Iteration 4700, loss = 0.0524996
I0311 14:30:43.091066 31185 solver.cpp:253]     Train net output #0: loss = 0.0524997 (* 1 = 0.0524997 loss)
I0311 14:30:43.091100 31185 sgd_solver.cpp:106] Iteration 4700, lr = 0.00749052
I0311 14:30:43.345510 31185 solver.cpp:237] Iteration 4800, loss = 0.0501285
I0311 14:30:43.345602 31185 solver.cpp:253]     Train net output #0: loss = 0.0501286 (* 1 = 0.0501286 loss)
I0311 14:30:43.345638 31185 sgd_solver.cpp:106] Iteration 4800, lr = 0.00745253
I0311 14:30:43.592427 31185 solver.cpp:237] Iteration 4900, loss = 0.0634923
I0311 14:30:43.592461 31185 solver.cpp:253]     Train net output #0: loss = 0.0634923 (* 1 = 0.0634923 loss)
I0311 14:30:43.592471 31185 sgd_solver.cpp:106] Iteration 4900, lr = 0.00741498
I0311 14:30:43.816882 31185 solver.cpp:341] Iteration 5000, Testing net (#0)
I0311 14:30:43.816911 31185 net.cpp:748] Ignoring source layer mnist
I0311 14:30:44.003136 31185 solver.cpp:409]     Test net output #0: accuracy = 0.9775
I0311 14:30:44.003170 31185 solver.cpp:409]     Test net output #1: loss = 0.0752829 (* 1 = 0.0752829 loss)
I0311 14:30:44.005393 31185 solver.cpp:237] Iteration 5000, loss = 0.08664
I0311 14:30:44.005414 31185 solver.cpp:253]     Train net output #0: loss = 0.08664 (* 1 = 0.08664 loss)
I0311 14:30:44.005425 31185 sgd_solver.cpp:106] Iteration 5000, lr = 0.00737788
I0311 14:30:44.239524 31185 solver.cpp:237] Iteration 5100, loss = 0.0912177
I0311 14:30:44.239557 31185 solver.cpp:253]     Train net output #0: loss = 0.0912178 (* 1 = 0.0912178 loss)
I0311 14:30:44.239567 31185 sgd_solver.cpp:106] Iteration 5100, lr = 0.0073412
I0311 14:30:44.472954 31185 solver.cpp:237] Iteration 5200, loss = 0.0381538
I0311 14:30:44.472986 31185 solver.cpp:253]     Train net output #0: loss = 0.0381539 (* 1 = 0.0381539 loss)
I0311 14:30:44.472995 31185 sgd_solver.cpp:106] Iteration 5200, lr = 0.00730495
I0311 14:30:44.703948 31185 solver.cpp:237] Iteration 5300, loss = 0.0483592
I0311 14:30:44.703982 31185 solver.cpp:253]     Train net output #0: loss = 0.0483593 (* 1 = 0.0483593 loss)
I0311 14:30:44.704023 31185 sgd_solver.cpp:106] Iteration 5300, lr = 0.00726911
I0311 14:30:44.938118 31185 solver.cpp:237] Iteration 5400, loss = 0.0467546
I0311 14:30:44.938150 31185 solver.cpp:253]     Train net output #0: loss = 0.0467547 (* 1 = 0.0467547 loss)
I0311 14:30:44.938159 31185 sgd_solver.cpp:106] Iteration 5400, lr = 0.00723368
I0311 14:30:45.165995 31185 solver.cpp:341] Iteration 5500, Testing net (#0)
I0311 14:30:45.166019 31185 net.cpp:748] Ignoring source layer mnist
I0311 14:30:45.337148 31185 solver.cpp:409]     Test net output #0: accuracy = 0.9758
I0311 14:30:45.337187 31185 solver.cpp:409]     Test net output #1: loss = 0.0791161 (* 1 = 0.0791161 loss)
I0311 14:30:45.339102 31185 solver.cpp:237] Iteration 5500, loss = 0.0601861
I0311 14:30:45.339123 31185 solver.cpp:253]     Train net output #0: loss = 0.0601861 (* 1 = 0.0601861 loss)
I0311 14:30:45.339134 31185 sgd_solver.cpp:106] Iteration 5500, lr = 0.00719865
I0311 14:30:45.569464 31185 solver.cpp:237] Iteration 5600, loss = 0.0789783
I0311 14:30:45.569499 31185 solver.cpp:253]     Train net output #0: loss = 0.0789783 (* 1 = 0.0789783 loss)
I0311 14:30:45.569507 31185 sgd_solver.cpp:106] Iteration 5600, lr = 0.00716402
I0311 14:30:45.797294 31185 solver.cpp:237] Iteration 5700, loss = 0.0780433
I0311 14:30:45.797327 31185 solver.cpp:253]     Train net output #0: loss = 0.0780433 (* 1 = 0.0780433 loss)
I0311 14:30:45.797336 31185 sgd_solver.cpp:106] Iteration 5700, lr = 0.00712977
I0311 14:30:46.028004 31185 solver.cpp:237] Iteration 5800, loss = 0.0346093
I0311 14:30:46.028036 31185 solver.cpp:253]     Train net output #0: loss = 0.0346093 (* 1 = 0.0346093 loss)
I0311 14:30:46.028045 31185 sgd_solver.cpp:106] Iteration 5800, lr = 0.0070959
I0311 14:30:46.258646 31185 solver.cpp:237] Iteration 5900, loss = 0.0446625
I0311 14:30:46.258678 31185 solver.cpp:253]     Train net output #0: loss = 0.0446626 (* 1 = 0.0446626 loss)
I0311 14:30:46.258687 31185 sgd_solver.cpp:106] Iteration 5900, lr = 0.0070624
I0311 14:30:46.488682 31185 solver.cpp:459] Snapshotting to binary proto file examples/mlp/mlp_absinner_iter_6000.caffemodel
I0311 14:30:46.868147 31185 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mlp/mlp_absinner_iter_6000.solverstate
I0311 14:30:46.870352 31185 solver.cpp:341] Iteration 6000, Testing net (#0)
I0311 14:30:46.870371 31185 net.cpp:748] Ignoring source layer mnist
I0311 14:30:47.051234 31185 solver.cpp:409]     Test net output #0: accuracy = 0.9753
I0311 14:30:47.051265 31185 solver.cpp:409]     Test net output #1: loss = 0.0790294 (* 1 = 0.0790294 loss)
I0311 14:30:47.052608 31185 solver.cpp:237] Iteration 6000, loss = 0.0436581
I0311 14:30:47.052629 31185 solver.cpp:253]     Train net output #0: loss = 0.0436582 (* 1 = 0.0436582 loss)
I0311 14:30:47.052640 31185 sgd_solver.cpp:106] Iteration 6000, lr = 0.00702927
I0311 14:30:47.290429 31185 solver.cpp:237] Iteration 6100, loss = 0.0559258
I0311 14:30:47.290462 31185 solver.cpp:253]     Train net output #0: loss = 0.0559258 (* 1 = 0.0559258 loss)
I0311 14:30:47.290532 31185 sgd_solver.cpp:106] Iteration 6100, lr = 0.0069965
I0311 14:30:47.515136 31185 solver.cpp:237] Iteration 6200, loss = 0.0708296
I0311 14:30:47.515171 31185 solver.cpp:253]     Train net output #0: loss = 0.0708297 (* 1 = 0.0708297 loss)
I0311 14:30:47.515179 31185 sgd_solver.cpp:106] Iteration 6200, lr = 0.00696408
I0311 14:30:47.740756 31185 solver.cpp:237] Iteration 6300, loss = 0.0691055
I0311 14:30:47.740790 31185 solver.cpp:253]     Train net output #0: loss = 0.0691056 (* 1 = 0.0691056 loss)
I0311 14:30:47.740859 31185 sgd_solver.cpp:106] Iteration 6300, lr = 0.00693201
I0311 14:30:47.967882 31185 solver.cpp:237] Iteration 6400, loss = 0.0320298
I0311 14:30:47.967914 31185 solver.cpp:253]     Train net output #0: loss = 0.0320299 (* 1 = 0.0320299 loss)
I0311 14:30:47.967923 31185 sgd_solver.cpp:106] Iteration 6400, lr = 0.00690029
I0311 14:30:48.191249 31185 solver.cpp:341] Iteration 6500, Testing net (#0)
I0311 14:30:48.191296 31185 net.cpp:748] Ignoring source layer mnist
I0311 14:30:48.371090 31185 solver.cpp:409]     Test net output #0: accuracy = 0.978
I0311 14:30:48.371120 31185 solver.cpp:409]     Test net output #1: loss = 0.0733701 (* 1 = 0.0733701 loss)
I0311 14:30:48.372385 31185 solver.cpp:237] Iteration 6500, loss = 0.0411629
I0311 14:30:48.372406 31185 solver.cpp:253]     Train net output #0: loss = 0.041163 (* 1 = 0.041163 loss)
I0311 14:30:48.372417 31185 sgd_solver.cpp:106] Iteration 6500, lr = 0.0068689
I0311 14:30:48.597383 31185 solver.cpp:237] Iteration 6600, loss = 0.0413225
I0311 14:30:48.597416 31185 solver.cpp:253]     Train net output #0: loss = 0.0413226 (* 1 = 0.0413226 loss)
I0311 14:30:48.597425 31185 sgd_solver.cpp:106] Iteration 6600, lr = 0.00683784
I0311 14:30:48.828110 31185 solver.cpp:237] Iteration 6700, loss = 0.0521437
I0311 14:30:48.828143 31185 solver.cpp:253]     Train net output #0: loss = 0.0521438 (* 1 = 0.0521438 loss)
I0311 14:30:48.828153 31185 sgd_solver.cpp:106] Iteration 6700, lr = 0.00680711
I0311 14:30:49.060036 31185 solver.cpp:237] Iteration 6800, loss = 0.0646912
I0311 14:30:49.060068 31185 solver.cpp:253]     Train net output #0: loss = 0.0646912 (* 1 = 0.0646912 loss)
I0311 14:30:49.060078 31185 sgd_solver.cpp:106] Iteration 6800, lr = 0.0067767
I0311 14:30:49.291051 31185 solver.cpp:237] Iteration 6900, loss = 0.0609811
I0311 14:30:49.291097 31185 solver.cpp:253]     Train net output #0: loss = 0.0609812 (* 1 = 0.0609812 loss)
I0311 14:30:49.291120 31185 sgd_solver.cpp:106] Iteration 6900, lr = 0.0067466
I0311 14:30:49.517577 31185 solver.cpp:341] Iteration 7000, Testing net (#0)
I0311 14:30:49.517601 31185 net.cpp:748] Ignoring source layer mnist
I0311 14:30:49.706118 31185 solver.cpp:409]     Test net output #0: accuracy = 0.9771
I0311 14:30:49.706254 31185 solver.cpp:409]     Test net output #1: loss = 0.0738477 (* 1 = 0.0738477 loss)
I0311 14:30:49.707569 31185 solver.cpp:237] Iteration 7000, loss = 0.0292005
I0311 14:30:49.707592 31185 solver.cpp:253]     Train net output #0: loss = 0.0292006 (* 1 = 0.0292006 loss)
I0311 14:30:49.707603 31185 sgd_solver.cpp:106] Iteration 7000, lr = 0.00671681
I0311 14:30:49.940143 31185 solver.cpp:237] Iteration 7100, loss = 0.0378615
I0311 14:30:49.940176 31185 solver.cpp:253]     Train net output #0: loss = 0.0378616 (* 1 = 0.0378616 loss)
I0311 14:30:49.940186 31185 sgd_solver.cpp:106] Iteration 7100, lr = 0.00668733
I0311 14:30:50.170563 31185 solver.cpp:237] Iteration 7200, loss = 0.0394573
I0311 14:30:50.170596 31185 solver.cpp:253]     Train net output #0: loss = 0.0394573 (* 1 = 0.0394573 loss)
I0311 14:30:50.170606 31185 sgd_solver.cpp:106] Iteration 7200, lr = 0.00665815
I0311 14:30:50.391052 31185 solver.cpp:237] Iteration 7300, loss = 0.0485014
I0311 14:30:50.391084 31185 solver.cpp:253]     Train net output #0: loss = 0.0485015 (* 1 = 0.0485015 loss)
I0311 14:30:50.391093 31185 sgd_solver.cpp:106] Iteration 7300, lr = 0.00662927
I0311 14:30:50.614799 31185 solver.cpp:237] Iteration 7400, loss = 0.059564
I0311 14:30:50.614833 31185 solver.cpp:253]     Train net output #0: loss = 0.059564 (* 1 = 0.059564 loss)
I0311 14:30:50.614842 31185 sgd_solver.cpp:106] Iteration 7400, lr = 0.00660067
I0311 14:30:50.833515 31185 solver.cpp:341] Iteration 7500, Testing net (#0)
I0311 14:30:50.833539 31185 net.cpp:748] Ignoring source layer mnist
I0311 14:30:51.021910 31185 solver.cpp:409]     Test net output #0: accuracy = 0.9778
I0311 14:30:51.021942 31185 solver.cpp:409]     Test net output #1: loss = 0.0693387 (* 1 = 0.0693387 loss)
I0311 14:30:51.023653 31185 solver.cpp:237] Iteration 7500, loss = 0.0553538
I0311 14:30:51.023671 31185 solver.cpp:253]     Train net output #0: loss = 0.0553539 (* 1 = 0.0553539 loss)
I0311 14:30:51.023684 31185 sgd_solver.cpp:106] Iteration 7500, lr = 0.00657236
I0311 14:30:51.229648 31185 solver.cpp:237] Iteration 7600, loss = 0.0273395
I0311 14:30:51.229681 31185 solver.cpp:253]     Train net output #0: loss = 0.0273396 (* 1 = 0.0273396 loss)
I0311 14:30:51.229691 31185 sgd_solver.cpp:106] Iteration 7600, lr = 0.00654433
I0311 14:30:51.450426 31185 solver.cpp:237] Iteration 7700, loss = 0.0351039
I0311 14:30:51.450465 31185 solver.cpp:253]     Train net output #0: loss = 0.035104 (* 1 = 0.035104 loss)
I0311 14:30:51.450475 31185 sgd_solver.cpp:106] Iteration 7700, lr = 0.00651658
I0311 14:30:51.671246 31185 solver.cpp:237] Iteration 7800, loss = 0.0370551
I0311 14:30:51.671278 31185 solver.cpp:253]     Train net output #0: loss = 0.0370552 (* 1 = 0.0370552 loss)
I0311 14:30:51.671288 31185 sgd_solver.cpp:106] Iteration 7800, lr = 0.00648911
I0311 14:30:51.901780 31185 solver.cpp:237] Iteration 7900, loss = 0.0458081
I0311 14:30:51.901813 31185 solver.cpp:253]     Train net output #0: loss = 0.0458082 (* 1 = 0.0458082 loss)
I0311 14:30:51.901823 31185 sgd_solver.cpp:106] Iteration 7900, lr = 0.0064619
I0311 14:30:52.130249 31185 solver.cpp:459] Snapshotting to binary proto file examples/mlp/mlp_absinner_iter_8000.caffemodel
I0311 14:30:52.510696 31185 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mlp/mlp_absinner_iter_8000.solverstate
I0311 14:30:52.512873 31185 solver.cpp:341] Iteration 8000, Testing net (#0)
I0311 14:30:52.512893 31185 net.cpp:748] Ignoring source layer mnist
I0311 14:30:52.699095 31185 solver.cpp:409]     Test net output #0: accuracy = 0.9788
I0311 14:30:52.699177 31185 solver.cpp:409]     Test net output #1: loss = 0.066797 (* 1 = 0.066797 loss)
I0311 14:30:52.701064 31185 solver.cpp:237] Iteration 8000, loss = 0.0553323
I0311 14:30:52.701084 31185 solver.cpp:253]     Train net output #0: loss = 0.0553323 (* 1 = 0.0553323 loss)
I0311 14:30:52.701095 31185 sgd_solver.cpp:106] Iteration 8000, lr = 0.00643496
I0311 14:30:52.927521 31185 solver.cpp:237] Iteration 8100, loss = 0.0500873
I0311 14:30:52.927556 31185 solver.cpp:253]     Train net output #0: loss = 0.0500874 (* 1 = 0.0500874 loss)
I0311 14:30:52.927564 31185 sgd_solver.cpp:106] Iteration 8100, lr = 0.00640827
I0311 14:30:53.155344 31185 solver.cpp:237] Iteration 8200, loss = 0.0255118
I0311 14:30:53.155376 31185 solver.cpp:253]     Train net output #0: loss = 0.0255119 (* 1 = 0.0255119 loss)
I0311 14:30:53.155385 31185 sgd_solver.cpp:106] Iteration 8200, lr = 0.00638185
I0311 14:30:53.379108 31185 solver.cpp:237] Iteration 8300, loss = 0.0331849
I0311 14:30:53.379142 31185 solver.cpp:253]     Train net output #0: loss = 0.033185 (* 1 = 0.033185 loss)
I0311 14:30:53.379151 31185 sgd_solver.cpp:106] Iteration 8300, lr = 0.00635567
I0311 14:30:53.603998 31185 solver.cpp:237] Iteration 8400, loss = 0.0346529
I0311 14:30:53.604032 31185 solver.cpp:253]     Train net output #0: loss = 0.034653 (* 1 = 0.034653 loss)
I0311 14:30:53.604043 31185 sgd_solver.cpp:106] Iteration 8400, lr = 0.00632975
I0311 14:30:53.825492 31185 solver.cpp:341] Iteration 8500, Testing net (#0)
I0311 14:30:53.825515 31185 net.cpp:748] Ignoring source layer mnist
I0311 14:30:54.017700 31185 solver.cpp:409]     Test net output #0: accuracy = 0.9776
I0311 14:30:54.017731 31185 solver.cpp:409]     Test net output #1: loss = 0.0710418 (* 1 = 0.0710418 loss)
I0311 14:30:54.019132 31185 solver.cpp:237] Iteration 8500, loss = 0.0420032
I0311 14:30:54.019153 31185 solver.cpp:253]     Train net output #0: loss = 0.0420032 (* 1 = 0.0420032 loss)
I0311 14:30:54.019165 31185 sgd_solver.cpp:106] Iteration 8500, lr = 0.00630407
I0311 14:30:54.243348 31185 solver.cpp:237] Iteration 8600, loss = 0.0511038
I0311 14:30:54.243383 31185 solver.cpp:253]     Train net output #0: loss = 0.0511039 (* 1 = 0.0511039 loss)
I0311 14:30:54.243392 31185 sgd_solver.cpp:106] Iteration 8600, lr = 0.00627864
I0311 14:30:54.476541 31185 solver.cpp:237] Iteration 8700, loss = 0.0464183
I0311 14:30:54.476575 31185 solver.cpp:253]     Train net output #0: loss = 0.0464184 (* 1 = 0.0464184 loss)
I0311 14:30:54.476584 31185 sgd_solver.cpp:106] Iteration 8700, lr = 0.00625344
I0311 14:30:54.708842 31185 solver.cpp:237] Iteration 8800, loss = 0.0240107
I0311 14:30:54.708874 31185 solver.cpp:253]     Train net output #0: loss = 0.0240108 (* 1 = 0.0240108 loss)
I0311 14:30:54.708919 31185 sgd_solver.cpp:106] Iteration 8800, lr = 0.00622847
I0311 14:30:54.939432 31185 solver.cpp:237] Iteration 8900, loss = 0.0311545
I0311 14:30:54.939465 31185 solver.cpp:253]     Train net output #0: loss = 0.0311546 (* 1 = 0.0311546 loss)
I0311 14:30:54.939474 31185 sgd_solver.cpp:106] Iteration 8900, lr = 0.00620374
I0311 14:30:55.167953 31185 solver.cpp:341] Iteration 9000, Testing net (#0)
I0311 14:30:55.167976 31185 net.cpp:748] Ignoring source layer mnist
I0311 14:30:55.346667 31185 solver.cpp:409]     Test net output #0: accuracy = 0.9787
I0311 14:30:55.346698 31185 solver.cpp:409]     Test net output #1: loss = 0.0709422 (* 1 = 0.0709422 loss)
I0311 14:30:55.349133 31185 solver.cpp:237] Iteration 9000, loss = 0.0325611
I0311 14:30:55.349153 31185 solver.cpp:253]     Train net output #0: loss = 0.0325612 (* 1 = 0.0325612 loss)
I0311 14:30:55.349164 31185 sgd_solver.cpp:106] Iteration 9000, lr = 0.00617924
I0311 14:30:55.578521 31185 solver.cpp:237] Iteration 9100, loss = 0.038755
I0311 14:30:55.578552 31185 solver.cpp:253]     Train net output #0: loss = 0.0387551 (* 1 = 0.0387551 loss)
I0311 14:30:55.578562 31185 sgd_solver.cpp:106] Iteration 9100, lr = 0.00615496
I0311 14:30:55.813119 31185 solver.cpp:237] Iteration 9200, loss = 0.0481953
I0311 14:30:55.813153 31185 solver.cpp:253]     Train net output #0: loss = 0.0481955 (* 1 = 0.0481955 loss)
I0311 14:30:55.813161 31185 sgd_solver.cpp:106] Iteration 9200, lr = 0.0061309
I0311 14:30:56.043380 31185 solver.cpp:237] Iteration 9300, loss = 0.0428333
I0311 14:30:56.043413 31185 solver.cpp:253]     Train net output #0: loss = 0.0428334 (* 1 = 0.0428334 loss)
I0311 14:30:56.043422 31185 sgd_solver.cpp:106] Iteration 9300, lr = 0.00610706
I0311 14:30:56.228328 31190 blocking_queue.cpp:50] Waiting for data
I0311 14:30:56.272387 31185 solver.cpp:237] Iteration 9400, loss = 0.022834
I0311 14:30:56.272418 31185 solver.cpp:253]     Train net output #0: loss = 0.0228342 (* 1 = 0.0228342 loss)
I0311 14:30:56.272423 31185 sgd_solver.cpp:106] Iteration 9400, lr = 0.00608343
I0311 14:30:56.494863 31185 solver.cpp:341] Iteration 9500, Testing net (#0)
I0311 14:30:56.494884 31185 net.cpp:748] Ignoring source layer mnist
I0311 14:30:56.669880 31185 solver.cpp:409]     Test net output #0: accuracy = 0.9787
I0311 14:30:56.669909 31185 solver.cpp:409]     Test net output #1: loss = 0.0677183 (* 1 = 0.0677183 loss)
I0311 14:30:56.671263 31185 solver.cpp:237] Iteration 9500, loss = 0.02947
I0311 14:30:56.671282 31185 solver.cpp:253]     Train net output #0: loss = 0.0294702 (* 1 = 0.0294702 loss)
I0311 14:30:56.671288 31185 sgd_solver.cpp:106] Iteration 9500, lr = 0.00606002
I0311 14:30:56.901790 31185 solver.cpp:237] Iteration 9600, loss = 0.0307721
I0311 14:30:56.901821 31185 solver.cpp:253]     Train net output #0: loss = 0.0307722 (* 1 = 0.0307722 loss)
I0311 14:30:56.901828 31185 sgd_solver.cpp:106] Iteration 9600, lr = 0.00603682
I0311 14:30:57.144006 31185 solver.cpp:237] Iteration 9700, loss = 0.0361612
I0311 14:30:57.144049 31185 solver.cpp:253]     Train net output #0: loss = 0.0361613 (* 1 = 0.0361613 loss)
I0311 14:30:57.144064 31185 sgd_solver.cpp:106] Iteration 9700, lr = 0.00601382
I0311 14:30:57.395614 31185 solver.cpp:237] Iteration 9800, loss = 0.0450963
I0311 14:30:57.395659 31185 solver.cpp:253]     Train net output #0: loss = 0.0450964 (* 1 = 0.0450964 loss)
I0311 14:30:57.395673 31185 sgd_solver.cpp:106] Iteration 9800, lr = 0.00599102
I0311 14:30:57.651823 31185 solver.cpp:237] Iteration 9900, loss = 0.0404249
I0311 14:30:57.651867 31185 solver.cpp:253]     Train net output #0: loss = 0.040425 (* 1 = 0.040425 loss)
I0311 14:30:57.651880 31185 sgd_solver.cpp:106] Iteration 9900, lr = 0.00596843
I0311 14:30:57.885308 31185 solver.cpp:459] Snapshotting to binary proto file examples/mlp/mlp_absinner_iter_10000.caffemodel
I0311 14:30:58.264259 31185 sgd_solver.cpp:269] Snapshotting solver state to binary proto file examples/mlp/mlp_absinner_iter_10000.solverstate
I0311 14:30:58.267946 31185 solver.cpp:321] Iteration 10000, loss = 0.0219371
I0311 14:30:58.267969 31185 solver.cpp:341] Iteration 10000, Testing net (#0)
I0311 14:30:58.267993 31185 net.cpp:748] Ignoring source layer mnist
I0311 14:30:58.441072 31185 solver.cpp:409]     Test net output #0: accuracy = 0.9787
I0311 14:30:58.441105 31185 solver.cpp:409]     Test net output #1: loss = 0.0665816 (* 1 = 0.0665816 loss)
I0311 14:30:58.441113 31185 solver.cpp:326] Optimization Done.
I0311 14:30:58.441118 31185 caffe.cpp:215] Optimization Done.
